Traceback (most recent call last):
  File "main_optuna.py", line 8, in <module>
    from get_model import get_model
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 2, in <module>
    from emp_both import EMP
ModuleNotFoundError: No module named 'emp_both'
Traceback (most recent call last):
  File "main_optuna.py", line 8, in <module>
    from get_model import get_model
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 3, in <module>
    from prop_mf import MFProp
ModuleNotFoundError: No module named 'prop_mf'
Traceback (most recent call last):
  File "main_optuna.py", line 10, in <module>
    from train_eval_ALTOPT import train, test
ModuleNotFoundError: No module named 'train_eval_ALTOPT'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0, 3, 6, 9, 15], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  5
[32m[I 2021-07-18 20:41:01,604][0m A new study created in memory with name: no-name-2cf86ae5-631d-4fbf-9bc8-6c8adec8ca7f[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
[33m[W 2021-07-18 20:41:08,505][0m Trial 0 failed because of the following error: TypeError('super(type, obj): obj must be an instance or subtype of type')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 105, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 156, in get_model
    args=args).cuda()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 56, in __init__
    super(APPNP, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type[0m
Traceback (most recent call last):
  File "main_optuna.py", line 305, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 105, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 156, in get_model
    args=args).cuda()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 56, in __init__
    super(APPNP, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0, 3, 6, 9, 15], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  5
[32m[I 2021-07-18 20:41:32,454][0m A new study created in memory with name: no-name-1c407a4a-16a4-4bb7-b597-1e45e5ce6c87[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
[33m[W 2021-07-18 20:41:34,204][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'prop'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model.reset_parameters()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 65, in reset_parameters
    self.prop.reset_parameters()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'prop'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 305, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model.reset_parameters()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 65, in reset_parameters
    self.prop.reset_parameters()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'prop'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0, 3, 6, 9, 15], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  5
[32m[I 2021-07-18 20:42:18,968][0m A new study created in memory with name: no-name-2001040c-8e91-42d5-a06d-8f3e5c241230[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6296, Train: 100.00%, Valid: 45.00% Test: 48.40%
Split: 01, Run: 01, Epoch: 40, Loss: 0.8459, Train: 100.00%, Valid: 59.00% Test: 59.30%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4050, Train: 100.00%, Valid: 61.00% Test: 59.50%
Split: 01, Run: 01, Epoch: 80, Loss: 0.2750, Train: 100.00%, Valid: 60.60% Test: 59.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2672, Train: 100.00%, Valid: 60.40% Test: 60.30%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2950, Train: 100.00%, Valid: 59.00% Test: 58.90%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2176, Train: 100.00%, Valid: 59.00% Test: 57.10%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2131, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2203, Train: 100.00%, Valid: 56.40% Test: 56.00%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 57.00%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2019, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2205, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2383, Train: 100.00%, Valid: 57.20% Test: 54.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2048, Train: 100.00%, Valid: 60.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1514, Train: 100.00%, Valid: 59.20% Test: 56.30%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2135, Train: 100.00%, Valid: 55.80% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1927, Train: 100.00%, Valid: 55.20% Test: 57.40%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2098, Train: 100.00%, Valid: 58.00% Test: 56.60%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1702, Train: 100.00%, Valid: 56.20% Test: 57.00%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2018, Train: 100.00%, Valid: 51.60% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  5.017946219071746
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 20, Loss: 1.5999, Train: 100.00%, Valid: 59.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 40, Loss: 0.7881, Train: 100.00%, Valid: 59.60% Test: 57.30%
Split: 01, Run: 02, Epoch: 60, Loss: 0.3974, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3144, Train: 100.00%, Valid: 59.00% Test: 58.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3007, Train: 100.00%, Valid: 57.60% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2404, Train: 100.00%, Valid: 57.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2344, Train: 100.00%, Valid: 56.40% Test: 56.20%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2561, Train: 100.00%, Valid: 57.40% Test: 57.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1924, Train: 100.00%, Valid: 56.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2064, Train: 100.00%, Valid: 57.60% Test: 57.80%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2297, Train: 100.00%, Valid: 58.20% Test: 55.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1974, Train: 100.00%, Valid: 58.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1818, Train: 100.00%, Valid: 56.20% Test: 54.90%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1891, Train: 100.00%, Valid: 51.00% Test: 55.10%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1828, Train: 100.00%, Valid: 56.40% Test: 57.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2363, Train: 100.00%, Valid: 55.40% Test: 55.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1829, Train: 100.00%, Valid: 54.20% Test: 56.00%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1737, Train: 100.00%, Valid: 55.20% Test: 55.60%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2007, Train: 100.00%, Valid: 57.60% Test: 56.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1539, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.499180801212788
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 20, Loss: 1.5795, Train: 100.00%, Valid: 55.80% Test: 54.60%
Split: 01, Run: 03, Epoch: 40, Loss: 0.8933, Train: 100.00%, Valid: 60.00% Test: 57.70%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4225, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3133, Train: 100.00%, Valid: 58.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3103, Train: 100.00%, Valid: 57.00% Test: 57.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2535, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2521, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2383, Train: 100.00%, Valid: 58.80% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2478, Train: 100.00%, Valid: 56.40% Test: 56.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2361, Train: 100.00%, Valid: 56.00% Test: 55.60%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1788, Train: 100.00%, Valid: 56.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1816, Train: 100.00%, Valid: 56.40% Test: 56.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2121, Train: 100.00%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1828, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 03, Epoch: 360, Loss: 0.1792, Train: 100.00%, Valid: 54.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1956, Train: 100.00%, Valid: 59.40% Test: 58.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1739, Train: 100.00%, Valid: 57.40% Test: 56.00%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1846, Train: 100.00%, Valid: 58.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1729, Train: 100.00%, Valid: 57.80% Test: 55.60%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1913, Train: 100.00%, Valid: 56.60% Test: 55.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.503529252484441
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  15.711061025038362
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 20:42:34,690][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6296, Train: 100.00%, Valid: 45.00% Test: 48.40%
Split: 01, Run: 01, Epoch: 40, Loss: 0.8459, Train: 100.00%, Valid: 59.00% Test: 59.30%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4050, Train: 100.00%, Valid: 61.00% Test: 59.50%
Split: 01, Run: 01, Epoch: 80, Loss: 0.2750, Train: 100.00%, Valid: 60.60% Test: 59.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2672, Train: 100.00%, Valid: 60.40% Test: 60.30%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2950, Train: 100.00%, Valid: 59.00% Test: 58.90%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2176, Train: 100.00%, Valid: 59.00% Test: 57.10%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2131, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2203, Train: 100.00%, Valid: 56.40% Test: 56.00%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 57.00%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2019, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2205, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2383, Train: 100.00%, Valid: 57.20% Test: 54.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2048, Train: 100.00%, Valid: 60.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1514, Train: 100.00%, Valid: 59.20% Test: 56.30%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2135, Train: 100.00%, Valid: 55.80% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1927, Train: 100.00%, Valid: 55.20% Test: 57.40%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2098, Train: 100.00%, Valid: 58.00% Test: 56.60%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1702, Train: 100.00%, Valid: 56.20% Test: 57.00%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2018, Train: 100.00%, Valid: 51.60% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.460771757178009
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 20, Loss: 1.5999, Train: 100.00%, Valid: 59.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 40, Loss: 0.7881, Train: 100.00%, Valid: 59.60% Test: 57.30%
Split: 01, Run: 02, Epoch: 60, Loss: 0.3974, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3144, Train: 100.00%, Valid: 59.00% Test: 58.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3007, Train: 100.00%, Valid: 57.60% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2404, Train: 100.00%, Valid: 57.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2344, Train: 100.00%, Valid: 56.40% Test: 56.20%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2561, Train: 100.00%, Valid: 57.40% Test: 57.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1924, Train: 100.00%, Valid: 56.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2064, Train: 100.00%, Valid: 57.60% Test: 57.80%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2297, Train: 100.00%, Valid: 58.20% Test: 55.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1974, Train: 100.00%, Valid: 58.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1818, Train: 100.00%, Valid: 56.20% Test: 54.90%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1891, Train: 100.00%, Valid: 51.00% Test: 55.10%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1828, Train: 100.00%, Valid: 56.40% Test: 57.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2363, Train: 100.00%, Valid: 55.40% Test: 55.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1829, Train: 100.00%, Valid: 54.20% Test: 56.00%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1737, Train: 100.00%, Valid: 55.20% Test: 55.60%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2007, Train: 100.00%, Valid: 57.60% Test: 56.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1539, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.48186297994107
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 20, Loss: 1.5795, Train: 100.00%, Valid: 55.80% Test: 54.60%
Split: 01, Run: 03, Epoch: 40, Loss: 0.8933, Train: 100.00%, Valid: 60.00% Test: 57.70%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4225, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3133, Train: 100.00%, Valid: 58.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3103, Train: 100.00%, Valid: 57.00% Test: 57.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2535, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2521, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2383, Train: 100.00%, Valid: 58.80% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2478, Train: 100.00%, Valid: 56.40% Test: 56.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2361, Train: 100.00%, Valid: 56.00% Test: 55.60%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1788, Train: 100.00%, Valid: 56.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1816, Train: 100.00%, Valid: 56.40% Test: 56.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2121, Train: 100.00%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1828, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 03, Epoch: 360, Loss: 0.1792, Train: 100.00%, Valid: 54.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1956, Train: 100.00%, Valid: 59.40% Test: 58.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1739, Train: 100.00%, Valid: 57.40% Test: 56.00%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1846, Train: 100.00%, Valid: 58.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1729, Train: 100.00%, Valid: 57.80% Test: 55.60%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1913, Train: 100.00%, Valid: 56.60% Test: 55.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.43318554200232
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  13.45285306032747
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 20:42:48,149][0m Trial 1 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6296, Train: 100.00%, Valid: 45.00% Test: 48.40%
Split: 01, Run: 01, Epoch: 40, Loss: 0.8459, Train: 100.00%, Valid: 59.00% Test: 59.30%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4050, Train: 100.00%, Valid: 61.00% Test: 59.50%
Split: 01, Run: 01, Epoch: 80, Loss: 0.2750, Train: 100.00%, Valid: 60.60% Test: 59.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2672, Train: 100.00%, Valid: 60.40% Test: 60.30%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2950, Train: 100.00%, Valid: 59.00% Test: 58.90%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2176, Train: 100.00%, Valid: 59.00% Test: 57.10%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2131, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2203, Train: 100.00%, Valid: 56.40% Test: 56.00%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 57.00%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2019, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2205, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2383, Train: 100.00%, Valid: 57.20% Test: 54.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2048, Train: 100.00%, Valid: 60.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1514, Train: 100.00%, Valid: 59.20% Test: 56.30%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2135, Train: 100.00%, Valid: 55.80% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1927, Train: 100.00%, Valid: 55.20% Test: 57.40%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2098, Train: 100.00%, Valid: 58.00% Test: 56.60%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1702, Train: 100.00%, Valid: 56.20% Test: 57.00%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2018, Train: 100.00%, Valid: 51.60% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.4684865940362215
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 20, Loss: 1.5999, Train: 100.00%, Valid: 59.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 40, Loss: 0.7881, Train: 100.00%, Valid: 59.60% Test: 57.30%
Split: 01, Run: 02, Epoch: 60, Loss: 0.3974, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3144, Train: 100.00%, Valid: 59.00% Test: 58.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3007, Train: 100.00%, Valid: 57.60% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2404, Train: 100.00%, Valid: 57.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2344, Train: 100.00%, Valid: 56.40% Test: 56.20%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2561, Train: 100.00%, Valid: 57.40% Test: 57.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1924, Train: 100.00%, Valid: 56.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2064, Train: 100.00%, Valid: 57.60% Test: 57.80%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2297, Train: 100.00%, Valid: 58.20% Test: 55.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1974, Train: 100.00%, Valid: 58.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1818, Train: 100.00%, Valid: 56.20% Test: 54.90%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1891, Train: 100.00%, Valid: 51.00% Test: 55.10%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1828, Train: 100.00%, Valid: 56.40% Test: 57.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2363, Train: 100.00%, Valid: 55.40% Test: 55.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1829, Train: 100.00%, Valid: 54.20% Test: 56.00%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1737, Train: 100.00%, Valid: 55.20% Test: 55.60%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2007, Train: 100.00%, Valid: 57.60% Test: 56.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1539, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.462709906511009
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 20, Loss: 1.5795, Train: 100.00%, Valid: 55.80% Test: 54.60%
Split: 01, Run: 03, Epoch: 40, Loss: 0.8933, Train: 100.00%, Valid: 60.00% Test: 57.70%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4225, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3133, Train: 100.00%, Valid: 58.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3103, Train: 100.00%, Valid: 57.00% Test: 57.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2535, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2521, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2383, Train: 100.00%, Valid: 58.80% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2478, Train: 100.00%, Valid: 56.40% Test: 56.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2361, Train: 100.00%, Valid: 56.00% Test: 55.60%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1788, Train: 100.00%, Valid: 56.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1816, Train: 100.00%, Valid: 56.40% Test: 56.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2121, Train: 100.00%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1828, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 03, Epoch: 360, Loss: 0.1792, Train: 100.00%, Valid: 54.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1956, Train: 100.00%, Valid: 59.40% Test: 58.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1739, Train: 100.00%, Valid: 57.40% Test: 56.00%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1846, Train: 100.00%, Valid: 58.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1729, Train: 100.00%, Valid: 57.80% Test: 55.60%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1913, Train: 100.00%, Valid: 56.60% Test: 55.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.470580571331084
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  13.471113012172282
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 20:43:01,626][0m Trial 2 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6296, Train: 100.00%, Valid: 45.00% Test: 48.40%
Split: 01, Run: 01, Epoch: 40, Loss: 0.8459, Train: 100.00%, Valid: 59.00% Test: 59.30%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4050, Train: 100.00%, Valid: 61.00% Test: 59.50%
Split: 01, Run: 01, Epoch: 80, Loss: 0.2750, Train: 100.00%, Valid: 60.60% Test: 59.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2672, Train: 100.00%, Valid: 60.40% Test: 60.30%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2950, Train: 100.00%, Valid: 59.00% Test: 58.90%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2176, Train: 100.00%, Valid: 59.00% Test: 57.10%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2131, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2203, Train: 100.00%, Valid: 56.40% Test: 56.00%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 57.00%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2019, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2205, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2383, Train: 100.00%, Valid: 57.20% Test: 54.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2048, Train: 100.00%, Valid: 60.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1514, Train: 100.00%, Valid: 59.20% Test: 56.30%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2135, Train: 100.00%, Valid: 55.80% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1927, Train: 100.00%, Valid: 55.20% Test: 57.40%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2098, Train: 100.00%, Valid: 58.00% Test: 56.60%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1702, Train: 100.00%, Valid: 56.20% Test: 57.00%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2018, Train: 100.00%, Valid: 51.60% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.523815866559744
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 20, Loss: 1.5999, Train: 100.00%, Valid: 59.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 40, Loss: 0.7881, Train: 100.00%, Valid: 59.60% Test: 57.30%
Split: 01, Run: 02, Epoch: 60, Loss: 0.3974, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3144, Train: 100.00%, Valid: 59.00% Test: 58.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3007, Train: 100.00%, Valid: 57.60% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2404, Train: 100.00%, Valid: 57.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2344, Train: 100.00%, Valid: 56.40% Test: 56.20%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2561, Train: 100.00%, Valid: 57.40% Test: 57.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1924, Train: 100.00%, Valid: 56.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2064, Train: 100.00%, Valid: 57.60% Test: 57.80%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2297, Train: 100.00%, Valid: 58.20% Test: 55.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1974, Train: 100.00%, Valid: 58.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1818, Train: 100.00%, Valid: 56.20% Test: 54.90%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1891, Train: 100.00%, Valid: 51.00% Test: 55.10%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1828, Train: 100.00%, Valid: 56.40% Test: 57.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2363, Train: 100.00%, Valid: 55.40% Test: 55.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1829, Train: 100.00%, Valid: 54.20% Test: 56.00%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1737, Train: 100.00%, Valid: 55.20% Test: 55.60%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2007, Train: 100.00%, Valid: 57.60% Test: 56.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1539, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.539323713630438
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 20, Loss: 1.5795, Train: 100.00%, Valid: 55.80% Test: 54.60%
Split: 01, Run: 03, Epoch: 40, Loss: 0.8933, Train: 100.00%, Valid: 60.00% Test: 57.70%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4225, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3133, Train: 100.00%, Valid: 58.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3103, Train: 100.00%, Valid: 57.00% Test: 57.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2535, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2521, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2383, Train: 100.00%, Valid: 58.80% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2478, Train: 100.00%, Valid: 56.40% Test: 56.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2361, Train: 100.00%, Valid: 56.00% Test: 55.60%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1788, Train: 100.00%, Valid: 56.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1816, Train: 100.00%, Valid: 56.40% Test: 56.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2121, Train: 100.00%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1828, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 03, Epoch: 360, Loss: 0.1792, Train: 100.00%, Valid: 54.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1956, Train: 100.00%, Valid: 59.40% Test: 58.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1739, Train: 100.00%, Valid: 57.40% Test: 56.00%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1846, Train: 100.00%, Valid: 58.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1729, Train: 100.00%, Valid: 57.80% Test: 55.60%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1913, Train: 100.00%, Valid: 56.60% Test: 55.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.492837239988148
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  13.614037099294364
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 20:43:15,246][0m Trial 3 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=None, log_steps=20, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6296, Train: 100.00%, Valid: 45.00% Test: 48.40%
Split: 01, Run: 01, Epoch: 40, Loss: 0.8459, Train: 100.00%, Valid: 59.00% Test: 59.30%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4050, Train: 100.00%, Valid: 61.00% Test: 59.50%
Split: 01, Run: 01, Epoch: 80, Loss: 0.2750, Train: 100.00%, Valid: 60.60% Test: 59.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2672, Train: 100.00%, Valid: 60.40% Test: 60.30%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2950, Train: 100.00%, Valid: 59.00% Test: 58.90%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2176, Train: 100.00%, Valid: 59.00% Test: 57.10%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2131, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2203, Train: 100.00%, Valid: 56.40% Test: 56.00%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 57.00%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2019, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2205, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2383, Train: 100.00%, Valid: 57.20% Test: 54.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2048, Train: 100.00%, Valid: 60.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1514, Train: 100.00%, Valid: 59.20% Test: 56.30%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2135, Train: 100.00%, Valid: 55.80% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1927, Train: 100.00%, Valid: 55.20% Test: 57.40%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2098, Train: 100.00%, Valid: 58.00% Test: 56.60%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1702, Train: 100.00%, Valid: 56.20% Test: 57.00%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2018, Train: 100.00%, Valid: 51.60% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.380372099578381
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 20, Loss: 1.5999, Train: 100.00%, Valid: 59.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 40, Loss: 0.7881, Train: 100.00%, Valid: 59.60% Test: 57.30%
Split: 01, Run: 02, Epoch: 60, Loss: 0.3974, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3144, Train: 100.00%, Valid: 59.00% Test: 58.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3007, Train: 100.00%, Valid: 57.60% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2404, Train: 100.00%, Valid: 57.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2344, Train: 100.00%, Valid: 56.40% Test: 56.20%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2561, Train: 100.00%, Valid: 57.40% Test: 57.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1924, Train: 100.00%, Valid: 56.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2064, Train: 100.00%, Valid: 57.60% Test: 57.80%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2297, Train: 100.00%, Valid: 58.20% Test: 55.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1974, Train: 100.00%, Valid: 58.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1818, Train: 100.00%, Valid: 56.20% Test: 54.90%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1891, Train: 100.00%, Valid: 51.00% Test: 55.10%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1828, Train: 100.00%, Valid: 56.40% Test: 57.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2363, Train: 100.00%, Valid: 55.40% Test: 55.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1829, Train: 100.00%, Valid: 54.20% Test: 56.00%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1737, Train: 100.00%, Valid: 55.20% Test: 55.60%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2007, Train: 100.00%, Valid: 57.60% Test: 56.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1539, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.382226997986436
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 20, Loss: 1.5795, Train: 100.00%, Valid: 55.80% Test: 54.60%
Split: 01, Run: 03, Epoch: 40, Loss: 0.8933, Train: 100.00%, Valid: 60.00% Test: 57.70%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4225, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3133, Train: 100.00%, Valid: 58.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3103, Train: 100.00%, Valid: 57.00% Test: 57.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2535, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2521, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2383, Train: 100.00%, Valid: 58.80% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2478, Train: 100.00%, Valid: 56.40% Test: 56.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2361, Train: 100.00%, Valid: 56.00% Test: 55.60%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1788, Train: 100.00%, Valid: 56.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1816, Train: 100.00%, Valid: 56.40% Test: 56.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2121, Train: 100.00%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1828, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 03, Epoch: 360, Loss: 0.1792, Train: 100.00%, Valid: 54.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1956, Train: 100.00%, Valid: 59.40% Test: 58.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1739, Train: 100.00%, Valid: 57.40% Test: 56.00%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1846, Train: 100.00%, Valid: 58.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1729, Train: 100.00%, Valid: 57.80% Test: 55.60%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1913, Train: 100.00%, Valid: 56.60% Test: 55.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.616941838525236
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  13.43884389474988
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 20:43:28,691][0m Trial 4 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in sort_trials
    trials.sort(key=get_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in get_key
    return trial.params[key]
KeyError: 'lambda1'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 20:45:20,643][0m A new study created in memory with name: no-name-655a9088-c382-4d66-b21b-cba1b401d552[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 20:45:20,645][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in sort_trials
    trials.sort(key=get_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in get_key
    return trial.params[key]
KeyError: 'lambda1'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 20:45:51,487][0m A new study created in memory with name: no-name-55c2ec98-020d-4547-aa4d-4d376efc0f5b[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 20:45:51,489][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in sort_trials
    trials.sort(key=get_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in get_key
    return trial.params[key]
KeyError: 'lambda1'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 20:46:14,691][0m A new study created in memory with name: no-name-247d009d-d02e-477b-a551-952bb576bb0f[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 20:46:14,693][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in sort_trials
    trials.sort(key=get_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in get_key
    return trial.params[key]
KeyError: 'lambda1'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 20:46:27,884][0m A new study created in memory with name: no-name-b633a460-d5eb-4424-97d1-dfe0f4003b81[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 20:46:27,886][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in sort_trials
    trials.sort(key=get_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in get_key
    return trial.params[key]
KeyError: 'lambda1'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:10:03,788][0m A new study created in memory with name: no-name-86b0af5d-97b8-4db7-8232-924b26d62b2b[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 21:10:03,791][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py[0m(5)[0;36msort_trials[0;34m()[0m
[0;32m      4 [0;31m    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m----> 5 [0;31m    [0;32mdef[0m [0mget_key[0m[0;34m([0m[0mtrial[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m      6 [0;31m        [0;32mreturn[0m [0mtrial[0m[0;34m.[0m[0mparams[0m[0;34m[[0m[0mkey[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'trial' is not defined
ipdb> *** AttributeError: 'list' object has no attribute 'paramss'
ipdb> *** AttributeError: 'list' object has no attribute 'paramss'
ipdb> *** AttributeError: 'list' object has no attribute 'params'
ipdb> *** AttributeError: 'list' object has no attribute 'params'
ipdb> *** AttributeError: 'list' object has no attribute 'params'
ipdb> *** AttributeError: 'list' object has no attribute 'params'
ipdb> False
ipdb> dict_keys(['lr', 'weight_decay', 'dropout', 'K'])
ipdb> True
ipdb> Traceback (most recent call last):
  File "main_optuna.py", line 314, in <module>
    sorted_trial = sort_trials(study.trials, key=args.sort_key)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 5, in sort_trials
    if not key in trials[0].params.keys():
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/myutil.py", line 6, in get_key
    return trials
KeyError: 'lambda1'

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:12:39,221][0m A new study created in memory with name: no-name-74fd3668-6b11-4485-9fc6-20b95542fcab[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 21:12:39,223][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  0    {}
Traceback (most recent call last):
  File "main_optuna.py", line 323, in <module>
    test_acc.append(trial.user_attrs['test'])
KeyError: 'test'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:14:34,522][0m A new study created in memory with name: no-name-80b7fca8-7d42-43e0-a2c4-ee9204a8ce7d[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
[32m[I 2021-07-18 21:14:34,525][0m Trial 0 finished with value: 0.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 0.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  0    {}
Traceback (most recent call last):
  File "main_optuna.py", line 323, in <module>
    test_acc.append(trial.user_attrs['test'])
KeyError: 'test'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:15:41,670][0m A new study created in memory with name: no-name-e53624a1-0649-48a0-bf7e-f62550f2cb84[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Using backend: pytorch
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:15:43,807][0m A new study created in memory with name: no-name-03c4f765-f127-4106-93bd-6531344a7864[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:15:44,106][0m A new study created in memory with name: no-name-b82c319e-a0a0-481d-b918-a957c0944dd6[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
[33m[W 2021-07-18 21:15:50,075][0m Trial 0 failed because of the following error: NameError("name 'ipda' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 305, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

[33m[W 2021-07-18 21:15:50,087][0m Trial 0 failed because of the following error: NameError("name 'ipda' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 305, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

[33m[W 2021-07-18 21:15:50,103][0m Trial 0 failed because of the following error: NameError("name 'ipda' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 305, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 83, in forward
    import ipdb; ipda.set_trace()
NameError: name 'ipda' is not defined

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:16:14,264][0m A new study created in memory with name: no-name-6cee24c0-995c-4c65-835f-76197828f903[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(84)[0;36mforward[0;34m()[0m
[0;32m     83 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 84 [0;31m        [0;32mreturn[0m [0mF[0m[0;34m.[0m[0mlog_softmax[0m[0;34m([0m[0mx[0m[0;34m,[0m [0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     85 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(84)[0;36mforward[0;34m()[0m
[0;32m     83 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 84 [0;31m        [0;32mreturn[0m [0mF[0m[0;34m.[0m[0mlog_softmax[0m[0;34m([0m[0mx[0m[0;34m,[0m [0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     85 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(84)[0;36mforward[0;34m()[0m
[0;32m     83 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 84 [0;31m        [0;32mreturn[0m [0mF[0m[0;34m.[0m[0mlog_softmax[0m[0;34m([0m[0mx[0m[0;34m,[0m [0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     85 [0;31m[0;34m[0m[0m
[0m
ipdb> ipdb> [33m[W 2021-07-18 21:16:38,953][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 117, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 84, in forward
    return F.log_softmax(x, dim=1)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 84, in forward
    return F.log_softmax(x, dim=1)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:16:47,204][0m A new study created in memory with name: no-name-156f8934-2222-46d5-974a-eb7548feb335[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.950310859829187
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.736533642746508
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.785592135973275
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  16.222709794528782
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-18 21:17:03,437][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
test_acc
['57.57 Â± 1.77']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  62.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
optuna total time:  16.23740037996322
Using backend: pytorch
Using backend: pytorch
main:  main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:17:12,027][0m A new study created in memory with name: no-name-7964b72f-fc18-495f-8ab2-fbf595576aa4[0m
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:17:12,027][0m A new study created in memory with name: no-name-c75aa0ed-4fe5-4535-8aed-f2a9d9d8c272[0m
K:  100
alpha:  None
lr:  K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.2588, Train: 100.00%, Valid: 56.20% Test: 56.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 100.00%, Valid: 72.20% Test: 72.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2533, Train: 100.00%, Valid: 55.20% Test: 56.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 100.00%, Valid: 71.00% Test: 71.40%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2029, Train: 100.00%, Valid: 56.00% Test: 54.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 100.00%, Valid: 69.40% Test: 71.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1519, Train: 100.00%, Valid: 56.20% Test: 56.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 100.00%, Valid: 71.20% Test: 71.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0971, Train: 100.00%, Valid: 55.20% Test: 55.40%
Split: 01, Run: 01
None time:  7.2999836495146155
None Run 01:
Highest Train: 100.00
Highest Valid: 58.40
  Final Train: 100.00
   Final Test: 56.00
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 100.00%, Valid: 69.00% Test: 70.60%
Split: 01, Run: 01
None time:  7.311366374604404
None Run 01:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 72.50
Split: 01, Run: 02, Epoch: 100, Loss: 0.2506, Train: 100.00%, Valid: 55.60% Test: 57.00%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 100.00%, Valid: 69.40% Test: 71.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2171, Train: 100.00%, Valid: 52.80% Test: 54.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0706, Train: 100.00%, Valid: 67.20% Test: 68.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1689, Train: 100.00%, Valid: 53.60% Test: 54.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0888, Train: 100.00%, Valid: 69.80% Test: 70.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1355, Train: 100.00%, Valid: 52.20% Test: 55.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.0912, Train: 100.00%, Valid: 70.60% Test: 72.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1399, Train: 100.00%, Valid: 52.60% Test: 54.30%
Split: 01, Run: 02
None time:  6.971351514570415
None Run 02:
Highest Train: 100.00
Highest Valid: 57.80
  Final Train: 100.00
   Final Test: 56.50
Split: 01, Run: 02, Epoch: 500, Loss: 0.0535, Train: 100.00%, Valid: 69.80% Test: 70.20%
Split: 01, Run: 02
None time:  7.003905691206455
None Run 02:
Highest Train: 100.00
Highest Valid: 72.80
  Final Train: 100.00
   Final Test: 72.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.2805, Train: 100.00%, Valid: 57.00% Test: 58.10%
Split: 01, Run: 03, Epoch: 100, Loss: 0.1885, Train: 100.00%, Valid: 70.00% Test: 71.20%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1730, Train: 100.00%, Valid: 52.80% Test: 54.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1082, Train: 100.00%, Valid: 69.40% Test: 69.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1536, Train: 100.00%, Valid: 53.60% Test: 56.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0853, Train: 100.00%, Valid: 69.60% Test: 69.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.1459, Train: 100.00%, Valid: 53.80% Test: 54.80%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0595, Train: 100.00%, Valid: 68.80% Test: 71.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1544, Train: 100.00%, Valid: 52.00% Test: 55.10%
Split: 01, Run: 03
None time:  6.972654811106622
None Run 03:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 57.60
total time:  23.04275236558169
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.07 Â± 0.31
  Final Train: 100.00 Â± 0.00
   Final Test: 56.70 Â± 0.82
[32m[I 2021-07-18 21:17:35,116][0m Trial 0 finished with value: 58.06666564941406 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 58.06666564941406.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  58.067    {'train': '100.00 Â± 0.00', 'valid': '58.07 Â± 0.31', 'test': '56.70 Â± 0.82'}
test_acc
['56.70 Â± 0.82']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  58.06666564941406
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '58.07 Â± 0.31', 'test': '56.70 Â± 0.82'}
optuna total time:  23.09389769192785
Split: 01, Run: 03, Epoch: 500, Loss: 0.0716, Train: 100.00%, Valid: 68.20% Test: 70.30%
Split: 01, Run: 03
None time:  6.958989412523806
None Run 03:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 73.40
total time:  23.09480953309685
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.80 Â± 0.20
  Final Train: 100.00 Â± 0.00
   Final Test: 72.87 Â± 0.47
[32m[I 2021-07-18 21:17:35,168][0m Trial 0 finished with value: 72.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 72.79999542236328.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  72.8    {'train': '100.00 Â± 0.00', 'valid': '72.80 Â± 0.20', 'test': '72.87 Â± 0.47'}
test_acc
['72.87 Â± 0.47']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  72.79999542236328
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '72.80 Â± 0.20', 'test': '72.87 Â± 0.47'}
optuna total time:  23.14596764743328
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:18:32,905][0m A new study created in memory with name: no-name-d56388c2-8a7b-4779-948d-4a517b13d6fb[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 10 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.69707939773798
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Using backend: pytorch
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:18:40,350][0m A new study created in memory with name: no-name-0956a67d-7bb5-4316-a74b-74e055d30c27[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 10 times
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Data: Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.2588, Train: 100.00%, Valid: 56.20% Test: 56.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  5.3001512652263045
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 01, Epoch: 200, Loss: 0.2533, Train: 100.00%, Valid: 55.20% Test: 56.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2029, Train: 100.00%, Valid: 56.00% Test: 54.20%
Using backend: pytorch
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1519, Train: 100.00%, Valid: 56.20% Test: 56.40%
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:18:47,486][0m A new study created in memory with name: no-name-b91fd9af-b250-4ff5-b088-6c14d68442aa[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=10, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 10 times
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0971, Train: 100.00%, Valid: 55.20% Test: 55.40%
Split: 01, Run: 01
None time:  6.1336713917553425
None Run 01:
Highest Train: 100.00
Highest Valid: 58.40
  Final Train: 100.00
   Final Test: 56.00
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
Split: 01, Run: 02, Epoch: 100, Loss: 0.2506, Train: 100.00%, Valid: 55.60% Test: 57.00%
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  6.159477912820876
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
Split: 01, Run: 02, Epoch: 200, Loss: 0.2171, Train: 100.00%, Valid: 52.80% Test: 54.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 100.00%, Valid: 72.20% Test: 72.20%
Split: 01, Run: 04, Epoch: 100, Loss: 0.2552, Train: 100.00%, Valid: 58.80% Test: 59.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1689, Train: 100.00%, Valid: 53.60% Test: 54.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 100.00%, Valid: 71.00% Test: 71.40%
Split: 01, Run: 04, Epoch: 200, Loss: 0.2332, Train: 100.00%, Valid: 59.00% Test: 57.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1355, Train: 100.00%, Valid: 52.20% Test: 55.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 100.00%, Valid: 69.40% Test: 71.00%
Split: 01, Run: 04, Epoch: 300, Loss: 0.2154, Train: 100.00%, Valid: 57.00% Test: 57.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1399, Train: 100.00%, Valid: 52.60% Test: 54.30%
Split: 01, Run: 02
None time:  7.904285693541169
None Run 02:
Highest Train: 100.00
Highest Valid: 57.80
  Final Train: 100.00
   Final Test: 56.50
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 100.00%, Valid: 71.20% Test: 71.30%
Split: 01, Run: 04, Epoch: 400, Loss: 0.2077, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 03, Epoch: 100, Loss: 0.2805, Train: 100.00%, Valid: 57.00% Test: 58.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 100.00%, Valid: 69.00% Test: 70.60%
Split: 01, Run: 01
None time:  8.880695066414773
None Run 01:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 72.50
Split: 01, Run: 04, Epoch: 500, Loss: 0.2044, Train: 100.00%, Valid: 55.20% Test: 53.90%
Split: 01, Run: 04
None time:  8.708183383569121
None Run 04:
Highest Train: 100.00
Highest Valid: 59.80
  Final Train: 100.00
   Final Test: 59.30
Split: 01, Run: 03, Epoch: 200, Loss: 0.1730, Train: 100.00%, Valid: 52.80% Test: 54.80%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 100.00%, Valid: 69.40% Test: 71.10%
Split: 01, Run: 05, Epoch: 100, Loss: 0.2913, Train: 100.00%, Valid: 60.40% Test: 58.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1536, Train: 100.00%, Valid: 53.60% Test: 56.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0706, Train: 100.00%, Valid: 67.20% Test: 68.50%
Split: 01, Run: 05, Epoch: 200, Loss: 0.2037, Train: 100.00%, Valid: 58.80% Test: 59.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.1459, Train: 100.00%, Valid: 53.80% Test: 54.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0888, Train: 100.00%, Valid: 69.80% Test: 70.60%
Split: 01, Run: 05, Epoch: 300, Loss: 0.2209, Train: 100.00%, Valid: 58.00% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1544, Train: 100.00%, Valid: 52.00% Test: 55.10%
Split: 01, Run: 03
None time:  8.593158608302474
None Run 03:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 57.60
Split: 01, Run: 02, Epoch: 400, Loss: 0.0912, Train: 100.00%, Valid: 70.60% Test: 72.40%
Split: 01, Run: 05, Epoch: 400, Loss: 0.2307, Train: 100.00%, Valid: 57.40% Test: 56.10%
Split: 01, Run: 04, Epoch: 100, Loss: 0.2751, Train: 100.00%, Valid: 58.40% Test: 58.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0535, Train: 100.00%, Valid: 69.80% Test: 70.20%
Split: 01, Run: 02
None time:  8.63133458700031
None Run 02:
Highest Train: 100.00
Highest Valid: 72.80
  Final Train: 100.00
   Final Test: 72.70
Split: 01, Run: 05, Epoch: 500, Loss: 0.2074, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 05
None time:  8.717221240513027
None Run 05:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 57.40
Split: 01, Run: 04, Epoch: 200, Loss: 0.2178, Train: 100.00%, Valid: 55.60% Test: 56.50%
Split: 01, Run: 03, Epoch: 100, Loss: 0.1885, Train: 100.00%, Valid: 70.00% Test: 71.20%
Split: 01, Run: 06, Epoch: 100, Loss: 0.2951, Train: 100.00%, Valid: 58.40% Test: 58.90%
Split: 01, Run: 04, Epoch: 300, Loss: 0.1429, Train: 100.00%, Valid: 53.20% Test: 56.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1082, Train: 100.00%, Valid: 69.40% Test: 69.70%
Split: 01, Run: 06, Epoch: 200, Loss: 0.2037, Train: 100.00%, Valid: 53.60% Test: 55.00%
Split: 01, Run: 04, Epoch: 400, Loss: 0.1601, Train: 100.00%, Valid: 55.00% Test: 55.80%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0853, Train: 100.00%, Valid: 69.60% Test: 69.30%
Split: 01, Run: 04, Epoch: 500, Loss: 0.1385, Train: 100.00%, Valid: 53.60% Test: 56.20%
Split: 01, Run: 04
None time:  8.615569639019668
None Run 04:
Highest Train: 100.00
Highest Valid: 58.60
  Final Train: 100.00
   Final Test: 57.90
Split: 01, Run: 06, Epoch: 300, Loss: 0.1929, Train: 100.00%, Valid: 55.80% Test: 56.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0595, Train: 100.00%, Valid: 68.80% Test: 71.50%
Split: 01, Run: 05, Epoch: 100, Loss: 0.3315, Train: 100.00%, Valid: 54.40% Test: 56.20%
Split: 01, Run: 06, Epoch: 400, Loss: 0.1955, Train: 100.00%, Valid: 55.60% Test: 55.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0716, Train: 100.00%, Valid: 68.20% Test: 70.30%
Split: 01, Run: 03
None time:  8.621647686697543
None Run 03:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 05, Epoch: 200, Loss: 0.1888, Train: 100.00%, Valid: 55.20% Test: 55.10%
Split: 01, Run: 06, Epoch: 500, Loss: 0.2184, Train: 100.00%, Valid: 57.00% Test: 56.00%
Split: 01, Run: 06
None time:  10.16018551774323
None Run 06:
Highest Train: 100.00
Highest Valid: 63.40
  Final Train: 100.00
   Final Test: 60.60
Split: 01, Run: 04, Epoch: 100, Loss: 0.0942, Train: 100.00%, Valid: 70.40% Test: 70.90%
Split: 01, Run: 05, Epoch: 300, Loss: 0.1814, Train: 100.00%, Valid: 54.00% Test: 55.60%
Split: 01, Run: 07, Epoch: 100, Loss: 0.2445, Train: 100.00%, Valid: 59.40% Test: 57.30%
Split: 01, Run: 04, Epoch: 200, Loss: 0.0909, Train: 100.00%, Valid: 71.40% Test: 71.70%
Split: 01, Run: 05, Epoch: 400, Loss: 0.1491, Train: 100.00%, Valid: 54.40% Test: 56.10%
Split: 01, Run: 07, Epoch: 200, Loss: 0.2154, Train: 100.00%, Valid: 57.00% Test: 56.00%
Split: 01, Run: 04, Epoch: 300, Loss: 0.0643, Train: 100.00%, Valid: 69.20% Test: 70.50%
Split: 01, Run: 05, Epoch: 500, Loss: 0.1621, Train: 100.00%, Valid: 55.40% Test: 55.10%
Split: 01, Run: 05
None time:  10.033438477665186
None Run 05:
Highest Train: 100.00
Highest Valid: 58.40
  Final Train: 100.00
   Final Test: 55.10
Split: 01, Run: 07, Epoch: 300, Loss: 0.2015, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 04, Epoch: 400, Loss: 0.0750, Train: 100.00%, Valid: 68.20% Test: 70.00%
Split: 01, Run: 06, Epoch: 100, Loss: 0.2748, Train: 100.00%, Valid: 53.40% Test: 56.00%
Split: 01, Run: 07, Epoch: 400, Loss: 0.1607, Train: 100.00%, Valid: 58.80% Test: 57.70%
Split: 01, Run: 04, Epoch: 500, Loss: 0.0676, Train: 100.00%, Valid: 70.60% Test: 71.50%
Split: 01, Run: 04
None time:  9.985077393241227
None Run 04:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 72.30
Split: 01, Run: 06, Epoch: 200, Loss: 0.2023, Train: 100.00%, Valid: 53.80% Test: 55.60%
Split: 01, Run: 07, Epoch: 500, Loss: 0.2314, Train: 100.00%, Valid: 58.00% Test: 56.00%
Split: 01, Run: 07
None time:  8.855633224360645
None Run 07:
Highest Train: 100.00
Highest Valid: 62.20
  Final Train: 100.00
   Final Test: 58.20
Split: 01, Run: 05, Epoch: 100, Loss: 0.1389, Train: 100.00%, Valid: 71.00% Test: 72.10%
Split: 01, Run: 06, Epoch: 300, Loss: 0.2004, Train: 100.00%, Valid: 54.60% Test: 55.90%
Split: 01, Run: 08, Epoch: 100, Loss: 0.2739, Train: 100.00%, Valid: 58.40% Test: 58.00%
Split: 01, Run: 05, Epoch: 200, Loss: 0.0788, Train: 100.00%, Valid: 70.40% Test: 71.10%
Split: 01, Run: 06, Epoch: 400, Loss: 0.1437, Train: 100.00%, Valid: 54.40% Test: 55.80%
Split: 01, Run: 08, Epoch: 200, Loss: 0.2445, Train: 100.00%, Valid: 57.80% Test: 57.70%
Split: 01, Run: 05, Epoch: 300, Loss: 0.0715, Train: 100.00%, Valid: 67.40% Test: 70.20%
Split: 01, Run: 06, Epoch: 500, Loss: 0.1265, Train: 100.00%, Valid: 52.80% Test: 54.80%
Split: 01, Run: 06
None time:  8.62305745948106
None Run 06:
Highest Train: 100.00
Highest Valid: 56.80
  Final Train: 100.00
   Final Test: 56.90
Split: 01, Run: 05, Epoch: 400, Loss: 0.0803, Train: 100.00%, Valid: 69.20% Test: 68.70%
Split: 01, Run: 08, Epoch: 300, Loss: 0.2043, Train: 100.00%, Valid: 56.80% Test: 56.50%
Split: 01, Run: 07, Epoch: 100, Loss: 0.2479, Train: 100.00%, Valid: 55.60% Test: 56.20%
Split: 01, Run: 05, Epoch: 500, Loss: 0.0638, Train: 100.00%, Valid: 69.40% Test: 70.90%
Split: 01, Run: 05
None time:  8.615860358811915
None Run 05:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 71.60
Split: 01, Run: 08, Epoch: 400, Loss: 0.2006, Train: 100.00%, Valid: 57.00% Test: 56.00%
Split: 01, Run: 07, Epoch: 200, Loss: 0.1965, Train: 100.00%, Valid: 52.80% Test: 56.30%
Split: 01, Run: 06, Epoch: 100, Loss: 0.1145, Train: 100.00%, Valid: 72.20% Test: 72.80%
Split: 01, Run: 08, Epoch: 500, Loss: 0.2282, Train: 100.00%, Valid: 55.40% Test: 55.30%
Split: 01, Run: 08
None time:  8.893616709858179
None Run 08:
Highest Train: 100.00
Highest Valid: 62.00
  Final Train: 100.00
   Final Test: 58.40
Split: 01, Run: 07, Epoch: 300, Loss: 0.1459, Train: 100.00%, Valid: 51.80% Test: 53.80%
Split: 01, Run: 06, Epoch: 200, Loss: 0.1000, Train: 100.00%, Valid: 71.60% Test: 74.00%
Split: 01, Run: 09, Epoch: 100, Loss: 0.3034, Train: 100.00%, Valid: 61.00% Test: 59.20%
Split: 01, Run: 07, Epoch: 400, Loss: 0.1322, Train: 100.00%, Valid: 54.80% Test: 56.50%
Split: 01, Run: 06, Epoch: 300, Loss: 0.1109, Train: 100.00%, Valid: 67.40% Test: 69.10%
Split: 01, Run: 09, Epoch: 200, Loss: 0.2102, Train: 100.00%, Valid: 57.80% Test: 57.00%
Split: 01, Run: 07, Epoch: 500, Loss: 0.1285, Train: 100.00%, Valid: 56.00% Test: 56.80%
Split: 01, Run: 07
None time:  8.58053170517087
None Run 07:
Highest Train: 100.00
Highest Valid: 57.20
  Final Train: 100.00
   Final Test: 54.40
Split: 01, Run: 06, Epoch: 400, Loss: 0.0575, Train: 100.00%, Valid: 70.00% Test: 70.50%
Split: 01, Run: 09, Epoch: 300, Loss: 0.1905, Train: 100.00%, Valid: 57.60% Test: 55.80%
Split: 01, Run: 08, Epoch: 100, Loss: 0.2946, Train: 100.00%, Valid: 52.40% Test: 56.30%
Split: 01, Run: 06, Epoch: 500, Loss: 0.0773, Train: 100.00%, Valid: 71.60% Test: 73.20%
Split: 01, Run: 06
None time:  8.597571138292551
None Run 06:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 73.10
Split: 01, Run: 09, Epoch: 400, Loss: 0.1602, Train: 100.00%, Valid: 58.40% Test: 56.90%
Split: 01, Run: 08, Epoch: 200, Loss: 0.2180, Train: 100.00%, Valid: 53.60% Test: 56.90%
Split: 01, Run: 07, Epoch: 100, Loss: 0.1246, Train: 100.00%, Valid: 71.40% Test: 72.20%
Split: 01, Run: 09, Epoch: 500, Loss: 0.1721, Train: 100.00%, Valid: 58.20% Test: 56.30%
Split: 01, Run: 09
None time:  8.848004947416484
None Run 09:
Highest Train: 100.00
Highest Valid: 62.20
  Final Train: 100.00
   Final Test: 58.60
Split: 01, Run: 08, Epoch: 300, Loss: 0.1588, Train: 100.00%, Valid: 54.00% Test: 56.60%
Split: 01, Run: 07, Epoch: 200, Loss: 0.1074, Train: 100.00%, Valid: 70.00% Test: 72.70%
Split: 01, Run: 10, Epoch: 100, Loss: 0.2745, Train: 100.00%, Valid: 59.40% Test: 57.40%
Split: 01, Run: 08, Epoch: 400, Loss: 0.1242, Train: 100.00%, Valid: 52.80% Test: 54.40%
Split: 01, Run: 07, Epoch: 300, Loss: 0.0873, Train: 100.00%, Valid: 68.80% Test: 70.70%
Split: 01, Run: 10, Epoch: 200, Loss: 0.2234, Train: 100.00%, Valid: 56.20% Test: 56.00%
Split: 01, Run: 08, Epoch: 500, Loss: 0.1691, Train: 100.00%, Valid: 52.60% Test: 53.80%
Split: 01, Run: 08
None time:  8.542876181192696
None Run 08:
Highest Train: 100.00
Highest Valid: 56.80
  Final Train: 100.00
   Final Test: 56.60
Split: 01, Run: 07, Epoch: 400, Loss: 0.0786, Train: 100.00%, Valid: 70.80% Test: 71.60%
Split: 01, Run: 10, Epoch: 300, Loss: 0.2188, Train: 100.00%, Valid: 57.60% Test: 57.50%
Split: 01, Run: 09, Epoch: 100, Loss: 0.2528, Train: 100.00%, Valid: 56.20% Test: 57.40%
Split: 01, Run: 07, Epoch: 500, Loss: 0.0562, Train: 100.00%, Valid: 70.60% Test: 72.30%
Split: 01, Run: 07
None time:  8.577568757347763
None Run 07:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 72.40
Split: 01, Run: 10, Epoch: 400, Loss: 0.2019, Train: 100.00%, Valid: 58.00% Test: 55.50%
Split: 01, Run: 09, Epoch: 200, Loss: 0.1921, Train: 100.00%, Valid: 53.40% Test: 55.20%
Split: 01, Run: 08, Epoch: 100, Loss: 0.1262, Train: 100.00%, Valid: 72.60% Test: 72.20%
Split: 01, Run: 10, Epoch: 500, Loss: 0.2134, Train: 100.00%, Valid: 55.80% Test: 56.60%
Split: 01, Run: 10
None time:  8.776256216689944
None Run 10:
Highest Train: 100.00
Highest Valid: 62.20
  Final Train: 100.00
   Final Test: 58.00
total time:  80.81371772941202
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.06 Â± 0.98
  Final Train: 100.00 Â± 0.00
   Final Test: 58.32 Â± 1.30
[32m[I 2021-07-18 21:19:53,732][0m Trial 0 finished with value: 62.06000518798828 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 62.06000518798828.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  62.06    {'train': '100.00 Â± 0.00', 'valid': '62.06 Â± 0.98', 'test': '58.32 Â± 1.30'}
test_acc
['58.32 Â± 1.30']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  62.06000518798828
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.06 Â± 0.98', 'test': '58.32 Â± 1.30'}
optuna total time:  80.83132497128099
Split: 01, Run: 09, Epoch: 300, Loss: 0.1687, Train: 100.00%, Valid: 55.00% Test: 55.40%
Split: 01, Run: 08, Epoch: 200, Loss: 0.0933, Train: 100.00%, Valid: 69.40% Test: 70.90%
Split: 01, Run: 09, Epoch: 400, Loss: 0.1453, Train: 100.00%, Valid: 53.40% Test: 54.90%
Split: 01, Run: 08, Epoch: 300, Loss: 0.0530, Train: 100.00%, Valid: 69.80% Test: 71.30%
Split: 01, Run: 09, Epoch: 500, Loss: 0.1365, Train: 100.00%, Valid: 55.40% Test: 55.70%
Split: 01, Run: 09
None time:  7.843457493931055
None Run 09:
Highest Train: 100.00
Highest Valid: 58.40
  Final Train: 100.00
   Final Test: 55.40
Split: 01, Run: 08, Epoch: 400, Loss: 0.0386, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 10, Epoch: 100, Loss: 0.2743, Train: 100.00%, Valid: 56.80% Test: 57.30%
Split: 01, Run: 08, Epoch: 500, Loss: 0.0569, Train: 100.00%, Valid: 71.80% Test: 73.20%
Split: 01, Run: 08
None time:  7.375794131308794
None Run 08:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 72.20
Split: 01, Run: 10, Epoch: 200, Loss: 0.1899, Train: 100.00%, Valid: 55.60% Test: 55.80%
Split: 01, Run: 09, Epoch: 100, Loss: 0.0991, Train: 100.00%, Valid: 70.40% Test: 71.40%
Split: 01, Run: 10, Epoch: 300, Loss: 0.1737, Train: 100.00%, Valid: 49.80% Test: 52.10%
Split: 01, Run: 09, Epoch: 200, Loss: 0.0856, Train: 100.00%, Valid: 71.20% Test: 71.50%
Split: 01, Run: 10, Epoch: 400, Loss: 0.1696, Train: 100.00%, Valid: 51.20% Test: 52.40%
Split: 01, Run: 09, Epoch: 300, Loss: 0.0727, Train: 100.00%, Valid: 70.80% Test: 71.70%
Split: 01, Run: 10, Epoch: 500, Loss: 0.1169, Train: 100.00%, Valid: 54.40% Test: 56.00%
Split: 01, Run: 10
None time:  11.646065670996904
None Run 10:
Highest Train: 100.00
Highest Valid: 57.60
  Final Train: 100.00
   Final Test: 57.60
total time:  88.3452272657305
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 57.80 Â± 0.68
  Final Train: 100.00 Â± 0.00
   Final Test: 56.40 Â± 1.17
[32m[I 2021-07-18 21:20:08,713][0m Trial 0 finished with value: 57.79999923706055 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 57.79999923706055.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  57.8    {'train': '100.00 Â± 0.00', 'valid': '57.80 Â± 0.68', 'test': '56.40 Â± 1.17'}
test_acc
['56.40 Â± 1.17']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  57.79999923706055
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '57.80 Â± 0.68', 'test': '56.40 Â± 1.17'}
optuna total time:  88.366385711357
Split: 01, Run: 09, Epoch: 400, Loss: 0.0590, Train: 100.00%, Valid: 69.20% Test: 69.60%
Split: 01, Run: 09, Epoch: 500, Loss: 0.0524, Train: 100.00%, Valid: 70.80% Test: 71.20%
Split: 01, Run: 09
None time:  11.194531456567347
None Run 09:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 73.20
Split: 01, Run: 10, Epoch: 100, Loss: 0.1296, Train: 100.00%, Valid: 71.40% Test: 71.70%
Split: 01, Run: 10, Epoch: 200, Loss: 0.0974, Train: 100.00%, Valid: 71.00% Test: 71.00%
Split: 01, Run: 10, Epoch: 300, Loss: 0.0479, Train: 100.00%, Valid: 68.60% Test: 71.30%
Split: 01, Run: 10, Epoch: 400, Loss: 0.0748, Train: 100.00%, Valid: 69.80% Test: 71.40%
Split: 01, Run: 10, Epoch: 500, Loss: 0.0851, Train: 100.00%, Valid: 71.00% Test: 71.40%
Split: 01, Run: 10
None time:  4.957412213087082
None Run 10:
Highest Train: 100.00
Highest Valid: 73.20
  Final Train: 100.00
   Final Test: 73.40
total time:  87.45244499482214
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 73.16 Â± 0.30
  Final Train: 100.00 Â± 0.00
   Final Test: 72.68 Â± 0.59
[32m[I 2021-07-18 21:20:14,954][0m Trial 0 finished with value: 73.16000366210938 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 73.16000366210938.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  73.16    {'train': '100.00 Â± 0.00', 'valid': '73.16 Â± 0.30', 'test': '72.68 Â± 0.59'}
test_acc
['72.68 Â± 0.59']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  73.16000366210938
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '73.16 Â± 0.30', 'test': '72.68 Â± 0.59'}
optuna total time:  87.47149822954088
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
main:  main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:24:26,203][0m A new study created in memory with name: no-name-0f1aeafb-78ee-45d2-a60a-5f4ea5e539e0[0m
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:24:26,203][0m A new study created in memory with name: no-name-6c184242-194c-407d-98fc-d5296819597b[0m
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:24:26,203][0m A new study created in memory with name: no-name-8c93839a-b765-489c-833a-dd077d439520[0m
K:  100
alpha:  None
lr:  K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
[33m[W 2021-07-18 21:24:28,512][0m Trial 0 failed because of the following error: NameError("name 'self' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined
[33m[W 2021-07-18 21:24:28,514][0m Trial 0 failed because of the following error: NameError("name 'self' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined[0m
[33m[W 2021-07-18 21:24:28,514][0m Trial 0 failed because of the following error: NameError("name 'self' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:25:36,905][0m A new study created in memory with name: no-name-8de58a12-f3fe-416e-88bc-bb4e38145c6a[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
[33m[W 2021-07-18 21:25:38,829][0m Trial 0 failed because of the following error: NameError("name 'self' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 15, in train
    if self.args.loss == 'CE':
NameError: name 'self' is not defined
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:26:04,942][0m A new study created in memory with name: no-name-2573cd0c-4fd0-40f4-87e4-dd89a52ec8e5[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Using backend: pytorch
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
[33m[W 2021-07-18 21:26:06,908][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:26:06,969][0m A new study created in memory with name: no-name-2edfd967-ff72-483b-b918-d9bf3fdb573d[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:26:07,649][0m A new study created in memory with name: no-name-f4c7eb34-f841-435b-bbbf-9e8621a4a148[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
[33m[W 2021-07-18 21:26:09,013][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
[33m[W 2021-07-18 21:26:09,714][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:27:30,752][0m A new study created in memory with name: no-name-ed7b3c60-baf9-4812-9bac-21578941192e[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(12)[0;36mtrain[0;34m()[0m
[0;32m     11 [0;31m    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 12 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     13 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m[[0m[0mtrain_idx[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[ 1.0788e-01, -5.0558e-03, -8.7890e-02, -1.1618e-01, -9.3768e-02,
          1.2137e-01, -1.1357e-01],
        [ 1.0417e-01, -9.7365e-03, -9.1494e-02, -1.0383e-01, -1.1050e-01,
          1.2337e-01, -1.0767e-01],
        [ 1.0394e-01, -3.7027e-03, -7.3466e-02, -1.0403e-01, -9.8216e-02,
          1.2542e-01, -1.0775e-01],
        [ 1.1240e-01, -6.9020e-03, -8.9741e-02, -1.1045e-01, -1.0229e-01,
          1.1875e-01, -1.1260e-01],
        [ 1.0299e-01, -6.1800e-03, -8.4843e-02, -1.1070e-01, -9.8654e-02,
          1.1051e-01, -1.1197e-01],
        [ 1.1585e-01, -1.1145e-03, -8.5027e-02, -1.0028e-01, -1.0256e-01,
          1.1976e-01, -1.1381e-01],
        [ 9.9152e-02, -5.3497e-03, -8.0912e-02, -1.1142e-01, -1.0500e-01,
          1.1708e-01, -1.0970e-01],
        [ 1.0875e-01,  3.3872e-03, -9.0322e-02, -1.0322e-01, -1.0796e-01,
          1.3322e-01, -1.0713e-01],
        [ 1.1525e-01,  2.3224e-03, -8.9209e-02, -1.1453e-01, -1.1037e-01,
          1.1395e-01, -1.1062e-01],
        [ 1.1078e-01, -1.3246e-02, -1.0925e-01, -1.1195e-01, -1.2868e-01,
          1.0546e-01, -1.2093e-01],
        [ 1.1492e-01,  4.4664e-03, -9.1210e-02, -1.0701e-01, -9.8658e-02,
          1.2031e-01, -1.0512e-01],
        [ 1.0513e-01,  7.5185e-04, -8.8339e-02, -1.1543e-01, -1.1415e-01,
          1.1084e-01, -1.0429e-01],
        [ 9.9092e-02, -1.5394e-02, -6.9991e-02, -1.2702e-01, -1.0027e-01,
          1.1115e-01, -1.1364e-01],
        [ 1.1088e-01,  2.9226e-04, -6.2595e-02, -1.1400e-01, -9.7771e-02,
          1.2133e-01, -1.0575e-01],
        [ 1.0907e-01, -2.3871e-03, -9.0908e-02, -1.1128e-01, -1.0971e-01,
          1.1625e-01, -1.0962e-01],
        [ 1.0649e-01, -2.9481e-03, -8.7138e-02, -1.1193e-01, -1.0248e-01,
          1.2066e-01, -1.0117e-01],
        [ 9.8293e-02, -1.5709e-02, -9.0198e-02, -1.1595e-01, -1.2011e-01,
          1.0967e-01, -1.0538e-01],
        [ 1.0173e-01,  3.4858e-05, -8.1376e-02, -1.1606e-01, -1.0197e-01,
          1.2914e-01, -9.0464e-02],
        [ 1.2490e-01, -1.5289e-02, -8.0281e-02, -1.0401e-01, -1.1176e-01,
          1.0410e-01, -1.1719e-01],
        [ 1.0081e-01,  1.7601e-03, -7.1079e-02, -1.0404e-01, -9.8896e-02,
          1.3199e-01, -9.1980e-02],
        [ 9.9731e-02,  5.9590e-03, -7.6606e-02, -1.1679e-01, -1.0339e-01,
          1.3197e-01, -9.3402e-02],
        [ 1.1902e-01, -9.1585e-03, -7.7619e-02, -1.1899e-01, -1.0905e-01,
          1.1184e-01, -9.6586e-02],
        [ 1.0427e-01, -3.3311e-03, -8.3748e-02, -1.1712e-01, -1.1615e-01,
          1.1525e-01, -1.0979e-01],
        [ 1.1994e-01, -7.0627e-03, -8.1185e-02, -9.9373e-02, -9.8439e-02,
          1.2064e-01, -1.0528e-01],
        [ 1.1088e-01, -1.4903e-02, -7.7951e-02, -1.0781e-01, -1.1278e-01,
          1.0767e-01, -1.1193e-01],
        [ 1.0958e-01, -1.3640e-03, -8.6770e-02, -1.0330e-01, -1.1295e-01,
          1.1950e-01, -9.5972e-02],
        [ 1.0835e-01, -1.3337e-02, -8.4887e-02, -1.0537e-01, -1.0050e-01,
          1.2064e-01, -1.0419e-01],
        [ 1.0504e-01, -1.5642e-02, -7.2383e-02, -1.0648e-01, -9.7634e-02,
          1.2893e-01, -1.0361e-01],
        [ 1.1628e-01, -9.0695e-03, -8.9280e-02, -1.0662e-01, -1.0251e-01,
          1.1729e-01, -1.1091e-01],
        [ 1.0544e-01,  1.0212e-03, -8.1694e-02, -1.1247e-01, -1.0481e-01,
          1.1791e-01, -1.0348e-01],
        [ 1.2078e-01, -6.9521e-03, -9.2762e-02, -1.0977e-01, -1.0729e-01,
          1.1640e-01, -9.9501e-02],
        [ 1.0213e-01, -4.5860e-03, -9.3918e-02, -1.0524e-01, -1.0831e-01,
          1.2898e-01, -1.0622e-01],
        [ 9.9153e-02, -1.4985e-02, -8.5411e-02, -1.1614e-01, -1.0020e-01,
          1.2171e-01, -1.0359e-01],
        [ 1.0871e-01, -2.6124e-03, -8.6407e-02, -1.0812e-01, -1.1708e-01,
          1.1630e-01, -9.3472e-02],
        [ 9.5761e-02, -4.5314e-03, -8.1227e-02, -1.1785e-01, -1.1321e-01,
          1.2747e-01, -1.0381e-01],
        [ 1.0154e-01, -1.1235e-02, -8.2314e-02, -1.1016e-01, -9.2960e-02,
          1.2875e-01, -1.0345e-01],
        [ 1.0666e-01, -1.4756e-02, -8.3565e-02, -1.0612e-01, -1.0659e-01,
          1.2359e-01, -1.0543e-01],
        [ 1.0227e-01, -1.4153e-02, -8.8803e-02, -1.0499e-01, -1.1016e-01,
          1.2198e-01, -1.0708e-01],
        [ 9.8522e-02, -1.9044e-02, -7.9770e-02, -1.1495e-01, -1.0730e-01,
          1.1948e-01, -1.0172e-01],
        [ 1.1584e-01, -1.1408e-02, -9.4192e-02, -1.1321e-01, -1.0841e-01,
          1.1159e-01, -1.0530e-01],
        [ 1.0632e-01, -1.4978e-03, -9.6128e-02, -9.9657e-02, -1.0263e-01,
          1.1462e-01, -1.0320e-01],
        [ 1.1318e-01, -2.1896e-02, -8.1665e-02, -1.1301e-01, -1.0196e-01,
          1.2339e-01, -1.0602e-01],
        [ 1.0659e-01, -3.8160e-03, -8.0018e-02, -1.1956e-01, -9.7064e-02,
          1.1642e-01, -1.1077e-01],
        [ 1.0129e-01, -8.4470e-03, -8.5744e-02, -1.1549e-01, -1.0717e-01,
          1.1969e-01, -1.0597e-01],
        [ 1.0951e-01, -1.0895e-02, -9.1909e-02, -1.1567e-01, -1.1080e-01,
          1.1623e-01, -1.0429e-01],
        [ 1.0169e-01, -7.7754e-03, -8.2312e-02, -1.1632e-01, -9.8723e-02,
          1.2663e-01, -1.0317e-01],
        [ 1.1050e-01, -9.5106e-03, -8.6480e-02, -1.0756e-01, -1.0562e-01,
          1.2139e-01, -8.9263e-02],
        [ 1.0492e-01, -4.2836e-03, -9.2552e-02, -1.0928e-01, -1.1726e-01,
          1.1402e-01, -1.1814e-01],
        [ 1.1144e-01, -2.5353e-03, -7.7512e-02, -1.1782e-01, -1.1212e-01,
          1.0747e-01, -1.0550e-01],
        [ 1.2022e-01, -2.2295e-03, -8.0313e-02, -1.0837e-01, -1.1831e-01,
          1.0937e-01, -1.0214e-01],
        [ 1.2084e-01,  2.1949e-03, -8.5117e-02, -1.1668e-01, -1.0723e-01,
          1.1313e-01, -1.0815e-01],
        [ 1.1810e-01,  5.2827e-03, -9.6255e-02, -1.0544e-01, -1.1438e-01,
          1.1774e-01, -9.6699e-02],
        [ 1.0746e-01, -1.1367e-02, -8.3195e-02, -1.1504e-01, -1.1079e-01,
          1.2426e-01, -1.0827e-01],
        [ 1.0797e-01, -9.9216e-03, -8.1205e-02, -1.0845e-01, -9.2687e-02,
          1.3077e-01, -1.0107e-01],
        [ 1.0104e-01, -1.9397e-03, -8.3107e-02, -1.2173e-01, -1.0204e-01,
          1.2408e-01, -1.0178e-01],
        [ 1.1766e-01, -5.7819e-03, -7.8149e-02, -1.1726e-01, -1.0694e-01,
          1.1788e-01, -1.1243e-01],
        [ 1.1974e-01, -7.9782e-03, -7.9711e-02, -1.1088e-01, -1.0597e-01,
          1.2424e-01, -1.0321e-01],
        [ 1.1339e-01, -3.1760e-03, -7.6297e-02, -1.1197e-01, -1.0739e-01,
          1.1132e-01, -1.0413e-01],
        [ 1.0931e-01, -8.9494e-03, -7.7276e-02, -1.1316e-01, -1.0345e-01,
          1.0605e-01, -1.0266e-01],
        [ 1.0408e-01, -5.9329e-03, -8.4814e-02, -1.1153e-01, -1.0667e-01,
          1.1585e-01, -9.7083e-02],
        [ 1.0862e-01, -4.1568e-03, -8.6936e-02, -1.1722e-01, -1.1474e-01,
          1.1294e-01, -1.0621e-01],
        [ 9.0188e-02, -9.9184e-03, -8.3371e-02, -1.1501e-01, -1.1022e-01,
          1.2359e-01, -1.0283e-01],
        [ 9.8037e-02, -3.9024e-03, -8.6829e-02, -1.1007e-01, -9.4081e-02,
          1.1744e-01, -1.0218e-01],
        [ 1.0643e-01, -1.4653e-03, -8.4469e-02, -1.0597e-01, -1.0983e-01,
          1.2232e-01, -9.1633e-02],
        [ 1.0026e-01, -5.5772e-03, -7.6918e-02, -1.2274e-01, -1.0658e-01,
          1.0174e-01, -1.1389e-01],
        [ 1.2505e-01, -1.6439e-04, -8.8551e-02, -1.0232e-01, -1.0090e-01,
          1.1112e-01, -1.1567e-01],
        [ 1.0195e-01,  5.1033e-03, -7.7732e-02, -1.1248e-01, -1.0995e-01,
          1.1868e-01, -1.0862e-01],
        [ 1.0433e-01,  1.8240e-03, -9.9240e-02, -1.1050e-01, -1.0247e-01,
          1.2931e-01, -1.0408e-01],
        [ 1.0440e-01, -5.1389e-03, -8.8252e-02, -1.1042e-01, -1.1351e-01,
          1.2488e-01, -9.1565e-02],
        [ 9.1336e-02, -3.8302e-03, -9.0689e-02, -1.1659e-01, -9.7786e-02,
          1.2511e-01, -1.0166e-01],
        [ 1.0665e-01,  1.0025e-02, -9.8264e-02, -1.0735e-01, -1.0887e-01,
          1.0971e-01, -1.0997e-01],
        [ 1.1399e-01, -4.7155e-03, -8.7851e-02, -1.0755e-01, -1.1608e-01,
          1.1023e-01, -1.0314e-01],
        [ 1.0263e-01, -8.7505e-04, -8.7705e-02, -1.0640e-01, -1.0489e-01,
          1.1901e-01, -1.1705e-01],
        [ 9.6619e-02,  4.9050e-03, -8.8375e-02, -1.1674e-01, -1.0388e-01,
          1.2888e-01, -1.0308e-01],
        [ 1.1358e-01, -1.3209e-02, -7.7874e-02, -1.0571e-01, -1.0739e-01,
          1.2416e-01, -8.8940e-02],
        [ 9.3952e-02,  8.1803e-03, -8.3134e-02, -1.0214e-01, -1.0025e-01,
          1.2719e-01, -1.0085e-01],
        [ 1.0878e-01,  5.7815e-03, -9.0821e-02, -1.0733e-01, -1.1402e-01,
          1.2083e-01, -9.5245e-02],
        [ 1.2233e-01, -8.6568e-03, -8.9097e-02, -1.1563e-01, -1.2199e-01,
          1.1431e-01, -1.0402e-01],
        [ 9.4312e-02, -7.2914e-03, -9.4178e-02, -1.1334e-01, -9.3354e-02,
          1.1876e-01, -1.1121e-01],
        [ 1.1370e-01, -5.3449e-03, -7.8102e-02, -1.1144e-01, -1.0288e-01,
          1.0779e-01, -1.1613e-01],
        [ 1.1618e-01, -1.0086e-02, -8.8887e-02, -1.1390e-01, -1.1030e-01,
          1.1915e-01, -1.1447e-01],
        [ 1.1346e-01, -6.1597e-03, -8.9456e-02, -1.0885e-01, -1.0465e-01,
          1.2739e-01, -1.0352e-01],
        [ 1.2277e-01,  2.0637e-04, -9.1357e-02, -1.0537e-01, -1.0648e-01,
          1.2301e-01, -1.1201e-01],
        [ 1.1035e-01, -1.1084e-02, -9.0066e-02, -1.1653e-01, -9.4381e-02,
          1.0168e-01, -1.2266e-01],
        [ 1.2127e-01,  5.4109e-03, -8.9933e-02, -1.1348e-01, -1.1792e-01,
          1.0883e-01, -1.0628e-01],
        [ 1.0283e-01, -1.1326e-02, -7.7658e-02, -1.1989e-01, -1.0912e-01,
          1.0792e-01, -1.1554e-01],
        [ 1.0964e-01, -3.2350e-03, -8.0314e-02, -1.1383e-01, -9.5185e-02,
          1.0945e-01, -1.0524e-01],
        [ 1.0840e-01,  7.3846e-03, -8.7374e-02, -1.0593e-01, -1.0515e-01,
          1.2334e-01, -9.7854e-02],
        [ 1.0847e-01, -1.6435e-02, -9.7670e-02, -1.0569e-01, -1.0905e-01,
          1.1247e-01, -1.1133e-01],
        [ 1.1685e-01,  4.5533e-03, -9.5808e-02, -1.0834e-01, -1.1016e-01,
          1.2028e-01, -9.3646e-02],
        [ 1.0903e-01, -4.1962e-03, -9.9303e-02, -1.0684e-01, -1.0740e-01,
          1.0784e-01, -1.2015e-01],
        [ 1.0381e-01,  7.0263e-03, -8.5641e-02, -1.0925e-01, -9.8064e-02,
          1.2875e-01, -9.1738e-02],
        [ 1.0753e-01, -4.4256e-03, -9.8273e-02, -1.1210e-01, -9.8016e-02,
          1.0706e-01, -9.9724e-02],
        [ 1.0283e-01, -6.5035e-03, -8.0044e-02, -1.1226e-01, -1.2025e-01,
          1.1320e-01, -1.0306e-01],
        [ 1.1682e-01, -7.8425e-04, -9.1427e-02, -1.0879e-01, -1.0859e-01,
          1.1721e-01, -9.5010e-02],
        [ 1.0642e-01, -4.9585e-03, -7.9452e-02, -1.0749e-01, -9.6561e-02,
          1.2100e-01, -1.0774e-01],
        [ 1.1961e-01, -9.6622e-03, -8.6151e-02, -1.1387e-01, -1.0642e-01,
          1.2487e-01, -1.1047e-01],
        [ 1.1117e-01, -3.8309e-03, -8.7749e-02, -1.1220e-01, -1.1045e-01,
          1.1476e-01, -1.0637e-01],
        [ 1.0918e-01,  5.7112e-04, -9.2477e-02, -1.1421e-01, -1.1575e-01,
          1.1379e-01, -1.0421e-01],
        [ 9.6861e-02, -6.4229e-03, -9.8469e-02, -1.0951e-01, -9.4896e-02,
          1.1490e-01, -1.0959e-01],
        [ 1.2445e-01, -3.7049e-03, -8.0076e-02, -1.0702e-01, -1.1800e-01,
          1.1214e-01, -1.0963e-01],
        [ 1.0486e-01, -2.8031e-03, -7.6665e-02, -1.1036e-01, -9.5077e-02,
          1.1847e-01, -1.0687e-01],
        [ 1.1384e-01, -8.9908e-03, -7.6098e-02, -1.0636e-01, -9.4930e-02,
          1.2799e-01, -1.0918e-01],
        [ 1.0371e-01, -7.4516e-03, -8.2390e-02, -1.0968e-01, -9.9509e-02,
          1.2209e-01, -1.1415e-01],
        [ 1.0912e-01, -8.4136e-04, -9.0307e-02, -1.1864e-01, -1.1633e-01,
          1.0505e-01, -1.0828e-01],
        [ 1.0641e-01,  6.1916e-03, -9.0634e-02, -1.1753e-01, -1.0648e-01,
          1.0140e-01, -1.0952e-01],
        [ 1.2295e-01, -6.9323e-03, -8.1629e-02, -1.1113e-01, -1.1789e-01,
          1.1227e-01, -1.0602e-01],
        [ 1.0451e-01, -1.5684e-02, -9.6743e-02, -1.1723e-01, -1.0428e-01,
          1.1936e-01, -1.0710e-01],
        [ 1.0787e-01, -2.0614e-03, -8.3240e-02, -1.0951e-01, -1.0567e-01,
          1.1239e-01, -1.0326e-01],
        [ 1.1100e-01, -1.5754e-02, -7.9654e-02, -1.1074e-01, -9.9544e-02,
          1.2373e-01, -1.0886e-01],
        [ 1.0757e-01, -6.1965e-03, -9.6387e-02, -1.1128e-01, -9.6259e-02,
          1.1640e-01, -1.0703e-01],
        [ 1.1198e-01, -6.9830e-03, -8.4130e-02, -1.0714e-01, -1.0311e-01,
          1.2148e-01, -1.0014e-01],
        [ 9.5126e-02, -6.0514e-03, -8.6236e-02, -1.1663e-01, -1.0588e-01,
          1.2349e-01, -1.1027e-01],
        [ 1.1366e-01,  3.7679e-04, -8.6248e-02, -1.1160e-01, -1.1589e-01,
          1.1540e-01, -1.0632e-01],
        [ 1.0331e-01, -4.4823e-03, -8.3458e-02, -1.1060e-01, -1.0819e-01,
          1.1914e-01, -9.1845e-02],
        [ 1.0171e-01, -6.5229e-03, -9.1930e-02, -1.1908e-01, -1.0495e-01,
          1.1732e-01, -1.0511e-01],
        [ 1.1415e-01, -1.6625e-02, -8.0675e-02, -1.0926e-01, -1.0723e-01,
          1.0995e-01, -1.0384e-01],
        [ 9.7089e-02, -6.3715e-03, -9.5165e-02, -1.1239e-01, -1.1161e-01,
          1.1791e-01, -1.0365e-01],
        [ 1.0464e-01,  6.9682e-03, -9.8665e-02, -1.0339e-01, -1.2545e-01,
          1.1839e-01, -9.7781e-02],
        [ 1.0434e-01, -7.9558e-03, -9.0549e-02, -1.1688e-01, -1.0737e-01,
          1.2053e-01, -1.0588e-01],
        [ 1.1536e-01,  2.0210e-05, -7.8318e-02, -1.2008e-01, -1.0046e-01,
          1.2254e-01, -1.0368e-01],
        [ 1.0123e-01,  3.2478e-03, -8.3429e-02, -1.1420e-01, -1.0351e-01,
          1.1038e-01, -1.0649e-01],
        [ 1.1182e-01, -6.2378e-03, -8.7749e-02, -1.0655e-01, -1.0555e-01,
          1.1759e-01, -1.1585e-01],
        [ 1.1145e-01, -7.7673e-03, -8.7480e-02, -1.1203e-01, -9.6156e-02,
          1.1982e-01, -1.0763e-01],
        [ 1.0357e-01,  1.0103e-03, -9.2547e-02, -1.0640e-01, -1.0064e-01,
          1.0917e-01, -1.0972e-01],
        [ 1.1604e-01, -5.3290e-03, -9.1367e-02, -1.1040e-01, -1.0969e-01,
          1.1796e-01, -1.1838e-01],
        [ 1.0261e-01, -6.5694e-03, -8.4823e-02, -1.1930e-01, -1.0817e-01,
          1.1784e-01, -1.2092e-01],
        [ 1.1509e-01,  5.9344e-03, -8.9105e-02, -1.1211e-01, -1.0381e-01,
          1.2026e-01, -1.0222e-01],
        [ 1.0912e-01, -2.5999e-03, -8.0828e-02, -1.1405e-01, -1.1096e-01,
          1.2150e-01, -1.0460e-01],
        [ 9.7319e-02, -1.3599e-04, -8.2082e-02, -1.2192e-01, -1.1326e-01,
          1.2848e-01, -1.1164e-01],
        [ 1.0501e-01, -7.0049e-03, -8.7977e-02, -1.1344e-01, -1.1541e-01,
          1.1435e-01, -9.9231e-02],
        [ 1.0892e-01, -1.9516e-02, -7.6904e-02, -1.0716e-01, -9.6222e-02,
          1.2508e-01, -9.8371e-02],
        [ 1.1066e-01, -1.1025e-02, -7.8792e-02, -1.0753e-01, -1.0558e-01,
          1.2423e-01, -1.0350e-01],
        [ 1.1812e-01, -2.4904e-03, -8.7921e-02, -1.1396e-01, -1.0927e-01,
          1.1148e-01, -1.0698e-01],
        [ 9.9889e-02,  2.5350e-03, -8.8069e-02, -1.1158e-01, -1.0808e-01,
          1.2681e-01, -9.7499e-02],
        [ 1.1317e-01,  4.5784e-03, -8.7814e-02, -1.0878e-01, -1.0887e-01,
          1.2251e-01, -8.7683e-02],
        [ 1.0821e-01, -1.4334e-02, -8.2497e-02, -1.1279e-01, -1.1922e-01,
          1.1203e-01, -1.1563e-01],
        [ 1.0911e-01,  1.7755e-04, -7.1294e-02, -1.1731e-01, -9.4017e-02,
          1.1953e-01, -1.1069e-01],
        [ 9.5208e-02, -2.9653e-03, -8.5350e-02, -1.1155e-01, -1.0762e-01,
          1.1550e-01, -1.0494e-01],
        [ 1.0466e-01, -4.8112e-03, -8.6831e-02, -1.0845e-01, -9.6182e-02,
          1.1831e-01, -1.1163e-01]], device='cuda:0', grad_fn=<IndexBackward>)
ipdb> torch.Size([140, 7])
ipdb> tensor([3, 4, 4,  ..., 3, 3, 3], device='cuda:0')
ipdb> torch.Size([2708])
ipdb> [33m[W 2021-07-18 21:29:52,680][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 12, in train
    if len(data.y.shape) == 1:
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 12, in train
    if len(data.y.shape) == 1:
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Traceback (most recent call last):
  File "main_optuna.py", line 9, in <module>
    from train_eval import train, test
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 21
    labels[]
           ^
SyntaxError: invalid syntax
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:30:24,870][0m A new study created in memory with name: no-name-275179e5-a6e0-448e-8b40-34565773268a[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(24)[0;36mtrain[0;34m()[0m

ipdb> tensor([[0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> torch.Size([140, 7])
ipdb> tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2, 0, 0, 4, 3, 3, 3, 2, 3, 1, 3, 5, 3, 4, 6,
        3, 3, 6, 3, 2, 4, 3, 6, 0, 4, 2, 0, 1, 5, 4, 4, 3, 6, 6, 4, 3, 3, 2, 5,
        3, 4, 5, 3, 0, 2, 1, 4, 6, 3, 2, 2, 0, 0, 0, 4, 2, 0, 4, 5, 2, 6, 5, 2,
        2, 2, 0, 4, 5, 6, 4, 0, 0, 0, 4, 2, 4, 1, 4, 6, 0, 4, 2, 4, 6, 6, 0, 0,
        6, 5, 0, 6, 0, 2, 1, 1, 1, 2, 6, 5, 6, 1, 2, 2, 1, 5, 5, 5, 6, 5, 6, 5,
        5, 1, 6, 6, 1, 5, 1, 6, 5, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0')
ipdb> torch.Size([140])
ipdb> ipdb> tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')
ipdb> torch.Size([140, 7])
ipdb> torch.Size([140, 7])
ipdb> torch.Size([140])
ipdb> torch.Size([140])
ipdb> 140
ipdb> *** TypeError: range() missing 1 required positional arguments: "end"
ipdb> /mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].
tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,
         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,
         24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,
         36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,
         48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,
         60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,
         72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,
         84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,
         96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106., 107.,
        108., 109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119.,
        120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
        132., 133., 134., 135., 136., 137., 138., 139., 140.])
ipdb> *** NameError: name 'labels' is not defined
ipdb> *** IndexError: tensors used as indices must be long, byte or bool tensors
ipdb> tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2, 0, 0, 4, 3, 3, 3, 2, 3, 1, 3, 5, 3, 4, 6,
        3, 3, 6, 3, 2, 4, 3, 6, 0, 4, 2, 0, 1, 5, 4, 4, 3, 6, 6, 4, 3, 3, 2, 5,
        3, 4, 5, 3, 0, 2, 1, 4, 6, 3, 2, 2, 0, 0, 0, 4, 2, 0, 4, 5, 2, 6, 5, 2,
        2, 2, 0, 4, 5, 6, 4, 0, 0, 0, 4, 2, 4, 1, 4, 6, 0, 4, 2, 4, 6, 6, 0, 0,
        6, 5, 0, 6, 0, 2, 1, 1, 1, 2, 6, 5, 6, 1, 2, 2, 1, 5, 5, 5, 6, 5, 6, 5,
        5, 1, 6, 6, 1, 5, 1, 6, 5, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0')
ipdb> torch.int64
ipdb> tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0')
ipdb> torch.Size([140])
ipdb> ipdb> tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> [33m[W 2021-07-18 21:44:18,427][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
main:  main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:44:48,528][0m A new study created in memory with name: no-name-8bae0dd4-1e3f-4acc-ba59-bb5f4153cd64[0m
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:44:48,529][0m A new study created in memory with name: no-name-0bfc8bc0-d698-42d8-be64-e38510a3f2f4[0m
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:44:48,529][0m A new study created in memory with name: no-name-b1f9a17a-4489-49f1-9d43-cb4c2850d637[0m
K:  100
alpha:  None
lr:  K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
[33m[W 2021-07-18 21:44:50,932][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (3) must match the size of tensor b (60) at non-singleton dimension 1
[33m[W 2021-07-18 21:44:50,935][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1[0m
[33m[W 2021-07-18 21:44:50,940][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (7) must match the size of tensor b (140) at non-singleton dimension 1
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-y), 2)
RuntimeError: The size of tensor a (6) must match the size of tensor b (120) at non-singleton dimension 1
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:45:14,584][0m A new study created in memory with name: no-name-ab02653f-9762-4a39-9630-60b78fcb605c[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(24)[0;36mtrain[0;34m()[0m
[0;32m     23 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 24 [0;31m        [0mloss[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mpow[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0mnorm[0m[0;34m([0m[0mout[0m[0;34m-[0m[0my[0m[0;34m)[0m[0;34m,[0m [0;36m2[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     25 [0;31m    [0;31m# print('####### training loss: ', loss)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> *** NameError: name 'lossss' is not defined
ipdb> *** NameError: name 'losss' is not defined
ipdb> tensor(157.0308, device='cuda:0', grad_fn=<PowBackward0>)
ipdb> [33m[W 2021-07-18 21:45:49,518][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-label), 2)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 24, in train
    loss = torch.pow(torch.norm(out-label), 2)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
main:  main:  main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:46:03,771][0m A new study created in memory with name: no-name-b750d357-071b-44a9-8152-5488b8698044[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:46:03,771][0m A new study created in memory with name: no-name-7bea71c4-9e83-40be-95d1-4db327c43365[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:46:03,771][0m A new study created in memory with name: no-name-1cc7edb4-3ed3-4e14-b204-1ce59e4ec42c[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 52.20% Test: 50.10%
Split: 01, Run: 01, Epoch: 100, Loss: 20.7499, Train: 100.00%, Valid: 49.60% Test: 50.10%
Split: 01, Run: 01, Epoch: 100, Loss: 6.6659, Train: 100.00%, Valid: 69.00% Test: 69.00%
Split: 01, Run: 01, Epoch: 200, Loss: 18.6607, Train: 100.00%, Valid: 49.40% Test: 50.70%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 100.00%, Valid: 49.60% Test: 48.30%
Split: 01, Run: 01, Epoch: 200, Loss: 5.2418, Train: 100.00%, Valid: 66.00% Test: 66.80%
Split: 01, Run: 01, Epoch: 300, Loss: 18.5572, Train: 100.00%, Valid: 49.00% Test: 49.20%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 51.80% Test: 48.80%
Split: 01, Run: 01, Epoch: 300, Loss: 4.4059, Train: 100.00%, Valid: 66.60% Test: 68.00%
Split: 01, Run: 01, Epoch: 400, Loss: 20.1188, Train: 100.00%, Valid: 45.60% Test: 47.80%
Split: 01, Run: 01, Epoch: 400, Loss: 3.5550, Train: 100.00%, Valid: 65.20% Test: 66.80%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 100.00%, Valid: 50.00% Test: 47.40%
Split: 01, Run: 01, Epoch: 500, Loss: 14.0976, Train: 100.00%, Valid: 48.20% Test: 49.20%
Split: 01, Run: 01
None time:  9.240131913684309
None Run 01:
Highest Train: 100.00
Highest Valid: 55.80
  Final Train: 100.00
   Final Test: 53.70
Split: 01, Run: 01, Epoch: 500, Loss: 4.2642, Train: 100.00%, Valid: 65.60% Test: 65.50%
Split: 01, Run: 01
None time:  9.328779605217278
None Run 01:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 70.90
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 51.40% Test: 47.00%
Split: 01, Run: 01
None time:  9.535615446977317
None Run 01:
Highest Train: 100.00
Highest Valid: 57.60
  Final Train: 100.00
   Final Test: 55.20
Split: 01, Run: 02, Epoch: 100, Loss: 20.5619, Train: 100.00%, Valid: 46.40% Test: 49.40%
Split: 01, Run: 02, Epoch: 100, Loss: 6.6070, Train: 100.00%, Valid: 67.40% Test: 67.70%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 51.80% Test: 50.60%
Split: 01, Run: 02, Epoch: 200, Loss: 19.0628, Train: 100.00%, Valid: 45.80% Test: 47.10%
Split: 01, Run: 02, Epoch: 200, Loss: 4.9526, Train: 100.00%, Valid: 66.00% Test: 67.10%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 48.80% Test: 48.30%
Split: 01, Run: 02, Epoch: 300, Loss: 17.6065, Train: 100.00%, Valid: 47.00% Test: 47.30%
Split: 01, Run: 02, Epoch: 300, Loss: 4.1322, Train: 100.00%, Valid: 67.40% Test: 67.50%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 49.00% Test: 48.90%
Split: 01, Run: 02, Epoch: 400, Loss: 16.7288, Train: 100.00%, Valid: 48.40% Test: 46.80%
Split: 01, Run: 02, Epoch: 400, Loss: 4.2228, Train: 100.00%, Valid: 66.20% Test: 67.40%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 47.60% Test: 49.30%
Split: 01, Run: 02, Epoch: 500, Loss: 14.9552, Train: 100.00%, Valid: 49.80% Test: 47.50%
Split: 01, Run: 02
None time:  8.943170060403645
None Run 02:
Highest Train: 100.00
Highest Valid: 53.60
  Final Train: 100.00
   Final Test: 55.20
Split: 01, Run: 02, Epoch: 500, Loss: 3.8474, Train: 100.00%, Valid: 66.80% Test: 68.10%
Split: 01, Run: 02
None time:  8.920942895114422
None Run 02:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 68.30
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 50.80% Test: 48.70%
Split: 01, Run: 02
None time:  9.101020433939993
None Run 02:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 55.10
Split: 01, Run: 03, Epoch: 100, Loss: 19.5975, Train: 100.00%, Valid: 51.20% Test: 51.40%
Split: 01, Run: 03, Epoch: 100, Loss: 7.0186, Train: 100.00%, Valid: 67.20% Test: 67.70%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 50.80% Test: 51.10%
Split: 01, Run: 03, Epoch: 200, Loss: 17.8517, Train: 100.00%, Valid: 47.80% Test: 49.20%
Split: 01, Run: 03, Epoch: 200, Loss: 4.4625, Train: 100.00%, Valid: 66.80% Test: 68.90%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 52.40% Test: 49.70%
Split: 01, Run: 03, Epoch: 300, Loss: 16.3770, Train: 100.00%, Valid: 48.00% Test: 48.70%
Split: 01, Run: 03, Epoch: 300, Loss: 3.5371, Train: 100.00%, Valid: 68.00% Test: 68.10%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 49.80% Test: 48.50%
Split: 01, Run: 03, Epoch: 400, Loss: 15.8797, Train: 100.00%, Valid: 48.00% Test: 49.60%
Split: 01, Run: 03, Epoch: 400, Loss: 4.2524, Train: 100.00%, Valid: 66.60% Test: 68.40%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 49.60% Test: 47.90%
Split: 01, Run: 03, Epoch: 500, Loss: 16.1225, Train: 100.00%, Valid: 47.80% Test: 47.80%
Split: 01, Run: 03
None time:  8.97416119929403
None Run 03:
Highest Train: 100.00
Highest Valid: 55.00
  Final Train: 100.00
   Final Test: 54.00
total time:  29.09438083320856
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 54.80 Â± 1.11
  Final Train: 100.00 Â± 0.00
   Final Test: 54.30 Â± 0.79
[32m[I 2021-07-18 21:46:32,879][0m Trial 0 finished with value: 54.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 54.80000305175781.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  54.8    {'train': '100.00 Â± 0.00', 'valid': '54.80 Â± 1.11', 'test': '54.30 Â± 0.79'}
test_acc
['54.30 Â± 0.79']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  54.80000305175781
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '54.80 Â± 1.11', 'test': '54.30 Â± 0.79'}
optuna total time:  29.113309473730624
Split: 01, Run: 03, Epoch: 500, Loss: 4.3612, Train: 100.00%, Valid: 67.80% Test: 68.10%
Split: 01, Run: 03
None time:  8.95985683053732
None Run 03:
Highest Train: 100.00
Highest Valid: 70.00
  Final Train: 100.00
   Final Test: 70.50
total time:  29.167963106185198
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.73 Â± 0.31
  Final Train: 100.00 Â± 0.00
   Final Test: 69.90 Â± 1.40
[32m[I 2021-07-18 21:46:32,952][0m Trial 0 finished with value: 69.73333740234375 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 69.73333740234375.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  69.733    {'train': '100.00 Â± 0.00', 'valid': '69.73 Â± 0.31', 'test': '69.90 Â± 1.40'}
test_acc
['69.90 Â± 1.40']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  69.73333740234375
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '69.73 Â± 0.31', 'test': '69.90 Â± 1.40'}
optuna total time:  29.185491897165775
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 48.80% Test: 47.60%
Split: 01, Run: 03
None time:  8.855313427746296
None Run 03:
Highest Train: 100.00
Highest Valid: 59.20
  Final Train: 100.00
   Final Test: 54.50
total time:  29.327174996957183
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.27 Â± 0.83
  Final Train: 100.00 Â± 0.00
   Final Test: 54.93 Â± 0.38
[32m[I 2021-07-18 21:46:33,107][0m Trial 0 finished with value: 58.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 58.266666412353516.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  58.267    {'train': '100.00 Â± 0.00', 'valid': '58.27 Â± 0.83', 'test': '54.93 Â± 0.38'}
test_acc
['54.93 Â± 0.38']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  58.266666412353516
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '58.27 Â± 0.83', 'test': '54.93 Â± 0.38'}
optuna total time:  29.34054690785706
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.1, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.1], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:48:13,763][0m A new study created in memory with name: no-name-2c1dad40-a297-4d31-8cd8-2e8cbc1e9179[0m
K:  100
alpha:  None
lr:  0.1
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.1, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 60.6982, Train: 100.00%, Valid: 52.40% Test: 50.00%
Split: 01, Run: 01, Epoch: 200, Loss: 44.5395, Train: 100.00%, Valid: 41.80% Test: 43.50%
Split: 01, Run: 01, Epoch: 300, Loss: 48.4639, Train: 100.00%, Valid: 47.00% Test: 44.20%
Split: 01, Run: 01, Epoch: 400, Loss: 47.3688, Train: 100.00%, Valid: 46.60% Test: 44.30%
Split: 01, Run: 01, Epoch: 500, Loss: 48.5420, Train: 100.00%, Valid: 42.80% Test: 41.90%
Split: 01, Run: 01
None time:  5.057100259698927
None Run 01:
Highest Train: 100.00
Highest Valid: 54.60
  Final Train: 100.00
   Final Test: 50.50
Split: 01, Run: 02, Epoch: 100, Loss: 70.1107, Train: 98.57%, Valid: 43.80% Test: 41.80%
Split: 01, Run: 02, Epoch: 200, Loss: 54.8327, Train: 100.00%, Valid: 45.00% Test: 45.50%
Split: 01, Run: 02, Epoch: 300, Loss: 49.2134, Train: 100.00%, Valid: 40.80% Test: 42.30%
Split: 01, Run: 02, Epoch: 400, Loss: 56.4618, Train: 100.00%, Valid: 43.00% Test: 43.10%
Split: 01, Run: 02, Epoch: 500, Loss: 59.4240, Train: 100.00%, Valid: 42.40% Test: 42.50%
Split: 01, Run: 02
None time:  4.602913181297481
None Run 02:
Highest Train: 100.00
Highest Valid: 51.60
  Final Train: 100.00
   Final Test: 48.50
Split: 01, Run: 03, Epoch: 100, Loss: 48.7527, Train: 100.00%, Valid: 46.00% Test: 46.90%
Split: 01, Run: 03, Epoch: 200, Loss: 49.0034, Train: 100.00%, Valid: 44.20% Test: 43.60%
Split: 01, Run: 03, Epoch: 300, Loss: 45.2712, Train: 100.00%, Valid: 40.40% Test: 39.70%
Split: 01, Run: 03, Epoch: 400, Loss: 52.0807, Train: 100.00%, Valid: 36.80% Test: 37.40%
Split: 01, Run: 03, Epoch: 500, Loss: 42.4614, Train: 100.00%, Valid: 40.60% Test: 40.40%
Split: 01, Run: 03
None time:  4.505509214475751
None Run 03:
Highest Train: 100.00
Highest Valid: 53.40
  Final Train: 100.00
   Final Test: 47.50
total time:  15.782179006375372
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 53.20 Â± 1.51
  Final Train: 100.00 Â± 0.00
   Final Test: 48.83 Â± 1.53
[32m[I 2021-07-18 21:48:29,555][0m Trial 0 finished with value: 53.19999694824219 and parameters: {'lr': 0.1, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 53.19999694824219.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.1, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  53.2    {'train': '100.00 Â± 0.00', 'valid': '53.20 Â± 1.51', 'test': '48.83 Â± 1.53'}
test_acc
['48.83 Â± 1.53']
Best params: {'lr': 0.1, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  53.19999694824219
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '53.20 Â± 1.51', 'test': '48.83 Â± 1.53'}
optuna total time:  15.795987696386874
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.05, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.05], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:49:07,673][0m A new study created in memory with name: no-name-59e259ab-dc46-4133-a2d5-08c8954fd6a8[0m
K:  100
alpha:  None
lr:  0.05
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.05, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 30.9099, Train: 100.00%, Valid: 49.00% Test: 47.60%
Split: 01, Run: 01, Epoch: 200, Loss: 31.1184, Train: 100.00%, Valid: 47.40% Test: 48.90%
Split: 01, Run: 01, Epoch: 300, Loss: 28.8175, Train: 100.00%, Valid: 49.00% Test: 47.10%
Split: 01, Run: 01, Epoch: 400, Loss: 26.3462, Train: 100.00%, Valid: 51.20% Test: 48.20%
Split: 01, Run: 01, Epoch: 500, Loss: 25.8684, Train: 100.00%, Valid: 45.40% Test: 44.90%
Split: 01, Run: 01
None time:  4.912941676564515
None Run 01:
Highest Train: 100.00
Highest Valid: 54.00
  Final Train: 100.00
   Final Test: 50.10
Split: 01, Run: 02, Epoch: 100, Loss: 29.1444, Train: 100.00%, Valid: 46.80% Test: 45.90%
Split: 01, Run: 02, Epoch: 200, Loss: 30.3159, Train: 100.00%, Valid: 49.80% Test: 47.10%
Split: 01, Run: 02, Epoch: 300, Loss: 26.5933, Train: 100.00%, Valid: 51.60% Test: 48.90%
Split: 01, Run: 02, Epoch: 400, Loss: 29.2350, Train: 100.00%, Valid: 46.20% Test: 45.60%
Split: 01, Run: 02, Epoch: 500, Loss: 25.8045, Train: 100.00%, Valid: 47.40% Test: 44.90%
Split: 01, Run: 02
None time:  4.633844438940287
None Run 02:
Highest Train: 100.00
Highest Valid: 55.20
  Final Train: 100.00
   Final Test: 53.20
Split: 01, Run: 03, Epoch: 100, Loss: 31.6376, Train: 100.00%, Valid: 48.20% Test: 46.70%
Split: 01, Run: 03, Epoch: 200, Loss: 31.4536, Train: 100.00%, Valid: 45.20% Test: 45.20%
Split: 01, Run: 03, Epoch: 300, Loss: 32.9572, Train: 100.00%, Valid: 46.40% Test: 44.60%
Split: 01, Run: 03, Epoch: 400, Loss: 28.1839, Train: 100.00%, Valid: 44.80% Test: 42.90%
Split: 01, Run: 03, Epoch: 500, Loss: 24.4795, Train: 100.00%, Valid: 46.00% Test: 45.00%
Split: 01, Run: 03
None time:  4.642049484886229
None Run 03:
Highest Train: 100.00
Highest Valid: 60.20
  Final Train: 100.00
   Final Test: 56.10
total time:  15.82860195171088
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 56.47 Â± 3.29
  Final Train: 100.00 Â± 0.00
   Final Test: 53.13 Â± 3.00
[32m[I 2021-07-18 21:49:23,514][0m Trial 0 finished with value: 56.4666633605957 and parameters: {'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 56.4666633605957.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  56.467    {'train': '100.00 Â± 0.00', 'valid': '56.47 Â± 3.29', 'test': '53.13 Â± 3.00'}
test_acc
['53.13 Â± 3.00']
Best params: {'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  56.4666633605957
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '56.47 Â± 3.29', 'test': '53.13 Â± 3.00'}
optuna total time:  15.84599137865007
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.005], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:49:52,129][0m A new study created in memory with name: no-name-c800359e-bbe1-4149-95ca-841703c4c07b[0m
K:  100
alpha:  None
lr:  0.005
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 33.9221, Train: 100.00%, Valid: 52.60% Test: 51.30%
Split: 01, Run: 01, Epoch: 200, Loss: 29.4252, Train: 100.00%, Valid: 49.60% Test: 49.40%
Split: 01, Run: 01, Epoch: 300, Loss: 29.3569, Train: 100.00%, Valid: 49.40% Test: 49.40%
Split: 01, Run: 01, Epoch: 400, Loss: 23.9447, Train: 100.00%, Valid: 50.00% Test: 47.40%
Split: 01, Run: 01, Epoch: 500, Loss: 25.1372, Train: 100.00%, Valid: 50.00% Test: 48.20%
Split: 01, Run: 01
None time:  4.942246994934976
None Run 01:
Highest Train: 100.00
Highest Valid: 57.60
  Final Train: 100.00
   Final Test: 55.00
Split: 01, Run: 02, Epoch: 100, Loss: 31.1683, Train: 100.00%, Valid: 52.20% Test: 51.20%
Split: 01, Run: 02, Epoch: 200, Loss: 28.1437, Train: 100.00%, Valid: 50.40% Test: 49.60%
Split: 01, Run: 02, Epoch: 300, Loss: 23.4454, Train: 100.00%, Valid: 48.60% Test: 49.30%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6824, Train: 100.00%, Valid: 49.20% Test: 48.40%
Split: 01, Run: 02, Epoch: 500, Loss: 22.2849, Train: 100.00%, Valid: 52.40% Test: 48.50%
Split: 01, Run: 02
None time:  4.740075978450477
None Run 02:
Highest Train: 100.00
Highest Valid: 59.00
  Final Train: 100.00
   Final Test: 55.10
Split: 01, Run: 03, Epoch: 100, Loss: 32.0689, Train: 100.00%, Valid: 52.00% Test: 50.50%
Split: 01, Run: 03, Epoch: 200, Loss: 30.1264, Train: 100.00%, Valid: 52.60% Test: 50.10%
Split: 01, Run: 03, Epoch: 300, Loss: 29.3178, Train: 100.00%, Valid: 49.80% Test: 48.80%
Split: 01, Run: 03, Epoch: 400, Loss: 25.4410, Train: 100.00%, Valid: 51.60% Test: 49.40%
Split: 01, Run: 03, Epoch: 500, Loss: 25.2515, Train: 100.00%, Valid: 50.60% Test: 49.10%
Split: 01, Run: 03
None time:  4.7634106408804655
None Run 03:
Highest Train: 100.00
Highest Valid: 59.40
  Final Train: 100.00
   Final Test: 55.60
total time:  16.059611557982862
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.67 Â± 0.95
  Final Train: 100.00 Â± 0.00
   Final Test: 55.23 Â± 0.32
[32m[I 2021-07-18 21:50:08,200][0m Trial 0 finished with value: 58.66666030883789 and parameters: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 58.66666030883789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  58.667    {'train': '100.00 Â± 0.00', 'valid': '58.67 Â± 0.95', 'test': '55.23 Â± 0.32'}
test_acc
['55.23 Â± 0.32']
Best params: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  58.66666030883789
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '58.67 Â± 0.95', 'test': '55.23 Â± 0.32'}
optuna total time:  16.074786501005292
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.005], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:50:38,414][0m A new study created in memory with name: no-name-7d3e68bf-9075-4fb0-b204-6a2a14d3825d[0m
K:  100
alpha:  None
lr:  0.005
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
)
Split: 01, Run: 01, Epoch: 100, Loss: 22.0836, Train: 100.00%, Valid: 48.80% Test: 51.90%
Split: 01, Run: 01, Epoch: 200, Loss: 20.2603, Train: 100.00%, Valid: 47.80% Test: 49.80%
Split: 01, Run: 01, Epoch: 300, Loss: 19.3011, Train: 100.00%, Valid: 48.20% Test: 50.20%
Using backend: pytorch
Split: 01, Run: 01, Epoch: 400, Loss: 19.8837, Train: 100.00%, Valid: 46.80% Test: 48.30%
main:  Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.005], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 21:50:45,343][0m A new study created in memory with name: no-name-326e63c8-b509-4c55-8716-56a66c50454e[0m
K:  100
alpha:  None
lr:  0.005
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.005, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 01, Epoch: 500, Loss: 14.4181, Train: 100.00%, Valid: 47.60% Test: 47.60%
Split: 01, Run: 01
None time:  5.3075468884781
None Run 01:
Highest Train: 100.00
Highest Valid: 55.60
  Final Train: 100.00
   Final Test: 51.70
Split: 01, Run: 02, Epoch: 100, Loss: 21.4119, Train: 100.00%, Valid: 47.60% Test: 49.90%
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
)
Split: 01, Run: 02, Epoch: 200, Loss: 20.8890, Train: 100.00%, Valid: 47.60% Test: 48.50%
Split: 01, Run: 01, Epoch: 100, Loss: 6.9589, Train: 100.00%, Valid: 67.80% Test: 68.90%
Split: 01, Run: 02, Epoch: 300, Loss: 18.3942, Train: 100.00%, Valid: 47.60% Test: 49.90%
Split: 01, Run: 01, Epoch: 200, Loss: 5.5289, Train: 100.00%, Valid: 66.00% Test: 67.10%
Split: 01, Run: 02, Epoch: 400, Loss: 18.7280, Train: 100.00%, Valid: 47.80% Test: 48.90%
Split: 01, Run: 01, Epoch: 300, Loss: 5.8676, Train: 100.00%, Valid: 67.00% Test: 68.70%
Split: 01, Run: 02, Epoch: 500, Loss: 17.5005, Train: 100.00%, Valid: 48.60% Test: 48.30%
Split: 01, Run: 02
None time:  6.5897323889657855
None Run 02:
Highest Train: 100.00
Highest Valid: 55.60
  Final Train: 100.00
   Final Test: 54.50
Split: 01, Run: 01, Epoch: 400, Loss: 3.9024, Train: 100.00%, Valid: 66.40% Test: 68.10%
Split: 01, Run: 03, Epoch: 100, Loss: 20.5973, Train: 100.00%, Valid: 53.40% Test: 52.30%
Split: 01, Run: 01, Epoch: 500, Loss: 4.9725, Train: 100.00%, Valid: 64.80% Test: 65.80%
Split: 01, Run: 01
None time:  7.578551980666816
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 98.33
   Final Test: 71.90
Split: 01, Run: 03, Epoch: 200, Loss: 19.9995, Train: 100.00%, Valid: 49.20% Test: 50.80%
Split: 01, Run: 02, Epoch: 100, Loss: 7.7457, Train: 100.00%, Valid: 68.60% Test: 69.30%
Split: 01, Run: 03, Epoch: 300, Loss: 19.0034, Train: 100.00%, Valid: 50.00% Test: 50.80%
Split: 01, Run: 02, Epoch: 200, Loss: 5.8098, Train: 100.00%, Valid: 67.20% Test: 67.90%
Split: 01, Run: 03, Epoch: 400, Loss: 17.1683, Train: 100.00%, Valid: 49.60% Test: 50.40%
Split: 01, Run: 02, Epoch: 300, Loss: 5.0706, Train: 100.00%, Valid: 67.00% Test: 68.40%
Split: 01, Run: 03, Epoch: 500, Loss: 16.4084, Train: 100.00%, Valid: 48.20% Test: 49.80%
Split: 01, Run: 03
None time:  7.365071748383343
None Run 03:
Highest Train: 100.00
Highest Valid: 54.60
  Final Train: 100.00
   Final Test: 53.90
total time:  21.11284366156906
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 55.27 Â± 0.58
  Final Train: 100.00 Â± 0.00
   Final Test: 53.37 Â± 1.47
[32m[I 2021-07-18 21:50:59,540][0m Trial 0 finished with value: 55.26666259765625 and parameters: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 55.26666259765625.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  55.267    {'train': '100.00 Â± 0.00', 'valid': '55.27 Â± 0.58', 'test': '53.37 Â± 1.47'}
test_acc
['53.37 Â± 1.47']
Best params: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  55.26666259765625
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '55.27 Â± 0.58', 'test': '53.37 Â± 1.47'}
optuna total time:  21.130560390651226
Split: 01, Run: 02, Epoch: 400, Loss: 3.7698, Train: 100.00%, Valid: 66.60% Test: 67.50%
Split: 01, Run: 02, Epoch: 500, Loss: 3.5303, Train: 100.00%, Valid: 67.20% Test: 68.40%
Split: 01, Run: 02
None time:  6.5075067058205605
None Run 02:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 100.00
   Final Test: 69.50
Split: 01, Run: 03, Epoch: 100, Loss: 8.2801, Train: 100.00%, Valid: 67.20% Test: 69.50%
Split: 01, Run: 03, Epoch: 200, Loss: 4.9849, Train: 100.00%, Valid: 67.20% Test: 67.80%
Split: 01, Run: 03, Epoch: 300, Loss: 4.0092, Train: 100.00%, Valid: 67.20% Test: 68.40%
Split: 01, Run: 03, Epoch: 400, Loss: 4.9201, Train: 100.00%, Valid: 67.40% Test: 67.90%
Split: 01, Run: 03, Epoch: 500, Loss: 4.3398, Train: 100.00%, Valid: 65.80% Test: 67.80%
Split: 01, Run: 03
None time:  5.190918365493417
None Run 03:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 71.00
total time:  21.335496393963695
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.87 Â± 0.31
  Final Train: 99.44 Â± 0.96
   Final Test: 70.80 Â± 1.21
[32m[I 2021-07-18 21:51:06,690][0m Trial 0 finished with value: 69.86666870117188 and parameters: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}. Best is trial 0 with value: 69.86666870117188.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}   trial.value:  69.867    {'train': '99.44 Â± 0.96', 'valid': '69.87 Â± 0.31', 'test': '70.80 Â± 1.21'}
test_acc
['70.80 Â± 1.21']
Best params: {'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'K': 100}
Best trial Value:  69.86666870117188
Best trial Acc:  {'train': '99.44 Â± 0.96', 'valid': '69.87 Â± 0.31', 'test': '70.80 Â± 1.21'}
optuna total time:  21.351523316465318
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 22:06:00,979][0m A new study created in memory with name: no-name-ee35821d-763e-446a-b07a-13726060f4bb[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
)
[33m[W 2021-07-18 22:06:02,820][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'prop'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 114, in objective
    model.reset_parameters()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 66, in reset_parameters
    self.prop.reset_parameters()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'prop'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 114, in objective
    model.reset_parameters()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 66, in reset_parameters
    self.prop.reset_parameters()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'prop'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [None], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 22:07:13,675][0m A new study created in memory with name: no-name-cc43c95a-c480-4d74-acaf-0d1a13251f7a[0m
K:  100
alpha:  None
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=0.005, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='None', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=100, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=None)
)
[33m[W 2021-07-18 22:07:16,796][0m Trial 0 failed because of the following error: TypeError("unsupported operand type(s) for -: 'int' and 'NoneType'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in forward
    x = self.prop(x, adj_t, data=data)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 146, in forward
    x = self.appnp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K, alpha=self.alpha)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 160, in appnp_forward
    x = x * (1 - alpha)
TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 308, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in forward
    x = self.prop(x, adj_t, data=data)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 146, in forward
    x = self.appnp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K, alpha=self.alpha)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 160, in appnp_forward
    x = x * (1 - alpha)
TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'
Using backend: pytorch
main:  Namespace(K=100, L21=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 22:09:42,151][0m A new study created in memory with name: no-name-b545a792-e350-4cb5-9c30-f02b22d4b2f1[0m
K:  100
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=100, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py[0m(164)[0;36mappnp_forward[0;34m()[0m
[0;32m    163 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 164 [0;31m        [0;32mreturn[0m [0mx[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    165 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py[0m(164)[0;36mappnp_forward[0;34m()[0m
[0;32m    163 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 164 [0;31m        [0;32mreturn[0m [0mx[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    165 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py[0m(164)[0;36mappnp_forward[0;34m()[0m
[0;32m    163 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 164 [0;31m        [0;32mreturn[0m [0mx[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    165 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py[0m(164)[0;36mappnp_forward[0;34m()[0m
[0;32m    163 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 164 [0;31m        [0;32mreturn[0m [0mx[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    165 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py[0m(164)[0;36mappnp_forward[0;34m()[0m
[0;32m    163 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 164 [0;31m        [0;32mreturn[0m [0mx[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    165 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-18 22:11:54,420][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 120, in objective
    args.current_epoch = epoch
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in forward
    x = self.prop(x, adj_t, data=data)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 146, in forward
    if self.mode == 'APPNP':
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 164, in appnp_forward
    import ipdb; ipdb.set_trace()
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 164, in appnp_forward
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=100, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 22:12:07,857][0m A new study created in memory with name: no-name-e471988d-9425-4416-9da9-6d6aa233a407[0m
K:  100
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=100, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-18 22:12:10,080][0m Trial 0 failed because of the following error: NameError("name 'args' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 121, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in forward
    x = self.prop(x, adj_t, data=data)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 141, in forward
    if args.LP:
NameError: name 'args' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 309, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 121, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 10, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in forward
    x = self.prop(x, adj_t, data=data)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/emp_both.py", line 141, in forward
    if args.LP:
NameError: name 'args' is not defined
Using backend: pytorch
main:  Namespace(K=100, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [100]}
num_trial:  1
[32m[I 2021-07-18 22:12:47,807][0m A new study created in memory with name: no-name-bef9fc01-d43b-4b8d-9753-052f9b1b6541[0m
K:  100
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=100, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=100, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 18.2452, Train: 100.00%, Valid: 79.80% Test: 80.90%
Split: 01, Run: 01, Epoch: 200, Loss: 14.3085, Train: 100.00%, Valid: 77.40% Test: 79.10%
Split: 01, Run: 01, Epoch: 300, Loss: 13.5364, Train: 100.00%, Valid: 78.20% Test: 78.60%
Split: 01, Run: 01, Epoch: 400, Loss: 11.0238, Train: 100.00%, Valid: 78.40% Test: 78.60%
Split: 01, Run: 01, Epoch: 500, Loss: 10.8914, Train: 100.00%, Valid: 77.40% Test: 78.20%
Split: 01, Run: 01
None time:  103.7598781939596
None Run 01:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 98.57
   Final Test: 81.60
Split: 01, Run: 02, Epoch: 100, Loss: 18.5011, Train: 100.00%, Valid: 78.80% Test: 80.20%
Split: 01, Run: 02, Epoch: 200, Loss: 13.6505, Train: 100.00%, Valid: 78.00% Test: 78.40%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:15:37,928][0m A new study created in memory with name: no-name-d3afb058-e649-45fd-9a84-ceb2818f2ee1[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 17.3556, Train: 100.00%, Valid: 79.20% Test: 80.30%
Split: 01, Run: 01, Epoch: 200, Loss: 13.8516, Train: 100.00%, Valid: 77.60% Test: 78.30%
Split: 01, Run: 01, Epoch: 300, Loss: 12.2897, Train: 100.00%, Valid: 78.40% Test: 78.50%
Split: 01, Run: 01, Epoch: 400, Loss: 10.1812, Train: 100.00%, Valid: 78.20% Test: 78.10%
Split: 01, Run: 01, Epoch: 500, Loss: 10.5280, Train: 100.00%, Valid: 78.20% Test: 78.10%
Split: 01, Run: 01
None time:  14.842518555000424
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 98.57
   Final Test: 81.20
Split: 01, Run: 02, Epoch: 100, Loss: 17.8000, Train: 100.00%, Valid: 78.80% Test: 79.90%
Split: 01, Run: 02, Epoch: 200, Loss: 12.8894, Train: 100.00%, Valid: 78.00% Test: 78.50%
Split: 01, Run: 02, Epoch: 300, Loss: 12.1229, Train: 100.00%, Valid: 78.00% Test: 78.80%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:16:05,164][0m A new study created in memory with name: no-name-588bdd5b-692c-433c-a0da-0d80382f73f9[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 400, Loss: 9.9547, Train: 100.00%, Valid: 78.40% Test: 78.90%
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:16:08,555][0m A new study created in memory with name: no-name-80b06646-30c8-4359-82fc-2a8484be07a9[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 500, Loss: 9.2484, Train: 100.00%, Valid: 77.60% Test: 77.80%
Split: 01, Run: 02
None time:  15.244573928415775
None Run 02:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 94.29
   Final Test: 82.40
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 14.4835, Train: 100.00%, Valid: 67.20% Test: 65.60%
Split: 01, Run: 03, Epoch: 100, Loss: 15.8191, Train: 100.00%, Valid: 80.60% Test: 81.30%
Split: 01, Run: 01, Epoch: 200, Loss: 10.7442, Train: 100.00%, Valid: 66.00% Test: 65.20%
Split: 01, Run: 01, Epoch: 100, Loss: 5.3084, Train: 98.33%, Valid: 81.00% Test: 78.50%
Split: 01, Run: 03, Epoch: 200, Loss: 12.3406, Train: 100.00%, Valid: 79.20% Test: 79.50%
Split: 01, Run: 01, Epoch: 300, Loss: 10.3228, Train: 100.00%, Valid: 66.20% Test: 64.80%
Split: 01, Run: 01, Epoch: 200, Loss: 3.2145, Train: 100.00%, Valid: 78.40% Test: 77.50%
Split: 01, Run: 03, Epoch: 300, Loss: 12.5297, Train: 100.00%, Valid: 78.60% Test: 79.10%
Split: 01, Run: 01, Epoch: 400, Loss: 10.6062, Train: 100.00%, Valid: 65.60% Test: 65.40%
Split: 01, Run: 01, Epoch: 300, Loss: 2.9794, Train: 100.00%, Valid: 78.20% Test: 77.40%
Split: 01, Run: 03, Epoch: 400, Loss: 10.7338, Train: 100.00%, Valid: 78.20% Test: 78.60%
Split: 01, Run: 01, Epoch: 500, Loss: 10.1617, Train: 100.00%, Valid: 66.20% Test: 65.20%
Split: 01, Run: 01
None time:  21.338729426264763
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 95.83
   Final Test: 70.40
Split: 01, Run: 01, Epoch: 400, Loss: 2.5371, Train: 100.00%, Valid: 77.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 500, Loss: 10.4846, Train: 100.00%, Valid: 78.40% Test: 78.30%
Split: 01, Run: 03
None time:  21.170093565247953
None Run 03:
Highest Train: 100.00
Highest Valid: 80.80
  Final Train: 99.29
   Final Test: 82.30
total time:  52.953898161649704
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.13 Â± 0.42
  Final Train: 97.38 Â± 2.70
   Final Test: 81.97 Â± 0.67
[32m[I 2021-07-18 22:16:30,894][0m Trial 0 finished with value: 81.13333892822266 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.13333892822266.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.133    {'train': '97.38 Â± 2.70', 'valid': '81.13 Â± 0.42', 'test': '81.97 Â± 0.67'}
test_acc
['81.97 Â± 0.67']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.13333892822266
Best trial Acc:  {'train': '97.38 Â± 2.70', 'valid': '81.13 Â± 0.42', 'test': '81.97 Â± 0.67'}
optuna total time:  52.96982659120113
Split: 01, Run: 02, Epoch: 100, Loss: 15.2567, Train: 100.00%, Valid: 66.00% Test: 66.00%
Split: 01, Run: 01, Epoch: 500, Loss: 2.4743, Train: 100.00%, Valid: 78.60% Test: 77.00%
Split: 01, Run: 01
None time:  22.52012326940894
None Run 01:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 98.33
   Final Test: 79.20
Using backend: pytorch
Split: 01, Run: 02, Epoch: 200, Loss: 11.3937, Train: 100.00%, Valid: 66.60% Test: 65.50%
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:16:36,802][0m A new study created in memory with name: no-name-b30459bd-69f3-4087-9de0-a5601f36799e[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 100, Loss: 4.3828, Train: 100.00%, Valid: 81.00% Test: 78.70%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 300, Loss: 10.9151, Train: 100.00%, Valid: 65.00% Test: 65.30%
Split: 01, Run: 02, Epoch: 200, Loss: 2.9639, Train: 100.00%, Valid: 78.40% Test: 77.70%
Split: 01, Run: 01, Epoch: 100, Loss: 17.3556, Train: 100.00%, Valid: 79.20% Test: 80.30%
Split: 01, Run: 02, Epoch: 400, Loss: 11.1688, Train: 100.00%, Valid: 66.20% Test: 65.50%
Split: 01, Run: 02, Epoch: 300, Loss: 2.7126, Train: 100.00%, Valid: 78.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 200, Loss: 13.8516, Train: 100.00%, Valid: 77.60% Test: 78.30%
Split: 01, Run: 02, Epoch: 500, Loss: 10.1318, Train: 100.00%, Valid: 66.20% Test: 63.40%
Split: 01, Run: 02
None time:  20.525032287463546
None Run 02:
Highest Train: 100.00
Highest Valid: 72.40
  Final Train: 92.50
   Final Test: 71.10
Split: 01, Run: 02, Epoch: 400, Loss: 2.4150, Train: 100.00%, Valid: 78.60% Test: 77.70%
Split: 01, Run: 01, Epoch: 300, Loss: 12.2897, Train: 100.00%, Valid: 78.40% Test: 78.50%
Split: 01, Run: 03, Epoch: 100, Loss: 12.7179, Train: 100.00%, Valid: 67.40% Test: 66.00%
Split: 01, Run: 02, Epoch: 500, Loss: 2.7662, Train: 100.00%, Valid: 79.00% Test: 77.80%
Split: 01, Run: 02
None time:  21.61068583186716
None Run 02:
Highest Train: 100.00
Highest Valid: 82.20
  Final Train: 98.33
   Final Test: 79.70
Split: 01, Run: 01, Epoch: 400, Loss: 10.1812, Train: 100.00%, Valid: 78.20% Test: 78.10%
Split: 01, Run: 03, Epoch: 200, Loss: 11.8551, Train: 100.00%, Valid: 66.20% Test: 64.40%
Split: 01, Run: 03, Epoch: 100, Loss: 5.9095, Train: 100.00%, Valid: 80.40% Test: 77.90%
Split: 01, Run: 01, Epoch: 500, Loss: 10.5280, Train: 100.00%, Valid: 78.20% Test: 78.10%
Split: 01, Run: 01
None time:  22.336152965202928
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 98.57
   Final Test: 81.20
Split: 01, Run: 03, Epoch: 300, Loss: 10.3173, Train: 100.00%, Valid: 66.20% Test: 65.10%
Split: 01, Run: 03, Epoch: 200, Loss: 3.2559, Train: 100.00%, Valid: 78.00% Test: 77.80%
Split: 01, Run: 02, Epoch: 100, Loss: 17.8000, Train: 100.00%, Valid: 78.80% Test: 79.90%
Split: 01, Run: 03, Epoch: 400, Loss: 10.3538, Train: 100.00%, Valid: 66.20% Test: 65.60%
Split: 01, Run: 03, Epoch: 300, Loss: 2.7857, Train: 100.00%, Valid: 78.40% Test: 77.80%
Split: 01, Run: 02, Epoch: 200, Loss: 12.8894, Train: 100.00%, Valid: 78.00% Test: 78.50%
Split: 01, Run: 03, Epoch: 500, Loss: 9.6974, Train: 100.00%, Valid: 66.60% Test: 65.90%
Split: 01, Run: 03
None time:  21.70897043682635
None Run 03:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 93.33
   Final Test: 69.30
total time:  65.46658202260733
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.13 Â± 1.03
  Final Train: 93.89 Â± 1.73
   Final Test: 70.27 Â± 0.91
[32m[I 2021-07-18 22:17:10,644][0m Trial 0 finished with value: 72.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 72.13333129882812.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  72.133    {'train': '93.89 Â± 1.73', 'valid': '72.13 Â± 1.03', 'test': '70.27 Â± 0.91'}
test_acc
['70.27 Â± 0.91']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  72.13333129882812
Best trial Acc:  {'train': '93.89 Â± 1.73', 'valid': '72.13 Â± 1.03', 'test': '70.27 Â± 0.91'}
optuna total time:  65.4841784723103
Split: 01, Run: 03, Epoch: 400, Loss: 2.3516, Train: 100.00%, Valid: 79.60% Test: 77.80%
Split: 01, Run: 02, Epoch: 300, Loss: 12.1229, Train: 100.00%, Valid: 78.00% Test: 78.80%
Split: 01, Run: 03, Epoch: 500, Loss: 2.0806, Train: 100.00%, Valid: 78.40% Test: 78.00%
Split: 01, Run: 03
None time:  21.52894735056907
None Run 03:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 98.33
   Final Test: 79.40
total time:  67.62060164753348
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.40 Â± 0.20
  Final Train: 98.33 Â± 0.00
   Final Test: 79.43 Â± 0.25
[32m[I 2021-07-18 22:17:16,187][0m Trial 0 finished with value: 82.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 82.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  82.4    {'train': '98.33 Â± 0.00', 'valid': '82.40 Â± 0.20', 'test': '79.43 Â± 0.25'}
test_acc
['79.43 Â± 0.25']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  82.4000015258789
Best trial Acc:  {'train': '98.33 Â± 0.00', 'valid': '82.40 Â± 0.20', 'test': '79.43 Â± 0.25'}
optuna total time:  67.63602958340198
Split: 01, Run: 02, Epoch: 400, Loss: 9.9547, Train: 100.00%, Valid: 78.40% Test: 78.90%
Split: 01, Run: 02, Epoch: 500, Loss: 9.2484, Train: 100.00%, Valid: 77.60% Test: 77.80%
Split: 01, Run: 02
None time:  19.513861002400517
None Run 02:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 94.29
   Final Test: 82.40
Split: 01, Run: 03, Epoch: 100, Loss: 15.8191, Train: 100.00%, Valid: 80.60% Test: 81.30%
Split: 01, Run: 03, Epoch: 200, Loss: 12.3406, Train: 100.00%, Valid: 79.20% Test: 79.50%
Split: 01, Run: 03, Epoch: 300, Loss: 12.5297, Train: 100.00%, Valid: 78.60% Test: 79.10%
Split: 01, Run: 03, Epoch: 400, Loss: 10.7338, Train: 100.00%, Valid: 78.20% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 10.4846, Train: 100.00%, Valid: 78.40% Test: 78.30%
Split: 01, Run: 03
None time:  13.897070505656302
None Run 03:
Highest Train: 100.00
Highest Valid: 80.80
  Final Train: 99.29
   Final Test: 82.30
total time:  57.52299421932548
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.13 Â± 0.42
  Final Train: 97.38 Â± 2.70
   Final Test: 81.97 Â± 0.67
[32m[I 2021-07-18 22:17:34,333][0m Trial 0 finished with value: 81.13333892822266 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.13333892822266.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.133    {'train': '97.38 Â± 2.70', 'valid': '81.13 Â± 0.42', 'test': '81.97 Â± 0.67'}
test_acc
['81.97 Â± 0.67']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.13333892822266
Best trial Acc:  {'train': '97.38 Â± 2.70', 'valid': '81.13 Â± 0.42', 'test': '81.97 Â± 0.67'}
optuna total time:  57.535820414312184
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:18:59,892][0m A new study created in memory with name: no-name-d603f00b-e407-4c3c-816f-a49f93fba65c[0m
main:  main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:18:59,893][0m A new study created in memory with name: no-name-0f87a5ed-4eed-454c-8831-7a089ecab3c0[0m
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:18:59,892][0m A new study created in memory with name: no-name-c49e1c70-a80a-49ed-acee-95b0505de2a7[0m
K:  10
alpha:  0.1
lr:  0.01K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.4542, Train: 95.83%, Valid: 73.20% Test: 71.70%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1734, Train: 98.33%, Valid: 80.80% Test: 78.50%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.3269, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.1367, Train: 100.00%, Valid: 80.60% Test: 80.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2871, Train: 99.17%, Valid: 72.60% Test: 70.60%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1129, Train: 100.00%, Valid: 80.60% Test: 79.80%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2669, Train: 99.17%, Valid: 70.20% Test: 71.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1016, Train: 100.00%, Valid: 81.80% Test: 80.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  22.02974057942629
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 01, Epoch: 500, Loss: 0.2309, Train: 98.33%, Valid: 71.20% Test: 71.20%
Split: 01, Run: 01
None time:  22.067383687011898
None Run 01:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 93.33
   Final Test: 71.50
Split: 01, Run: 01, Epoch: 500, Loss: 0.0852, Train: 100.00%, Valid: 80.40% Test: 80.10%
Split: 01, Run: 01
None time:  22.69464420992881
None Run 01:
Highest Train: 100.00
Highest Valid: 82.80
  Final Train: 98.33
   Final Test: 80.60
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4669, Train: 95.83%, Valid: 71.60% Test: 70.70%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1799, Train: 98.33%, Valid: 81.20% Test: 80.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.3001, Train: 98.33%, Valid: 71.20% Test: 71.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1296, Train: 100.00%, Valid: 80.00% Test: 79.10%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2705, Train: 98.33%, Valid: 71.20% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1361, Train: 100.00%, Valid: 81.40% Test: 79.70%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2552, Train: 99.17%, Valid: 71.80% Test: 71.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1359, Train: 100.00%, Valid: 79.80% Test: 79.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  21.342089535668492
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 02, Epoch: 500, Loss: 0.2435, Train: 99.17%, Valid: 71.40% Test: 70.80%
Split: 01, Run: 02
None time:  21.341579739935696
None Run 02:
Highest Train: 100.00
Highest Valid: 74.40
  Final Train: 93.33
   Final Test: 71.40
Split: 01, Run: 02, Epoch: 500, Loss: 0.0909, Train: 100.00%, Valid: 80.20% Test: 80.00%
Split: 01, Run: 02
None time:  21.98104060906917
None Run 02:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 100.00
   Final Test: 80.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4736, Train: 94.17%, Valid: 72.00% Test: 71.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.2029, Train: 98.33%, Valid: 80.60% Test: 79.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.3200, Train: 97.50%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1490, Train: 100.00%, Valid: 80.60% Test: 79.40%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2875, Train: 98.33%, Valid: 70.40% Test: 69.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1375, Train: 98.33%, Valid: 79.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2421, Train: 99.17%, Valid: 71.00% Test: 71.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0913, Train: 100.00%, Valid: 80.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  21.524364643730223
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  66.71921173483133
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-18 22:20:06,654][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  66.7666561966762
Split: 01, Run: 03, Epoch: 500, Loss: 0.2637, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 03
None time:  21.422856669873
None Run 03:
Highest Train: 100.00
Highest Valid: 74.00
  Final Train: 95.00
   Final Test: 71.70
total time:  66.77644383814186
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.40 Â± 0.40
  Final Train: 93.89 Â± 0.96
   Final Test: 71.53 Â± 0.15
[32m[I 2021-07-18 22:20:06,716][0m Trial 0 finished with value: 74.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 74.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  74.4    {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
test_acc
['71.53 Â± 0.15']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  74.4000015258789
Best trial Acc:  {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
optuna total time:  66.8283183304593
Split: 01, Run: 03, Epoch: 500, Loss: 0.0967, Train: 100.00%, Valid: 80.20% Test: 79.90%
Split: 01, Run: 03
None time:  21.610310395248234
None Run 03:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 98.33
   Final Test: 80.10
total time:  68.26791098713875
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.60 Â± 0.20
  Final Train: 98.89 Â± 0.96
   Final Test: 80.37 Â± 0.25
[32m[I 2021-07-18 22:20:08,205][0m Trial 0 finished with value: 82.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 82.5999984741211.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  82.6    {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
test_acc
['80.37 Â± 0.25']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  82.5999984741211
Best trial Acc:  {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
optuna total time:  68.31757210195065
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
main:  main:  main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:22:03,806][0m A new study created in memory with name: no-name-dcc2b772-a599-4b8e-b450-c1c423a16880[0m
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:22:03,805][0m A new study created in memory with name: no-name-6fe1b0ff-fe8b-4946-806c-4f104c77e03b[0m
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:22:03,805][0m A new study created in memory with name: no-name-b9aa3cd0-96fa-4a30-98dc-fae2901b953d[0m
K:  10
alpha:  0.1
lr: K:  10 0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)

alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
ALTOPT(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
ALTOPT(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 100, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 200, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01, Epoch: 300, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 01, Epoch: 300, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 300, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01, Epoch: 400, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 01, Epoch: 400, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01, Epoch: 500, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 01
None time:  26.58863520156592
None Run 01:
Highest Train: 100.00
Highest Valid: 54.80
  Final Train: 100.00
   Final Test: 53.80
Split: 01, Run: 01, Epoch: 500, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01
None time:  26.988898067735136
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 01, Epoch: 500, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01
None time:  27.262797022238374
None Run 01:
Highest Train: 100.00
Highest Valid: 71.20
  Final Train: 100.00
   Final Test: 71.10
Split: 01, Run: 02, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 100, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 02, Epoch: 100, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 200, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 300, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 300, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 400, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 400, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 500, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02
None time:  23.083785213530064
None Run 02:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 02, Epoch: 500, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 02
None time:  23.181200076825917
None Run 02:
Highest Train: 100.00
Highest Valid: 54.80
  Final Train: 100.00
   Final Test: 53.80
Split: 01, Run: 02, Epoch: 500, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02
None time:  23.72236544173211
None Run 02:
Highest Train: 100.00
Highest Valid: 71.20
  Final Train: 100.00
   Final Test: 71.10
Split: 01, Run: 03, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 100, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 200, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03, Epoch: 300, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 300, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 03, Epoch: 300, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03, Epoch: 400, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 400, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 03, Epoch: 400, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03, Epoch: 500, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03
None time:  23.190259872935712
None Run 03:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
total time:  78.44708537776023
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.40 Â± 0.00
[32m[I 2021-07-18 22:23:22,262][0m Trial 0 finished with value: 71.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 71.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  71    {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '73.40 Â± 0.00'}
test_acc
['73.40 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  71.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '73.40 Â± 0.00'}
optuna total time:  78.46163651999086
Split: 01, Run: 03, Epoch: 500, Loss: 1.5460, Train: 100.00%, Valid: 54.80% Test: 53.80%
Split: 01, Run: 03
None time:  23.217984452843666
None Run 03:
Highest Train: 100.00
Highest Valid: 54.80
  Final Train: 100.00
   Final Test: 53.80
total time:  78.53485986962914
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 54.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 53.80 Â± 0.00
[32m[I 2021-07-18 22:23:22,357][0m Trial 0 finished with value: 54.79999923706055 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 54.79999923706055.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  54.8    {'train': '100.00 Â± 0.00', 'valid': '54.80 Â± 0.00', 'test': '53.80 Â± 0.00'}
test_acc
['53.80 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  54.79999923706055
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '54.80 Â± 0.00', 'test': '53.80 Â± 0.00'}
optuna total time:  78.55611186940223
Split: 01, Run: 03, Epoch: 500, Loss: 0.9784, Train: 100.00%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 03
None time:  23.41109117679298
None Run 03:
Highest Train: 100.00
Highest Valid: 71.20
  Final Train: 100.00
   Final Test: 71.10
total time:  80.03967144154012
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.20 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.10 Â± 0.00
[32m[I 2021-07-18 22:23:23,859][0m Trial 0 finished with value: 71.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 71.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  71.2    {'train': '100.00 Â± 0.00', 'valid': '71.20 Â± 0.00', 'test': '71.10 Â± 0.00'}
test_acc
['71.10 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  71.20000457763672
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '71.20 Â± 0.00', 'test': '71.10 Â± 0.00'}
optuna total time:  80.05807114113122
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:23:47,611][0m A new study created in memory with name: no-name-e5063e3c-295d-4b81-9ff8-0d700ec68786[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 200, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 300, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 400, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 500, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01
None time:  16.28628994617611
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 02, Epoch: 100, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 200, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 300, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 400, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 500, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02
None time:  15.275065459311008
None Run 02:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 03, Epoch: 100, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 200, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 300, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 400, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 500, Loss: 80.9648, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03
None time:  15.311998304910958
None Run 03:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
total time:  48.528352092951536
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.40 Â± 0.00
[32m[I 2021-07-18 22:24:36,152][0m Trial 0 finished with value: 71.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 71.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  71    {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '73.40 Â± 0.00'}
test_acc
['73.40 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  71.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '73.40 Â± 0.00'}
optuna total time:  48.544544314965606
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:25:04,467][0m A new study created in memory with name: no-name-f59194f3-6cf2-4598-908a-ecd835db4b24[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4365.9629, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 200, Loss: 4365.9629, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 300, Loss: 4365.9629, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.9629, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.9629, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01
None time:  16.29392700176686
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:25:44,832][0m A new study created in memory with name: no-name-df722652-a804-4f0f-9b1a-ad73147d5241[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  14.415627313777804
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  13.795828632079065
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  13.791998440399766
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  43.683511048555374
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-18 22:26:28,526][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  43.698414851911366
Using backend: pytorch
Using backend: pytorch
main:  main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:32:42,460][0m A new study created in memory with name: no-name-5fbd8ff4-5f4b-473a-b8b8-757890145812[0m
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:32:42,460][0m A new study created in memory with name: no-name-f45a1a21-444c-4cc7-9c72-e7082e9b043d[0m
K: K:   10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  17.93421505857259
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  18.13699086010456
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  16.130040953867137
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  16.169917333871126
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  16.29766921978444
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  56.479149509221315
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-18 22:33:38,963][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  56.507623757235706
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  16.294956068508327
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  56.71900044567883
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-18 22:33:39,204][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  56.74848352000117
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:35:10,804][0m A new study created in memory with name: no-name-3c1a357f-1df8-4e1d-a775-6f85acba2793[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
APPNP(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Using backend: pytorch
main:  Split: 01, Run: 01, Epoch: 100, Loss: 0.4542, Train: 95.83%, Valid: 73.20% Test: 71.70%
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:35:16,123][0m A new study created in memory with name: no-name-59576847-997a-4c3b-864e-83284397141a[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 0.3269, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2871, Train: 99.17%, Valid: 72.60% Test: 70.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1734, Train: 98.33%, Valid: 80.80% Test: 78.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2669, Train: 99.17%, Valid: 70.20% Test: 71.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.1367, Train: 100.00%, Valid: 80.60% Test: 80.00%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2309, Train: 98.33%, Valid: 71.20% Test: 71.20%
Split: 01, Run: 01
None time:  17.286870139651
None Run 01:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 93.33
   Final Test: 71.50
Split: 01, Run: 01, Epoch: 300, Loss: 0.1129, Train: 100.00%, Valid: 80.60% Test: 79.80%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4669, Train: 95.83%, Valid: 71.60% Test: 70.70%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1016, Train: 100.00%, Valid: 81.80% Test: 80.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.3001, Train: 98.33%, Valid: 71.20% Test: 71.80%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0852, Train: 100.00%, Valid: 80.40% Test: 80.10%
Split: 01, Run: 01
None time:  19.086106466129422
None Run 01:
Highest Train: 100.00
Highest Valid: 82.80
  Final Train: 98.33
   Final Test: 80.60
Split: 01, Run: 02, Epoch: 300, Loss: 0.2705, Train: 98.33%, Valid: 71.20% Test: 72.00%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1799, Train: 98.33%, Valid: 81.20% Test: 80.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2552, Train: 99.17%, Valid: 71.80% Test: 71.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1296, Train: 100.00%, Valid: 80.00% Test: 79.10%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2435, Train: 99.17%, Valid: 71.40% Test: 70.80%
Split: 01, Run: 02
None time:  17.73178528342396
None Run 02:
Highest Train: 100.00
Highest Valid: 74.40
  Final Train: 93.33
   Final Test: 71.40
Split: 01, Run: 02, Epoch: 300, Loss: 0.1361, Train: 100.00%, Valid: 81.40% Test: 79.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4736, Train: 94.17%, Valid: 72.00% Test: 71.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1359, Train: 100.00%, Valid: 79.80% Test: 79.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.3200, Train: 97.50%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0909, Train: 100.00%, Valid: 80.20% Test: 80.00%
Split: 01, Run: 02
None time:  18.31123612821102
None Run 02:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 100.00
   Final Test: 80.40
Split: 01, Run: 03, Epoch: 300, Loss: 0.2875, Train: 98.33%, Valid: 70.40% Test: 69.50%
Split: 01, Run: 03, Epoch: 100, Loss: 0.2029, Train: 98.33%, Valid: 80.60% Test: 79.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2421, Train: 99.17%, Valid: 71.00% Test: 71.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1490, Train: 100.00%, Valid: 80.60% Test: 79.40%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2637, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 03
None time:  18.111651614308357
None Run 03:
Highest Train: 100.00
Highest Valid: 74.00
  Final Train: 95.00
   Final Test: 71.70
total time:  54.846801922656596
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.40 Â± 0.40
  Final Train: 93.89 Â± 0.96
   Final Test: 71.53 Â± 0.15
[32m[I 2021-07-18 22:36:05,662][0m Trial 0 finished with value: 74.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 74.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  74.4    {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
test_acc
['71.53 Â± 0.15']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  74.4000015258789
Best trial Acc:  {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
optuna total time:  54.86304149311036
Split: 01, Run: 03, Epoch: 300, Loss: 0.1375, Train: 98.33%, Valid: 79.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0913, Train: 100.00%, Valid: 80.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0967, Train: 100.00%, Valid: 80.20% Test: 79.90%
Split: 01, Run: 03
None time:  18.13822289183736
None Run 03:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 98.33
   Final Test: 80.10
total time:  57.49892883002758
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.60 Â± 0.20
  Final Train: 98.89 Â± 0.96
   Final Test: 80.37 Â± 0.25
[32m[I 2021-07-18 22:36:13,635][0m Trial 0 finished with value: 82.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 82.5999984741211.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  82.6    {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
test_acc
['80.37 Â± 0.25']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  82.5999984741211
Best trial Acc:  {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
optuna total time:  57.51638145651668
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-18 22:38:02,248][0m A new study created in memory with name: no-name-5f593c6d-ffd8-41a4-9bec-d1e96f9a24fe[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3202, Train: 100.00%, Valid: 76.80% Test: 80.60%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2189, Train: 100.00%, Valid: 75.60% Test: 80.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1985, Train: 100.00%, Valid: 77.00% Test: 79.80%
Using backend: pytorch
Split: 01, Run: 01, Epoch: 400, Loss: 0.1632, Train: 100.00%, Valid: 78.20% Test: 80.80%
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-18 22:38:16,683][0m A new study created in memory with name: no-name-d0cdcfd9-7de7-4177-bfcf-1482a3a99641[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
APPNP(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 01, Epoch: 500, Loss: 0.1969, Train: 100.00%, Valid: 77.00% Test: 79.30%
Split: 01, Run: 01
None time:  15.09240949805826
None Run 01:
Highest Train: 100.00
Highest Valid: 80.80
  Final Train: 100.00
   Final Test: 82.40
Split: 01, Run: 02, Epoch: 100, Loss: 0.3288, Train: 100.00%, Valid: 77.40% Test: 80.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3888, Train: 99.17%, Valid: 72.20% Test: 72.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1927, Train: 100.00%, Valid: 76.80% Test: 80.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2753, Train: 100.00%, Valid: 71.40% Test: 71.10%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-18 22:38:27,998][0m A new study created in memory with name: no-name-9d31d0d2-0557-4170-9028-5802dec5b192[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 300, Loss: 0.1739, Train: 100.00%, Valid: 75.60% Test: 78.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 71.80% Test: 71.70%
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 02, Epoch: 400, Loss: 0.1615, Train: 100.00%, Valid: 76.60% Test: 81.10%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2256, Train: 100.00%, Valid: 71.60% Test: 70.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1700, Train: 100.00%, Valid: 78.60% Test: 81.40%
Split: 01, Run: 02
None time:  15.624389002099633
None Run 02:
Highest Train: 100.00
Highest Valid: 80.80
  Final Train: 100.00
   Final Test: 82.90
Split: 01, Run: 01, Epoch: 100, Loss: 0.1435, Train: 100.00%, Valid: 80.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1848, Train: 100.00%, Valid: 71.20% Test: 71.60%
Split: 01, Run: 01
None time:  16.871882353909314
None Run 01:
Highest Train: 100.00
Highest Valid: 74.00
  Final Train: 98.33
   Final Test: 71.00
Split: 01, Run: 03, Epoch: 100, Loss: 0.3235, Train: 100.00%, Valid: 78.40% Test: 80.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.1022, Train: 100.00%, Valid: 79.80% Test: 79.20%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3841, Train: 100.00%, Valid: 72.00% Test: 71.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2258, Train: 100.00%, Valid: 79.20% Test: 80.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2497, Train: 100.00%, Valid: 71.40% Test: 71.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.0896, Train: 100.00%, Valid: 79.60% Test: 79.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1874, Train: 100.00%, Valid: 77.80% Test: 80.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2154, Train: 100.00%, Valid: 72.60% Test: 72.10%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0910, Train: 100.00%, Valid: 80.20% Test: 79.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2369, Train: 100.00%, Valid: 80.20% Test: 81.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2059, Train: 100.00%, Valid: 71.80% Test: 72.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0655, Train: 100.00%, Valid: 80.80% Test: 79.80%
Split: 01, Run: 01
None time:  19.790470478124917
None Run 01:
Highest Train: 100.00
Highest Valid: 82.00
  Final Train: 100.00
   Final Test: 79.00
Split: 01, Run: 03, Epoch: 500, Loss: 0.1795, Train: 100.00%, Valid: 78.00% Test: 80.90%
Split: 01, Run: 03
None time:  15.916928411461413
None Run 03:
Highest Train: 100.00
Highest Valid: 80.40
  Final Train: 100.00
   Final Test: 81.90
total time:  48.348853858187795
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 80.67 Â± 0.23
  Final Train: 100.00 Â± 0.00
   Final Test: 82.40 Â± 0.50
[32m[I 2021-07-18 22:38:50,607][0m Trial 0 finished with value: 80.66666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 0 with value: 80.66666412353516.[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 500, Loss: 0.1975, Train: 100.00%, Valid: 70.40% Test: 70.80%
Split: 01, Run: 02
None time:  16.295674088411033
None Run 02:
Highest Train: 100.00
Highest Valid: 73.60
  Final Train: 100.00
   Final Test: 72.10
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1484, Train: 100.00%, Valid: 80.40% Test: 78.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4025, Train: 99.17%, Valid: 72.40% Test: 72.70%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1110, Train: 100.00%, Valid: 79.20% Test: 79.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2728, Train: 100.00%, Valid: 70.80% Test: 70.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1074, Train: 100.00%, Valid: 80.20% Test: 79.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2299, Train: 100.00%, Valid: 70.20% Test: 71.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.1949, Train: 100.00%, Valid: 70.60% Test: 71.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1205, Train: 100.00%, Valid: 79.00% Test: 79.40%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  16.378005106933415
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 03, Epoch: 500, Loss: 0.2057, Train: 100.00%, Valid: 69.40% Test: 69.60%
Split: 01, Run: 03
None time:  16.571261917240918
None Run 03:
Highest Train: 100.00
Highest Valid: 73.60
  Final Train: 99.17
   Final Test: 72.60
total time:  51.527642372995615
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 73.73 Â± 0.23
  Final Train: 99.17 Â± 0.83
   Final Test: 71.90 Â± 0.82
[32m[I 2021-07-18 22:39:08,223][0m Trial 0 finished with value: 73.73333740234375 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 0 with value: 73.73333740234375.[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
APPNP(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 500, Loss: 0.0679, Train: 100.00%, Valid: 80.60% Test: 79.50%
Split: 01, Run: 02
None time:  19.258142998442054
None Run 02:
Highest Train: 100.00
Highest Valid: 81.80
  Final Train: 98.33
   Final Test: 79.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 01, Epoch: 100, Loss: 0.4542, Train: 95.83%, Valid: 73.20% Test: 71.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.1717, Train: 100.00%, Valid: 80.00% Test: 79.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.3269, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1275, Train: 100.00%, Valid: 80.60% Test: 79.30%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2871, Train: 99.17%, Valid: 72.60% Test: 70.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0922, Train: 100.00%, Valid: 79.60% Test: 78.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2669, Train: 99.17%, Valid: 70.20% Test: 71.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  15.971533740870655
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 400, Loss: 0.0720, Train: 100.00%, Valid: 79.60% Test: 79.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2309, Train: 98.33%, Valid: 71.20% Test: 71.20%
Split: 01, Run: 01
None time:  16.030674665234983
None Run 01:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 93.33
   Final Test: 71.50
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0775, Train: 100.00%, Valid: 78.60% Test: 78.80%
Split: 01, Run: 03
None time:  18.69286916870624
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 100.00
   Final Test: 79.30
total time:  59.574721567332745
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.80 Â± 0.20
  Final Train: 99.44 Â± 0.96
   Final Test: 79.13 Â± 0.15
[32m[I 2021-07-18 22:39:27,586][0m Trial 0 finished with value: 81.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 0 with value: 81.79999542236328.[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 100, Loss: 0.4669, Train: 95.83%, Valid: 71.60% Test: 70.70%
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.3001, Train: 98.33%, Valid: 71.20% Test: 71.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1734, Train: 98.33%, Valid: 80.80% Test: 78.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2705, Train: 98.33%, Valid: 71.20% Test: 72.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.1367, Train: 100.00%, Valid: 80.60% Test: 80.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2552, Train: 99.17%, Valid: 71.80% Test: 71.80%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  16.110111691989005
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  48.53722541872412
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-18 22:39:39,151][0m Trial 1 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 1 with value: 81.33333587646484.[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 01, Epoch: 300, Loss: 0.1129, Train: 100.00%, Valid: 80.60% Test: 79.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2435, Train: 99.17%, Valid: 71.40% Test: 70.80%
Split: 01, Run: 02
None time:  16.022889602929354
None Run 02:
Highest Train: 100.00
Highest Valid: 74.40
  Final Train: 93.33
   Final Test: 71.40
Split: 01, Run: 01, Epoch: 100, Loss: 0.3318, Train: 100.00%, Valid: 80.20% Test: 82.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1016, Train: 100.00%, Valid: 81.80% Test: 80.20%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4736, Train: 94.17%, Valid: 72.00% Test: 71.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2381, Train: 100.00%, Valid: 76.40% Test: 80.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0852, Train: 100.00%, Valid: 80.40% Test: 80.10%
Split: 01, Run: 01
None time:  18.637702248059213
None Run 01:
Highest Train: 100.00
Highest Valid: 82.80
  Final Train: 98.33
   Final Test: 80.60
Split: 01, Run: 03, Epoch: 200, Loss: 0.3200, Train: 97.50%, Valid: 71.20% Test: 71.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2186, Train: 100.00%, Valid: 79.20% Test: 82.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2875, Train: 98.33%, Valid: 70.40% Test: 69.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1799, Train: 98.33%, Valid: 81.20% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1664, Train: 100.00%, Valid: 79.20% Test: 82.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2421, Train: 99.17%, Valid: 71.00% Test: 71.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1296, Train: 100.00%, Valid: 80.00% Test: 79.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2034, Train: 100.00%, Valid: 80.20% Test: 82.10%
Split: 01, Run: 01
None time:  16.140374157577753
None Run 01:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 100.00
   Final Test: 83.00
Split: 01, Run: 03, Epoch: 500, Loss: 0.2637, Train: 99.17%, Valid: 71.20% Test: 70.30%
Split: 01, Run: 03
None time:  15.86113896407187
None Run 03:
Highest Train: 100.00
Highest Valid: 74.00
  Final Train: 95.00
   Final Test: 71.70
total time:  48.05746981780976
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.40 Â± 0.40
  Final Train: 93.89 Â± 0.96
   Final Test: 71.53 Â± 0.15
[32m[I 2021-07-18 22:39:56,291][0m Trial 1 finished with value: 74.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 1 with value: 74.4000015258789.[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='CiteSeer', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[3327, 3327, nnz=9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])
APPNP(
  (lin1): Linear(in_features=3703, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=6, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 02, Epoch: 300, Loss: 0.1361, Train: 100.00%, Valid: 81.40% Test: 79.70%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3431, Train: 99.29%, Valid: 78.60% Test: 81.30%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3543, Train: 100.00%, Valid: 70.40% Test: 72.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1359, Train: 100.00%, Valid: 79.80% Test: 79.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2087, Train: 100.00%, Valid: 78.60% Test: 82.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2645, Train: 100.00%, Valid: 69.60% Test: 71.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0909, Train: 100.00%, Valid: 80.20% Test: 80.00%
Split: 01, Run: 02
None time:  18.14621487632394
None Run 02:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 100.00
   Final Test: 80.40
Split: 01, Run: 02, Epoch: 300, Loss: 0.1909, Train: 100.00%, Valid: 78.40% Test: 80.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2058, Train: 100.00%, Valid: 71.40% Test: 70.00%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1730, Train: 100.00%, Valid: 79.00% Test: 82.60%
Split: 01, Run: 03, Epoch: 100, Loss: 0.2029, Train: 98.33%, Valid: 80.60% Test: 79.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1979, Train: 100.00%, Valid: 69.40% Test: 71.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1860, Train: 100.00%, Valid: 79.60% Test: 82.10%
Split: 01, Run: 02
None time:  16.24096885509789
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 100.00
   Final Test: 83.90
Split: 01, Run: 03, Epoch: 200, Loss: 0.1490, Train: 100.00%, Valid: 80.60% Test: 79.40%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1545, Train: 100.00%, Valid: 72.40% Test: 72.30%
Split: 01, Run: 01
None time:  15.948430429212749
None Run 01:
Highest Train: 100.00
Highest Valid: 73.20
  Final Train: 100.00
   Final Test: 71.80
Split: 01, Run: 03, Epoch: 100, Loss: 0.3493, Train: 100.00%, Valid: 79.60% Test: 83.20%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3493, Train: 100.00%, Valid: 71.40% Test: 70.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1375, Train: 98.33%, Valid: 79.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2475, Train: 100.00%, Valid: 80.60% Test: 81.90%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2326, Train: 100.00%, Valid: 70.60% Test: 70.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0913, Train: 100.00%, Valid: 80.40% Test: 80.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2056, Train: 100.00%, Valid: 79.20% Test: 81.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1911, Train: 100.00%, Valid: 71.40% Test: 70.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0967, Train: 100.00%, Valid: 80.20% Test: 79.90%
Split: 01, Run: 03
None time:  18.978202989324927
None Run 03:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 98.33
   Final Test: 80.10
total time:  55.98441708087921
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.60 Â± 0.20
  Final Train: 98.89 Â± 0.96
   Final Test: 80.37 Â± 0.25
[32m[I 2021-07-18 22:40:23,581][0m Trial 1 finished with value: 82.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 1 with value: 82.5999984741211.[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 03, Epoch: 400, Loss: 0.2421, Train: 100.00%, Valid: 80.60% Test: 82.70%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1858, Train: 100.00%, Valid: 71.60% Test: 72.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1797, Train: 100.00%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 03
None time:  16.05857662949711
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 83.20
total time:  48.5112023986876
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.40 Â± 0.20
  Final Train: 100.00 Â± 0.00
   Final Test: 83.37 Â± 0.47
[32m[I 2021-07-18 22:40:27,668][0m Trial 2 finished with value: 81.39999389648438 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 2 with value: 81.39999389648438.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  80.667    {'train': '100.00 Â± 0.00', 'valid': '80.67 Â± 0.23', 'test': '82.40 Â± 0.50'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  81.4    {'train': '100.00 Â± 0.00', 'valid': '81.40 Â± 0.20', 'test': '83.37 Â± 0.47'}
test_acc
['82.40 Â± 0.50', '83.77 Â± 0.58', '83.37 Â± 0.47']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}
Best trial Value:  81.39999389648438
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '81.40 Â± 0.20', 'test': '83.37 Â± 0.47'}
optuna total time:  145.4258955642581
Split: 01, Run: 01, Epoch: 100, Loss: 0.1342, Train: 100.00%, Valid: 79.40% Test: 78.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1930, Train: 100.00%, Valid: 70.60% Test: 71.00%
Split: 01, Run: 02
None time:  15.91686368919909
None Run 02:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 71.80
Split: 01, Run: 03, Epoch: 100, Loss: 0.3646, Train: 100.00%, Valid: 72.20% Test: 72.60%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0830, Train: 100.00%, Valid: 78.20% Test: 77.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2393, Train: 100.00%, Valid: 68.80% Test: 69.70%
Split: 01, Run: 01, Epoch: 300, Loss: 0.0832, Train: 100.00%, Valid: 77.40% Test: 78.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2052, Train: 100.00%, Valid: 69.80% Test: 71.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0811, Train: 100.00%, Valid: 78.40% Test: 78.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.1703, Train: 100.00%, Valid: 70.40% Test: 70.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 100.00%, Valid: 79.00% Test: 79.00%
Split: 01, Run: 01
None time:  17.759463883936405
None Run 01:
Highest Train: 100.00
Highest Valid: 80.60
  Final Train: 100.00
   Final Test: 79.10
Split: 01, Run: 03, Epoch: 500, Loss: 0.1788, Train: 100.00%, Valid: 68.80% Test: 68.90%
Split: 01, Run: 03
None time:  15.664571977220476
None Run 03:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 71.60
total time:  47.67135371826589
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 73.07 Â± 0.12
  Final Train: 100.00 Â± 0.00
   Final Test: 71.73 Â± 0.12
[32m[I 2021-07-18 22:40:43,973][0m Trial 2 finished with value: 73.06666564941406 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 1 with value: 74.4000015258789.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  73.733    {'train': '99.17 Â± 0.83', 'valid': '73.73 Â± 0.23', 'test': '71.90 Â± 0.82'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  74.4    {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  73.067    {'train': '100.00 Â± 0.00', 'valid': '73.07 Â± 0.12', 'test': '71.73 Â± 0.12'}
test_acc
['71.90 Â± 0.82', '71.53 Â± 0.15', '71.73 Â± 0.12']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  74.4000015258789
Best trial Acc:  {'train': '93.89 Â± 0.96', 'valid': '74.40 Â± 0.40', 'test': '71.53 Â± 0.15'}
optuna total time:  147.29493342805654
Split: 01, Run: 02, Epoch: 100, Loss: 0.1407, Train: 100.00%, Valid: 78.20% Test: 78.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0907, Train: 100.00%, Valid: 76.60% Test: 78.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1068, Train: 100.00%, Valid: 78.20% Test: 78.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1035, Train: 100.00%, Valid: 78.20% Test: 78.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0573, Train: 100.00%, Valid: 78.80% Test: 79.40%
Split: 01, Run: 02
None time:  16.04572687111795
None Run 02:
Highest Train: 100.00
Highest Valid: 80.20
  Final Train: 100.00
   Final Test: 79.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.1630, Train: 100.00%, Valid: 78.40% Test: 78.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1255, Train: 100.00%, Valid: 78.40% Test: 78.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1045, Train: 100.00%, Valid: 79.60% Test: 78.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0791, Train: 100.00%, Valid: 78.40% Test: 78.70%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0694, Train: 100.00%, Valid: 77.60% Test: 77.70%
Split: 01, Run: 03
None time:  17.053635846823454
None Run 03:
Highest Train: 100.00
Highest Valid: 80.00
  Final Train: 100.00
   Final Test: 79.00
total time:  51.07787294499576
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 80.27 Â± 0.31
  Final Train: 100.00 Â± 0.00
   Final Test: 79.07 Â± 0.06
[32m[I 2021-07-18 22:41:14,668][0m Trial 2 finished with value: 80.26666259765625 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 1 with value: 82.5999984741211.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  81.8    {'train': '99.44 Â± 0.96', 'valid': '81.80 Â± 0.20', 'test': '79.13 Â± 0.15'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  82.6    {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  80.267    {'train': '100.00 Â± 0.00', 'valid': '80.27 Â± 0.31', 'test': '79.07 Â± 0.06'}
test_acc
['79.13 Â± 0.15', '80.37 Â± 0.25', '79.07 Â± 0.06']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  82.5999984741211
Best trial Acc:  {'train': '98.89 Â± 0.96', 'valid': '82.60 Â± 0.20', 'test': '80.37 Â± 0.25'}
optuna total time:  166.6750308247283
  File "main_optuna.py", line 9
    from train_eval import train, test, 
                          ^
SyntaxError: trailing comma not allowed without surrounding parentheses
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-18 22:56:21,487][0m A new study created in memory with name: no-name-6e7ae8f0-6ff9-426e-975d-0d7c1d0f2943[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(79)[0;36mforward[0;34m()[0m
[0;32m     78 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 79 [0;31m        [0;32massert[0m [0misinstance[0m[0;34m([0m[0medge_index[0m[0;34m,[0m [0mSparseTensor[0m[0;34m)[0m[0;34m,[0m [0;34m"Only support SparseTensor now"[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     80 [0;31m        [0medge_weight[0m [0;34m==[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707], device='cuda:0'),
             col=tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706], device='cuda:0'),
             size=(2708, 2708), nnz=10556, density=0.14%)
ipdb> SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707], device='cuda:0'),
             col=tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706], device='cuda:0'),
             size=(2708, 2708), nnz=10556, density=0.14%)
ipdb> ipdb> ipdb> ipdb> SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707], device='cuda:0'),
             col=tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706], device='cuda:0'),
             size=(2708, 2708), nnz=10556, density=0.14%)
ipdb> Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:12:52,149][0m A new study created in memory with name: no-name-9ace5e54-06a3-45bf-a7c5-6cfb7109d284[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:12:59,095][0m Trial 0 failed because of the following error: NameError("name 'SparseTensor' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    assert isinstance(edge_index, SparseTensor), "Only support SparseTensor now"
NameError: name 'SparseTensor' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 316, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    assert isinstance(edge_index, SparseTensor), "Only support SparseTensor now"
NameError: name 'SparseTensor' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:18:58,657][0m A new study created in memory with name: no-name-aba94045-fe62-4d88-9c52-eef3cb15e98b[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:19:05,439][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'propagate_update'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 316, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:26:12,358][0m A new study created in memory with name: no-name-e221604b-0827-443d-ac82-4483981d424f[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:26:19,109][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'propagate_update'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 316, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:29:14,670][0m A new study created in memory with name: no-name-9d3ad501-6b74-4230-b1db-ae2b38141f91[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:29:19,297][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'propagate_update'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 316, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'propagate_update'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:32:22,597][0m A new study created in memory with name: no-name-c88eaa1d-d277-4634-bfeb-7ee99291bca7[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:32:27,226][0m Trial 0 failed because of the following error: NameError("name 'label' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 17, in train_altopt
    label == model.FF
NameError: name 'label' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 316, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 17, in train_altopt
    label == model.FF
NameError: name 'label' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:33:05,085][0m A new study created in memory with name: no-name-c0a0aec9-8a03-4891-b697-ac8695b89551[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 17:33:07,140][0m Trial 0 failed because of the following error: AttributeError("'NoneType' object has no attribute 'model'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 64, in test
    if args.model == 'ALTOPT':
AttributeError: 'NoneType' object has no attribute 'model'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 317, in <module>
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 64, in test
    if args.model == 'ALTOPT':
AttributeError: 'NoneType' object has no attribute 'model'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:34:31,963][0m A new study created in memory with name: no-name-3fc5ef0f-9892-49b4-8340-57399ee13219[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=True, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 300, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 400, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 500, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 01
None time:  16.08441505022347
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 02, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 300, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 400, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02, Epoch: 500, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 02
None time:  16.613551303744316
None Run 02:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.40
Split: 01, Run: 03, Epoch: 100, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Split: 01, Run: 03, Epoch: 200, Loss: 1.7397, Train: 100.00%, Valid: 71.00% Test: 73.40%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:35:30,633][0m A new study created in memory with name: no-name-840be12a-ba17-4a96-b9b6-56eed6fbf4d7[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  14.550360158085823
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:35:54,720][0m A new study created in memory with name: no-name-6b82dee4-fa88-497f-84da-f2775aba1749[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  15.220385944470763
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  16.399829235859215
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  15.943751848302782
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  47.45067986380309
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-19 17:36:18,093][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  47.46434502862394
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  14.520557289011776
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  13.865578492172062
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  46.52897589188069
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-19 17:36:41,259][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  46.54254460800439
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 17:50:44,834][0m A new study created in memory with name: no-name-f320cfe9-86d4-4769-974e-4e7f127bd9fa[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  15.183950318954885
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  14.485151746310294
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  14.288480391725898
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  50.084336472675204
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-19 17:51:34,981][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  50.151770238764584
Traceback (most recent call last):
  File "main_optuna.py", line 8, in <module>
    from get_model import get_model
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 4, in <module>
    from prop import Propagation
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 95
    def apt_forward(self, mlp, x=FF, edge_index, K, alpha, data):
                   ^
SyntaxError: non-default argument follows default argument
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:00:48,905][0m A new study created in memory with name: no-name-af90ab9c-5cdc-4f94-957e-cf805b8168df[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:00:51,121][0m Trial 0 failed because of the following error: TypeError("forward() got an unexpected keyword argument 'mode'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'mode'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'mode'
Traceback (most recent call last):
  File "main_optuna.py", line 8, in <module>
    from get_model import get_model
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 4, in <module>
    from prop import Propagation
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 83
    mode = self.mode if mode == None
                                   ^
SyntaxError: invalid syntax
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:02:24,634][0m A new study created in memory with name: no-name-bc1c93df-54e6-4dc1-a847-6431cf9991b8[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:02:26,735][0m Trial 0 failed because of the following error: NameError("name 'x' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(x).cuda()
NameError: name 'x' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(x).cuda()
NameError: name 'x' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:02:46,213][0m A new study created in memory with name: no-name-138ed859-b486-4f00-ae74-bda9ab960de6[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:02:48,233][0m Trial 0 failed because of the following error: TypeError("zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(FF).cuda()
TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 85, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(FF).cuda()
TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:03:25,741][0m A new study created in memory with name: no-name-5b4d360d-3296-486b-b9a6-45c6356a851c[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:03:27,767][0m Trial 0 failed because of the following error: TypeError("zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(FF).cuda()
TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 99, in apt_forward
    label = torch.zeros_like(FF).cuda()
TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:03:42,994][0m A new study created in memory with name: no-name-b3517ec6-794f-446b-8f20-8b57f9950753[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(105)[0;36mapt_forward[0;34m()[0m
[0;32m    104 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 105 [0;31m        [0;32mfor[0m [0mk[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mK[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    106 [0;31m            [0mAF[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mpropagate[0m[0;34m([0m[0medge_index[0m[0;34m,[0m [0mx[0m[0;34m=[0m[0mFF[0m[0;34m,[0m [0medge_weight[0m[0;34m=[0m[0;32mNone[0m[0;34m,[0m [0msize[0m[0;34m=[0m[0;32mNone[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 18:03:51,410][0m Trial 0 failed because of the following error: RuntimeError('shape mismatch: value tensor of shape [2708, 7] cannot be broadcast to indexing result of shape [140, 7]')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 105, in apt_forward
    for k in range(K):
RuntimeError: shape mismatch: value tensor of shape [2708, 7] cannot be broadcast to indexing result of shape [140, 7][0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 105, in apt_forward
    for k in range(K):
RuntimeError: shape mismatch: value tensor of shape [2708, 7] cannot be broadcast to indexing result of shape [140, 7]

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:04:11,688][0m A new study created in memory with name: no-name-0c99b2c4-3d93-4049-9564-e8f96006df30[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(105)[0;36mapt_forward[0;34m()[0m
[0;32m    104 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 105 [0;31m        [0;32mfor[0m [0mk[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mK[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    106 [0;31m            [0mAF[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mpropagate[0m[0;34m([0m[0medge_index[0m[0;34m,[0m [0mx[0m[0;34m=[0m[0mFF[0m[0;34m,[0m [0medge_weight[0m[0;34m=[0m[0;32mNone[0m[0;34m,[0m [0msize[0m[0;34m=[0m[0;32mNone[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'AF' is not defined
ipdb> ipdb> tensor([[ 0.1063, -0.0021, -0.0865,  ..., -0.1046,  0.1134, -0.1030],
        [ 0.1197, -0.0104, -0.0914,  ..., -0.1147,  0.1274, -0.1241],
        [ 0.1104, -0.0069, -0.0901,  ..., -0.1088,  0.1239, -0.1074],
        ...,
        [ 0.0985, -0.0074, -0.0804,  ..., -0.1010,  0.1187, -0.1056],
        [ 0.1131, -0.0053, -0.0884,  ..., -0.1120,  0.1265, -0.1056],
        [ 0.0937, -0.0047, -0.0764,  ..., -0.0909,  0.1026, -0.0857]],
       device='cuda:0')
ipdb> torch.Size([2708, 7])
ipdb> torch.Size([140, 7])
ipdb> torch.Size([2568, 7])
ipdb> [33m[W 2021-07-19 18:05:17,954][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 105, in apt_forward
    for k in range(K):
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 105, in apt_forward
    for k in range(K):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:05:30,076][0m A new study created in memory with name: no-name-da9e4aa2-781f-4e32-a838-fe6985be668b[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(105)[0;36mapt_forward[0;34m()[0m
[0;32m    104 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 105 [0;31m        [0;32mfor[0m [0mk[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mK[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    106 [0;31m            [0mAF[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mpropagate[0m[0;34m([0m[0medge_index[0m[0;34m,[0m [0mx[0m[0;34m=[0m[0mFF[0m[0;34m,[0m [0medge_weight[0m[0;34m=[0m[0;32mNone[0m[0;34m,[0m [0msize[0m[0;34m=[0m[0;32mNone[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 18:05:39,791][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:06:19,749][0m A new study created in memory with name: no-name-1d24764a-1698-4732-8138-6072cd195acd[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 18:06:23,892][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:06:44,672][0m A new study created in memory with name: no-name-b131d762-9ae0-4145-a91c-47952c5652f1[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 18:06:52,030][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    x = self.FF = self.prop(x, adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:07:00,219][0m A new study created in memory with name: no-name-1e75254e-623d-4061-9e9d-bde8868985ed[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> tensor([[ 0.0953, -0.0032, -0.0757,  ..., -0.0927,  0.1014, -0.0924],
        [ 0.0994, -0.0054, -0.0789,  ..., -0.0970,  0.1065, -0.0983],
        [ 0.1177, -0.0056, -0.0936,  ..., -0.1151,  0.1261, -0.1151],
        ...,
        [ 0.0985, -0.0074, -0.0804,  ..., -0.1010,  0.1187, -0.1056],
        [ 0.1017, -0.0045, -0.0814,  ..., -0.0999,  0.1106, -0.0970],
        [ 0.1017, -0.0045, -0.0815,  ..., -0.0997,  0.1101, -0.0971]],
       device='cuda:0', grad_fn=<IndexPutBackward>)
ipdb> [33m[W 2021-07-19 18:10:05,648][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    # loss = train(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 70, in train
    out = model(data=data)[train_idx]
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 87, in forward
    # x = self.propagate_update(data, K=args.K)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 111, in apt_forward
    return FF
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 111, in apt_forward
    return FF
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:10:20,351][0m A new study created in memory with name: no-name-becf5810-558c-46d8-8fcd-b7e92c24d0bb[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:10:22,113][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'mlp'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 76, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'mlp'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 76, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'mlp'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:33:26,992][0m A new study created in memory with name: no-name-36e65526-0031-4bf6-a0c8-bd2a9f29e8f9[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:33:28,836][0m Trial 0 failed because of the following error: ModuleAttributeError("'ALTOPT' object has no attribute 'mlp'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 76, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'mlp'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 76, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'ALTOPT' object has no attribute 'mlp'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:34:09,411][0m A new study created in memory with name: no-name-ae60bb10-f65b-4085-a43c-dca4d960cf54[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:34:11,426][0m Trial 0 failed because of the following error: NameError("name 'label' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 17, in train_altopt
    label == model.FF
NameError: name 'label' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 17, in train_altopt
    label == model.FF
NameError: name 'label' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:34:31,684][0m A new study created in memory with name: no-name-5b7f6767-7555-4326-b3ba-7ddc4593ef02[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:34:33,498][0m Trial 0 failed because of the following error: RuntimeError('The size of tensor a (140) must match the size of tensor b (2708) at non-singleton dimension 0')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train_altopt
    loss = torch.pow(torch.norm(out-label), 2)
RuntimeError: The size of tensor a (140) must match the size of tensor b (2708) at non-singleton dimension 0[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 125, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 18, in train_altopt
    loss = torch.pow(torch.norm(out-label), 2)
RuntimeError: The size of tensor a (140) must match the size of tensor b (2708) at non-singleton dimension 0
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:34:54,855][0m A new study created in memory with name: no-name-a37f0249-76d7-4b8b-b7c4-24d272b88738[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:34:56,746][0m Trial 0 failed because of the following error: NameError("name 'test_altopt' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx)
NameError: name 'test_altopt' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx)
NameError: name 'test_altopt' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:35:14,886][0m A new study created in memory with name: no-name-edeeb0b7-1c44-4327-ad18-7eed0dfcadd3[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:35:16,801][0m Trial 0 failed because of the following error: AttributeError("'NoneType' object has no attribute 'model'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 37, in test_altopt
    if args.model == 'ALTOPT':
AttributeError: 'NoneType' object has no attribute 'model'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 37, in test_altopt
    if args.model == 'ALTOPT':
AttributeError: 'NoneType' object has no attribute 'model'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:35:47,026][0m A new study created in memory with name: no-name-a8630d02-2a63-42f0-9314-1fe3794a2513[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 18:35:48,914][0m Trial 0 failed because of the following error: NameError("name 'adj_t' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 78, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
NameError: name 'adj_t' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 78, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=adj_t, data=data, FF=self.FF, mode='ALTOPT')
NameError: name 'adj_t' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 18:36:11,991][0m A new study created in memory with name: no-name-f8c12b69-66c6-4762-b892-34ff53b5cfd4[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 18:36:23,847][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:48:21,828][0m A new study created in memory with name: no-name-004a0f22-e8c3-4c65-a5a7-e62f758a5383[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'mode' is not defined
ipdb> 'APPNP'
ipdb> [33m[W 2021-07-19 19:50:30,120][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 111, in apt_forward
    return FF
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 111, in apt_forward
    return FF
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:50:40,592][0m A new study created in memory with name: no-name-94de5c6f-0aba-4ee9-82df-574e1cd5b665[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> ipdb> tensor([[ 0.0953, -0.0032, -0.0757,  ..., -0.0927,  0.1014, -0.0924],
        [ 0.0994, -0.0054, -0.0789,  ..., -0.0970,  0.1065, -0.0983],
        [ 0.1177, -0.0056, -0.0936,  ..., -0.1151,  0.1261, -0.1151],
        ...,
        [ 0.0985, -0.0074, -0.0804,  ..., -0.1010,  0.1187, -0.1056],
        [ 0.1017, -0.0045, -0.0814,  ..., -0.0999,  0.1106, -0.0970],
        [ 0.1017, -0.0045, -0.0815,  ..., -0.0997,  0.1101, -0.0971]],
       device='cuda:0')
ipdb> [33m[W 2021-07-19 19:51:00,928][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:51:30,971][0m A new study created in memory with name: no-name-22b8dbda-3614-48be-861a-f3d9e9df677f[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(111)[0;36mapt_forward[0;34m()[0m
[0;32m    110 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 111 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    112 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 19:51:37,813][0m Trial 0 failed because of the following error: ValueError('wrong propagate mode')
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode[0m
Traceback (most recent call last):
  File "main_optuna.py", line 319, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 93, in forward
    raise ValueError('wrong propagate mode')
ValueError: wrong propagate mode

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:51:56,308][0m A new study created in memory with name: no-name-f2a2525f-dc18-458f-940d-e2886225f75e[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> test
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(113)[0;36mapt_forward[0;34m()[0m
[0;32m    112 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 113 [0;31m        [0;32mreturn[0m [0mFF[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    114 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-19 19:52:03,416][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 113, in apt_forward
    return FF
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 113, in apt_forward
    return FF
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Traceback (most recent call last):
  File "main_optuna.py", line 8, in <module>
    from get_model import get_model
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 4, in <module>
    from prop import Propagation
ImportError: cannot import name 'Propagation' from 'prop' (/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py)
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:52:23,689][0m A new study created in memory with name: no-name-c8a462ca-e453-4d0b-bc75-c3598d8f247c[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
test
Split: 01, Run: 01, Epoch: 100, Loss: 70091.8203, Train: 14.29%, Valid: 11.00% Test: 10.30%
test
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:52:45,334][0m A new study created in memory with name: no-name-13d5ae97-bab9-4d81-8117-7a58de10afd9[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 70094.3750, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 40, Loss: 70092.0469, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 60, Loss: 70091.9609, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 80, Loss: 70091.8984, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 100, Loss: 70091.8203, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 120, Loss: 70091.8047, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 140, Loss: 70091.8047, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 160, Loss: 70091.8047, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 180, Loss: 70091.7578, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 200, Loss: 70091.7891, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 220, Loss: 70091.7578, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 240, Loss: 70091.6875, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 260, Loss: 70091.7578, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 280, Loss: 70091.7031, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 300, Loss: 70091.6719, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 320, Loss: 70091.6406, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 340, Loss: 70091.6250, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 360, Loss: 70091.6562, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 380, Loss: 70091.6094, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 400, Loss: 70091.6719, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 420, Loss: 70091.6094, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 440, Loss: 70091.5781, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 460, Loss: 70091.6250, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 480, Loss: 70091.6094, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01, Epoch: 500, Loss: 70091.5781, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 01
None time:  11.6315259905532
None Run 01:
Highest Train: 15.71
Highest Valid: 11.40
  Final Train: 15.71
   Final Test: 10.60
Split: 01, Run: 02, Epoch: 20, Loss: 70095.2734, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 40, Loss: 70092.2578, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 60, Loss: 70092.0781, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 80, Loss: 70091.7891, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 100, Loss: 70091.4766, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 120, Loss: 70091.4141, Train: 14.29%, Valid: 11.00% Test: 10.30%
Split: 01, Run: 02, Epoch: 140, Loss: 70091.3828, Train: 14.29%, Valid: 11.00% Test: 10.30%
Using backend: pytorch
usage: main_optuna.py [-h] [--device DEVICE] [--log_steps LOG_STEPS]
                      [--dataset DATASET] [--model MODEL]
                      [--num_layers NUM_LAYERS]
                      [--hidden_channels HIDDEN_CHANNELS] [--dropout DROPOUT]
                      [--weight_decay WEIGHT_DECAY] [--lr LR]
                      [--epochs EPOCHS] [--runs RUNS]
                      [--normalize_features NORMALIZE_FEATURES]
                      [--random_splits RANDOM_SPLITS] [--seed SEED]
                      [--prop PROP] [--K K] [--gamma GAMMA]
                      [--lambda1 LAMBDA1] [--lambda2 LAMBDA2] [--L21 L21]
                      [--alpha ALPHA] [--defense DEFENSE]
                      [--ptb_rate PTB_RATE] [--sort_key SORT_KEY]
                      [--debug DEBUG] [--loss LOSS] [--LP LP]
main_optuna.py: error: unrecognized arguments: --0.1
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [0.5], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 19:53:59,892][0m A new study created in memory with name: no-name-7dacb9e7-341d-4ed0-88bc-4feb57de375b[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 73253.6328, Train: 100.00%, Valid: 70.20% Test: 71.40%
Split: 01, Run: 01, Epoch: 40, Loss: 73723.9844, Train: 100.00%, Valid: 71.80% Test: 73.40%
Split: 01, Run: 01, Epoch: 60, Loss: 73725.5703, Train: 100.00%, Valid: 72.60% Test: 74.90%
Split: 01, Run: 01, Epoch: 80, Loss: 73706.5859, Train: 100.00%, Valid: 72.20% Test: 73.80%
Split: 01, Run: 01, Epoch: 100, Loss: 73730.4609, Train: 100.00%, Valid: 72.00% Test: 74.00%
Split: 01, Run: 01, Epoch: 120, Loss: 73735.8984, Train: 100.00%, Valid: 71.20% Test: 73.20%
Split: 01, Run: 01, Epoch: 140, Loss: 73758.8750, Train: 100.00%, Valid: 70.40% Test: 73.00%
Split: 01, Run: 01, Epoch: 160, Loss: 73766.0000, Train: 100.00%, Valid: 71.20% Test: 72.40%
Split: 01, Run: 01, Epoch: 180, Loss: 73772.6328, Train: 100.00%, Valid: 71.00% Test: 72.50%
Split: 01, Run: 01, Epoch: 200, Loss: 73786.3281, Train: 100.00%, Valid: 70.20% Test: 72.30%
Split: 01, Run: 01, Epoch: 220, Loss: 73788.0781, Train: 100.00%, Valid: 70.80% Test: 72.70%
Split: 01, Run: 01, Epoch: 240, Loss: 73789.9688, Train: 100.00%, Valid: 70.60% Test: 72.20%
Split: 01, Run: 01, Epoch: 260, Loss: 73786.4375, Train: 100.00%, Valid: 70.20% Test: 72.60%
Split: 01, Run: 01, Epoch: 280, Loss: 73793.7188, Train: 100.00%, Valid: 70.80% Test: 73.70%
Split: 01, Run: 01, Epoch: 300, Loss: 73794.9609, Train: 100.00%, Valid: 70.20% Test: 73.60%
Split: 01, Run: 01, Epoch: 320, Loss: 73802.1562, Train: 100.00%, Valid: 70.20% Test: 73.30%
Split: 01, Run: 01, Epoch: 340, Loss: 73798.1797, Train: 100.00%, Valid: 71.60% Test: 73.30%
Split: 01, Run: 01, Epoch: 360, Loss: 73800.4375, Train: 100.00%, Valid: 71.80% Test: 72.80%
Split: 01, Run: 01, Epoch: 380, Loss: 73807.1484, Train: 100.00%, Valid: 70.40% Test: 72.50%
Split: 01, Run: 01, Epoch: 400, Loss: 73824.5312, Train: 100.00%, Valid: 70.60% Test: 72.90%
Split: 01, Run: 01, Epoch: 420, Loss: 73822.6875, Train: 100.00%, Valid: 71.40% Test: 73.60%
Split: 01, Run: 01, Epoch: 440, Loss: 73831.0625, Train: 100.00%, Valid: 70.00% Test: 71.90%
Split: 01, Run: 01, Epoch: 460, Loss: 73832.0938, Train: 100.00%, Valid: 70.20% Test: 72.50%
Split: 01, Run: 01, Epoch: 480, Loss: 73842.9375, Train: 100.00%, Valid: 71.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 500, Loss: 73836.9688, Train: 100.00%, Valid: 71.00% Test: 72.40%
Split: 01, Run: 01
None time:  11.705926359631121
None Run 01:
Highest Train: 100.00
Highest Valid: 73.60
  Final Train: 100.00
   Final Test: 73.80
total time:  13.431312495842576
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 73.60 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 73.80 Â± nan
[32m[I 2021-07-19 19:54:13,333][0m Trial 0 finished with value: 73.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 73.5999984741211.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  73.6    {'train': '100.00 Â± nan', 'valid': '73.60 Â± nan', 'test': '73.80 Â± nan'}
test_acc
['73.80 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  73.5999984741211
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '73.60 Â± nan', 'test': '73.80 Â± nan'}
optuna total time:  13.44484281167388
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [0.5], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 19:55:08,565][0m A new study created in memory with name: no-name-5ee2d096-0cc6-46fe-b0f9-8033a20540af[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 72485.0938, Train: 100.00%, Valid: 42.80% Test: 46.30%
Split: 01, Run: 01, Epoch: 40, Loss: 73370.5703, Train: 100.00%, Valid: 65.80% Test: 65.80%
Split: 01, Run: 01, Epoch: 60, Loss: 73605.7734, Train: 100.00%, Valid: 70.40% Test: 71.40%
Split: 01, Run: 01, Epoch: 80, Loss: 73608.3516, Train: 100.00%, Valid: 72.20% Test: 73.60%
Split: 01, Run: 01, Epoch: 100, Loss: 73628.5234, Train: 100.00%, Valid: 72.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 120, Loss: 73646.6094, Train: 100.00%, Valid: 72.60% Test: 73.50%
Split: 01, Run: 01, Epoch: 140, Loss: 73669.7656, Train: 100.00%, Valid: 72.00% Test: 73.50%
Split: 01, Run: 01, Epoch: 160, Loss: 73683.4062, Train: 100.00%, Valid: 71.20% Test: 72.30%
Split: 01, Run: 01, Epoch: 180, Loss: 73696.6250, Train: 100.00%, Valid: 71.20% Test: 72.20%
Split: 01, Run: 01, Epoch: 200, Loss: 73708.1875, Train: 100.00%, Valid: 71.40% Test: 72.10%
Split: 01, Run: 01, Epoch: 220, Loss: 73718.7969, Train: 100.00%, Valid: 70.80% Test: 72.60%
Split: 01, Run: 01, Epoch: 240, Loss: 73726.9141, Train: 100.00%, Valid: 70.20% Test: 71.90%
Split: 01, Run: 01, Epoch: 260, Loss: 73724.8438, Train: 100.00%, Valid: 70.40% Test: 71.90%
Split: 01, Run: 01, Epoch: 280, Loss: 73733.1484, Train: 100.00%, Valid: 71.00% Test: 72.70%
Split: 01, Run: 01, Epoch: 300, Loss: 73734.7734, Train: 100.00%, Valid: 70.00% Test: 72.40%
Split: 01, Run: 01, Epoch: 320, Loss: 73737.0391, Train: 100.00%, Valid: 71.60% Test: 72.10%
Split: 01, Run: 01, Epoch: 340, Loss: 73739.4297, Train: 100.00%, Valid: 70.80% Test: 72.30%
Split: 01, Run: 01, Epoch: 360, Loss: 73743.9062, Train: 100.00%, Valid: 70.80% Test: 71.80%
Split: 01, Run: 01, Epoch: 380, Loss: 73745.5312, Train: 100.00%, Valid: 70.40% Test: 72.30%
Split: 01, Run: 01, Epoch: 400, Loss: 73756.6016, Train: 100.00%, Valid: 71.20% Test: 72.30%
Split: 01, Run: 01, Epoch: 420, Loss: 73754.8906, Train: 100.00%, Valid: 70.80% Test: 72.20%
Split: 01, Run: 01, Epoch: 440, Loss: 73761.1797, Train: 100.00%, Valid: 71.00% Test: 71.60%
Split: 01, Run: 01, Epoch: 460, Loss: 73756.1562, Train: 100.00%, Valid: 71.00% Test: 72.30%
Split: 01, Run: 01, Epoch: 480, Loss: 73759.4375, Train: 100.00%, Valid: 70.80% Test: 72.30%
Split: 01, Run: 01, Epoch: 500, Loss: 73757.2812, Train: 100.00%, Valid: 71.00% Test: 72.00%
Split: 01, Run: 01
None time:  5.776347431354225
None Run 01:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 73.70
total time:  7.433506326749921
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 73.40 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 73.70 Â± nan
[32m[I 2021-07-19 19:55:16,008][0m Trial 0 finished with value: 73.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 73.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  73.4    {'train': '100.00 Â± nan', 'valid': '73.40 Â± nan', 'test': '73.70 Â± nan'}
test_acc
['73.70 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  73.4000015258789
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '73.40 Â± nan', 'test': '73.70 Â± nan'}
optuna total time:  7.447918192483485
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [10.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 19:55:29,851][0m A new study created in memory with name: no-name-2608473f-18dc-4ac7-a2d0-ade1663815be[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 110145.4688, Train: 100.00%, Valid: 68.40% Test: 69.90%
Split: 01, Run: 01, Epoch: 40, Loss: 111143.9609, Train: 100.00%, Valid: 71.80% Test: 74.10%
Split: 01, Run: 01, Epoch: 60, Loss: 110735.8281, Train: 100.00%, Valid: 71.80% Test: 74.30%
Split: 01, Run: 01, Epoch: 80, Loss: 110739.4609, Train: 100.00%, Valid: 72.80% Test: 75.10%
Split: 01, Run: 01, Epoch: 100, Loss: 110842.7422, Train: 100.00%, Valid: 72.60% Test: 74.50%
Split: 01, Run: 01, Epoch: 120, Loss: 110791.3828, Train: 100.00%, Valid: 72.20% Test: 74.10%
Split: 01, Run: 01, Epoch: 140, Loss: 110814.5000, Train: 100.00%, Valid: 72.20% Test: 75.30%
Split: 01, Run: 01, Epoch: 160, Loss: 110820.7812, Train: 100.00%, Valid: 73.00% Test: 74.30%
Split: 01, Run: 01, Epoch: 180, Loss: 110816.1250, Train: 100.00%, Valid: 71.60% Test: 74.20%
Split: 01, Run: 01, Epoch: 200, Loss: 110744.9219, Train: 100.00%, Valid: 72.40% Test: 74.10%
Split: 01, Run: 01, Epoch: 220, Loss: 110906.1328, Train: 100.00%, Valid: 71.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 240, Loss: 110890.6875, Train: 100.00%, Valid: 70.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 260, Loss: 110876.0156, Train: 100.00%, Valid: 72.60% Test: 74.50%
Split: 01, Run: 01, Epoch: 280, Loss: 110759.7734, Train: 100.00%, Valid: 72.00% Test: 74.90%
Split: 01, Run: 01, Epoch: 300, Loss: 110897.4141, Train: 100.00%, Valid: 71.20% Test: 74.30%
Split: 01, Run: 01, Epoch: 320, Loss: 110937.7656, Train: 100.00%, Valid: 71.60% Test: 74.40%
Split: 01, Run: 01, Epoch: 340, Loss: 110870.9531, Train: 100.00%, Valid: 72.60% Test: 74.60%
Split: 01, Run: 01, Epoch: 360, Loss: 110800.9688, Train: 100.00%, Valid: 72.00% Test: 74.40%
Split: 01, Run: 01, Epoch: 380, Loss: 110868.1250, Train: 100.00%, Valid: 72.40% Test: 74.00%
Split: 01, Run: 01, Epoch: 400, Loss: 110861.6016, Train: 100.00%, Valid: 72.60% Test: 74.80%
Split: 01, Run: 01, Epoch: 420, Loss: 110848.7812, Train: 100.00%, Valid: 73.20% Test: 74.50%
Split: 01, Run: 01, Epoch: 440, Loss: 110904.6328, Train: 100.00%, Valid: 72.20% Test: 74.70%
Split: 01, Run: 01, Epoch: 460, Loss: 110889.4844, Train: 100.00%, Valid: 73.40% Test: 74.40%
Split: 01, Run: 01, Epoch: 480, Loss: 110893.9375, Train: 100.00%, Valid: 72.00% Test: 74.10%
Split: 01, Run: 01, Epoch: 500, Loss: 110877.8594, Train: 100.00%, Valid: 73.00% Test: 73.80%
Split: 01, Run: 01
None time:  5.620110438205302
None Run 01:
Highest Train: 100.00
Highest Valid: 73.80
  Final Train: 100.00
   Final Test: 74.40
total time:  7.3411416141316295
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 73.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.40 Â± nan
[32m[I 2021-07-19 19:55:37,201][0m Trial 0 finished with value: 73.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 73.79999542236328.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  73.8    {'train': '100.00 Â± nan', 'valid': '73.80 Â± nan', 'test': '74.40 Â± nan'}
test_acc
['74.40 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  73.79999542236328
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '73.80 Â± nan', 'test': '74.40 Â± nan'}
optuna total time:  7.3539928663522005
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 19:55:50,810][0m A new study created in memory with name: no-name-8b089219-1417-46ac-97a6-a64e61a1d8f2[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 2011046.3750, Train: 100.00%, Valid: 72.00% Test: 72.90%
Split: 01, Run: 01, Epoch: 40, Loss: 2043888.3750, Train: 100.00%, Valid: 73.40% Test: 75.30%
Split: 01, Run: 01, Epoch: 60, Loss: 2041180.3750, Train: 100.00%, Valid: 73.00% Test: 75.60%
Split: 01, Run: 01, Epoch: 80, Loss: 2036823.3750, Train: 100.00%, Valid: 73.20% Test: 74.80%
Split: 01, Run: 01, Epoch: 100, Loss: 2036151.6250, Train: 100.00%, Valid: 73.20% Test: 74.40%
Split: 01, Run: 01, Epoch: 120, Loss: 2034618.1250, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 140, Loss: 2035413.8750, Train: 100.00%, Valid: 73.20% Test: 75.00%
Split: 01, Run: 01, Epoch: 160, Loss: 2035377.3750, Train: 100.00%, Valid: 73.20% Test: 74.90%
Split: 01, Run: 01, Epoch: 180, Loss: 2035718.6250, Train: 100.00%, Valid: 72.00% Test: 74.80%
Split: 01, Run: 01, Epoch: 200, Loss: 2034227.3750, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 220, Loss: 2036430.7500, Train: 100.00%, Valid: 73.00% Test: 75.20%
Split: 01, Run: 01, Epoch: 240, Loss: 2035752.1250, Train: 100.00%, Valid: 73.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 260, Loss: 2035163.7500, Train: 100.00%, Valid: 73.20% Test: 74.30%
Split: 01, Run: 01, Epoch: 280, Loss: 2034475.2500, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 300, Loss: 2035392.6250, Train: 100.00%, Valid: 73.00% Test: 74.50%
Split: 01, Run: 01, Epoch: 320, Loss: 2035544.1250, Train: 100.00%, Valid: 73.60% Test: 74.30%
Split: 01, Run: 01, Epoch: 340, Loss: 2033746.1250, Train: 100.00%, Valid: 72.80% Test: 74.30%
Split: 01, Run: 01, Epoch: 360, Loss: 2032842.1250, Train: 100.00%, Valid: 72.80% Test: 74.90%
Split: 01, Run: 01, Epoch: 380, Loss: 2032377.3750, Train: 100.00%, Valid: 73.40% Test: 75.00%
Split: 01, Run: 01, Epoch: 400, Loss: 2033306.1250, Train: 100.00%, Valid: 73.40% Test: 75.30%
Split: 01, Run: 01, Epoch: 420, Loss: 2033364.6250, Train: 100.00%, Valid: 73.40% Test: 74.30%
Split: 01, Run: 01, Epoch: 440, Loss: 2032330.1250, Train: 100.00%, Valid: 73.20% Test: 74.60%
Split: 01, Run: 01, Epoch: 460, Loss: 2033932.5000, Train: 100.00%, Valid: 73.80% Test: 74.90%
Split: 01, Run: 01, Epoch: 480, Loss: 2033105.6250, Train: 100.00%, Valid: 73.00% Test: 74.50%
Split: 01, Run: 01, Epoch: 500, Loss: 2033362.8750, Train: 100.00%, Valid: 73.60% Test: 74.60%
Split: 01, Run: 01
None time:  5.573879700154066
None Run 01:
Highest Train: 100.00
Highest Valid: 74.20
  Final Train: 100.00
   Final Test: 74.80
total time:  7.295485255308449
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 74.20 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.80 Â± nan
[32m[I 2021-07-19 19:55:58,114][0m Trial 0 finished with value: 74.19999694824219 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 74.19999694824219.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  74.2    {'train': '100.00 Â± nan', 'valid': '74.20 Â± nan', 'test': '74.80 Â± nan'}
test_acc
['74.80 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  74.19999694824219
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '74.20 Â± nan', 'test': '74.80 Â± nan'}
optuna total time:  7.308835355564952
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 19:56:12,178][0m A new study created in memory with name: no-name-92d7fb77-b005-4070-ac01-765074340e15[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 19:56:14,192][0m Trial 0 failed because of the following error: TypeError("unsupported operand type(s) for +: 'NoneType' and 'float'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 109, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 320, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 109, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 19:57:07,476][0m A new study created in memory with name: no-name-47d924e6-c99d-4ac3-9c67-59dff6055122[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-19 19:57:09,456][0m Trial 0 failed because of the following error: TypeError("unsupported operand type(s) for +: 'NoneType' and 'float'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 109, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 321, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 79, in propagate_update
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 109, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 19:58:38,597][0m A new study created in memory with name: no-name-982ad3d6-5c09-469c-90d5-fdda1acc2a91[0m
lambda1:  1.0
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 266477.6250, Train: 100.00%, Valid: 68.60% Test: 69.80%
Split: 01, Run: 01, Epoch: 40, Loss: 261030.7188, Train: 100.00%, Valid: 61.60% Test: 63.10%
Split: 01, Run: 01, Epoch: 60, Loss: 258932.0312, Train: 100.00%, Valid: 59.60% Test: 62.10%
Split: 01, Run: 01, Epoch: 80, Loss: 258864.2969, Train: 100.00%, Valid: 59.80% Test: 62.30%
Split: 01, Run: 01, Epoch: 100, Loss: 259299.2656, Train: 100.00%, Valid: 61.40% Test: 63.30%
Split: 01, Run: 01, Epoch: 120, Loss: 259034.8750, Train: 100.00%, Valid: 63.00% Test: 63.60%
Split: 01, Run: 01, Epoch: 140, Loss: 259247.0781, Train: 100.00%, Valid: 63.80% Test: 64.80%
Split: 01, Run: 01, Epoch: 160, Loss: 259123.8125, Train: 100.00%, Valid: 62.20% Test: 64.20%
Split: 01, Run: 01, Epoch: 180, Loss: 259223.3125, Train: 100.00%, Valid: 62.60% Test: 63.90%
Split: 01, Run: 01, Epoch: 200, Loss: 258855.2500, Train: 100.00%, Valid: 64.00% Test: 62.60%
Split: 01, Run: 01, Epoch: 220, Loss: 259542.7969, Train: 100.00%, Valid: 61.00% Test: 62.50%
Split: 01, Run: 01, Epoch: 240, Loss: 259665.7344, Train: 100.00%, Valid: 62.40% Test: 64.40%
Split: 01, Run: 01, Epoch: 260, Loss: 259028.2500, Train: 100.00%, Valid: 60.40% Test: 62.30%
Split: 01, Run: 01, Epoch: 280, Loss: 258982.6250, Train: 100.00%, Valid: 61.80% Test: 63.80%
Split: 01, Run: 01, Epoch: 300, Loss: 259312.0938, Train: 100.00%, Valid: 62.00% Test: 65.00%
Split: 01, Run: 01, Epoch: 320, Loss: 259358.2812, Train: 100.00%, Valid: 62.80% Test: 63.40%
Split: 01, Run: 01, Epoch: 340, Loss: 259250.2812, Train: 100.00%, Valid: 62.40% Test: 62.80%
Split: 01, Run: 01, Epoch: 360, Loss: 259146.8594, Train: 100.00%, Valid: 61.80% Test: 63.50%
Split: 01, Run: 01, Epoch: 380, Loss: 258830.9375, Train: 100.00%, Valid: 62.20% Test: 63.70%
Split: 01, Run: 01, Epoch: 400, Loss: 258918.8594, Train: 100.00%, Valid: 63.20% Test: 66.50%
Split: 01, Run: 01, Epoch: 420, Loss: 259266.8125, Train: 100.00%, Valid: 61.20% Test: 63.40%
Split: 01, Run: 01, Epoch: 440, Loss: 258748.8438, Train: 100.00%, Valid: 60.80% Test: 64.00%
Split: 01, Run: 01, Epoch: 460, Loss: 258792.3125, Train: 100.00%, Valid: 63.40% Test: 63.60%
Split: 01, Run: 01, Epoch: 480, Loss: 258982.9062, Train: 100.00%, Valid: 60.80% Test: 62.60%
Split: 01, Run: 01, Epoch: 500, Loss: 259007.5938, Train: 100.00%, Valid: 58.00% Test: 62.60%
Split: 01, Run: 01
None time:  5.816113128326833
None Run 01:
Highest Train: 100.00
Highest Valid: 69.20
  Final Train: 100.00
   Final Test: 70.60
total time:  7.5633306028321385
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 69.20 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 70.60 Â± nan
[32m[I 2021-07-19 19:58:46,170][0m Trial 0 finished with value: 69.19999694824219 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 69.19999694824219.[0m
lambda1:  0.5
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 538353.1250, Train: 100.00%, Valid: 72.00% Test: 72.50%
Split: 01, Run: 01, Epoch: 40, Loss: 529642.8125, Train: 100.00%, Valid: 67.60% Test: 71.30%
Split: 01, Run: 01, Epoch: 60, Loss: 525766.8125, Train: 100.00%, Valid: 67.00% Test: 69.30%
Split: 01, Run: 01, Epoch: 80, Loss: 524952.0625, Train: 100.00%, Valid: 66.20% Test: 68.80%
Split: 01, Run: 01, Epoch: 100, Loss: 525549.1875, Train: 100.00%, Valid: 68.20% Test: 69.20%
Split: 01, Run: 01, Epoch: 120, Loss: 525067.8750, Train: 100.00%, Valid: 67.80% Test: 68.50%
Split: 01, Run: 01, Epoch: 140, Loss: 525432.2500, Train: 100.00%, Valid: 66.40% Test: 70.00%
Split: 01, Run: 01, Epoch: 160, Loss: 525051.3750, Train: 100.00%, Valid: 67.80% Test: 70.30%
Split: 01, Run: 01, Epoch: 180, Loss: 525348.6250, Train: 100.00%, Valid: 66.60% Test: 68.30%
Split: 01, Run: 01, Epoch: 200, Loss: 524701.8125, Train: 100.00%, Valid: 67.20% Test: 68.80%
Split: 01, Run: 01, Epoch: 220, Loss: 525750.0625, Train: 100.00%, Valid: 67.40% Test: 68.70%
Split: 01, Run: 01, Epoch: 240, Loss: 525666.0000, Train: 100.00%, Valid: 67.40% Test: 68.20%
Split: 01, Run: 01, Epoch: 260, Loss: 525006.1875, Train: 100.00%, Valid: 66.60% Test: 69.20%
Split: 01, Run: 01, Epoch: 280, Loss: 524814.1250, Train: 100.00%, Valid: 68.40% Test: 69.20%
Split: 01, Run: 01, Epoch: 300, Loss: 525357.4375, Train: 100.00%, Valid: 67.40% Test: 69.10%
Split: 01, Run: 01, Epoch: 320, Loss: 525513.7500, Train: 100.00%, Valid: 67.00% Test: 69.20%
Split: 01, Run: 01, Epoch: 340, Loss: 524868.5625, Train: 100.00%, Valid: 67.40% Test: 68.50%
Split: 01, Run: 01, Epoch: 360, Loss: 525031.2500, Train: 100.00%, Valid: 67.80% Test: 67.70%
Split: 01, Run: 01, Epoch: 380, Loss: 524380.8125, Train: 100.00%, Valid: 67.40% Test: 68.40%
Split: 01, Run: 01, Epoch: 400, Loss: 524566.0625, Train: 100.00%, Valid: 67.20% Test: 68.80%
Split: 01, Run: 01, Epoch: 420, Loss: 525080.0000, Train: 100.00%, Valid: 66.80% Test: 68.50%
Split: 01, Run: 01, Epoch: 440, Loss: 524214.6250, Train: 100.00%, Valid: 67.40% Test: 69.10%
Split: 01, Run: 01, Epoch: 460, Loss: 524650.4375, Train: 100.00%, Valid: 68.20% Test: 68.10%
Split: 01, Run: 01, Epoch: 480, Loss: 524651.3125, Train: 100.00%, Valid: 65.20% Test: 68.40%
Split: 01, Run: 01, Epoch: 500, Loss: 524639.9375, Train: 100.00%, Valid: 65.80% Test: 68.20%
Split: 01, Run: 01
None time:  5.4304159339517355
None Run 01:
Highest Train: 100.00
Highest Valid: 72.40
  Final Train: 100.00
   Final Test: 73.10
total time:  5.504348052665591
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 72.40 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 73.10 Â± nan
[32m[I 2021-07-19 19:58:51,680][0m Trial 1 finished with value: 72.39999389648438 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}. Best is trial 1 with value: 72.39999389648438.[0m
lambda1:  0.1
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 2011046.3750, Train: 100.00%, Valid: 72.00% Test: 72.90%
Split: 01, Run: 01, Epoch: 40, Loss: 2043888.3750, Train: 100.00%, Valid: 73.40% Test: 75.30%
Split: 01, Run: 01, Epoch: 60, Loss: 2041180.3750, Train: 100.00%, Valid: 73.00% Test: 75.60%
Split: 01, Run: 01, Epoch: 80, Loss: 2036823.3750, Train: 100.00%, Valid: 73.20% Test: 74.80%
Split: 01, Run: 01, Epoch: 100, Loss: 2036151.6250, Train: 100.00%, Valid: 73.20% Test: 74.40%
Split: 01, Run: 01, Epoch: 120, Loss: 2034618.1250, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 140, Loss: 2035413.8750, Train: 100.00%, Valid: 73.20% Test: 75.00%
Split: 01, Run: 01, Epoch: 160, Loss: 2035377.3750, Train: 100.00%, Valid: 73.20% Test: 74.90%
Split: 01, Run: 01, Epoch: 180, Loss: 2035718.6250, Train: 100.00%, Valid: 72.00% Test: 74.80%
Split: 01, Run: 01, Epoch: 200, Loss: 2034227.3750, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 220, Loss: 2036430.7500, Train: 100.00%, Valid: 73.00% Test: 75.20%
Split: 01, Run: 01, Epoch: 240, Loss: 2035752.1250, Train: 100.00%, Valid: 73.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 260, Loss: 2035163.7500, Train: 100.00%, Valid: 73.20% Test: 74.30%
Split: 01, Run: 01, Epoch: 280, Loss: 2034475.2500, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 01, Epoch: 300, Loss: 2035392.6250, Train: 100.00%, Valid: 73.00% Test: 74.50%
Split: 01, Run: 01, Epoch: 320, Loss: 2035544.1250, Train: 100.00%, Valid: 73.60% Test: 74.30%
Split: 01, Run: 01, Epoch: 340, Loss: 2033746.1250, Train: 100.00%, Valid: 72.80% Test: 74.30%
Split: 01, Run: 01, Epoch: 360, Loss: 2032842.1250, Train: 100.00%, Valid: 72.80% Test: 74.90%
Split: 01, Run: 01, Epoch: 380, Loss: 2032377.3750, Train: 100.00%, Valid: 73.40% Test: 75.00%
Split: 01, Run: 01, Epoch: 400, Loss: 2033306.1250, Train: 100.00%, Valid: 73.40% Test: 75.30%
Split: 01, Run: 01, Epoch: 420, Loss: 2033364.6250, Train: 100.00%, Valid: 73.40% Test: 74.30%
Split: 01, Run: 01, Epoch: 440, Loss: 2032330.1250, Train: 100.00%, Valid: 73.20% Test: 74.60%
Split: 01, Run: 01, Epoch: 460, Loss: 2033932.5000, Train: 100.00%, Valid: 73.80% Test: 74.90%
Split: 01, Run: 01, Epoch: 480, Loss: 2033105.6250, Train: 100.00%, Valid: 73.00% Test: 74.50%
Split: 01, Run: 01, Epoch: 500, Loss: 2033362.8750, Train: 100.00%, Valid: 73.60% Test: 74.60%
Split: 01, Run: 01
None time:  5.479858415201306
None Run 01:
Highest Train: 100.00
Highest Valid: 74.20
  Final Train: 100.00
   Final Test: 74.80
total time:  5.548756597563624
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 74.20 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.80 Â± nan
[32m[I 2021-07-19 19:58:57,233][0m Trial 2 finished with value: 74.19999694824219 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}. Best is trial 2 with value: 74.19999694824219.[0m
lambda1:  0.2
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1235087.8750, Train: 100.00%, Valid: 72.60% Test: 73.30%
Split: 01, Run: 01, Epoch: 40, Loss: 1225365.7500, Train: 100.00%, Valid: 72.00% Test: 74.50%
Split: 01, Run: 01, Epoch: 60, Loss: 1219436.7500, Train: 100.00%, Valid: 71.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 80, Loss: 1216321.7500, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 01, Epoch: 100, Loss: 1216326.2500, Train: 100.00%, Valid: 72.40% Test: 73.60%
Split: 01, Run: 01, Epoch: 120, Loss: 1215404.7500, Train: 100.00%, Valid: 71.80% Test: 74.00%
Split: 01, Run: 01, Epoch: 140, Loss: 1216267.8750, Train: 100.00%, Valid: 71.60% Test: 74.00%
Split: 01, Run: 01, Epoch: 160, Loss: 1215728.8750, Train: 100.00%, Valid: 72.20% Test: 73.70%
Split: 01, Run: 01, Epoch: 180, Loss: 1216173.3750, Train: 100.00%, Valid: 71.00% Test: 72.70%
Split: 01, Run: 01, Epoch: 200, Loss: 1215208.2500, Train: 100.00%, Valid: 71.60% Test: 73.60%
Split: 01, Run: 01, Epoch: 220, Loss: 1216669.0000, Train: 100.00%, Valid: 71.80% Test: 73.20%
Split: 01, Run: 01, Epoch: 240, Loss: 1216404.1250, Train: 100.00%, Valid: 72.20% Test: 73.40%
Split: 01, Run: 01, Epoch: 260, Loss: 1215970.6250, Train: 100.00%, Valid: 72.20% Test: 73.20%
Split: 01, Run: 01, Epoch: 280, Loss: 1215496.8750, Train: 100.00%, Valid: 71.60% Test: 73.60%
Split: 01, Run: 01, Epoch: 300, Loss: 1215820.1250, Train: 100.00%, Valid: 71.40% Test: 73.70%
Split: 01, Run: 01, Epoch: 320, Loss: 1216088.2500, Train: 100.00%, Valid: 71.40% Test: 73.30%
Split: 01, Run: 01, Epoch: 340, Loss: 1214990.3750, Train: 100.00%, Valid: 71.80% Test: 72.90%
Split: 01, Run: 01, Epoch: 360, Loss: 1214744.3750, Train: 100.00%, Valid: 71.00% Test: 72.90%
Split: 01, Run: 01, Epoch: 380, Loss: 1214226.7500, Train: 100.00%, Valid: 72.00% Test: 73.30%
Split: 01, Run: 01, Epoch: 400, Loss: 1214573.7500, Train: 100.00%, Valid: 71.60% Test: 73.60%
Split: 01, Run: 01, Epoch: 420, Loss: 1215003.7500, Train: 100.00%, Valid: 71.40% Test: 73.40%
Split: 01, Run: 01, Epoch: 440, Loss: 1214342.1250, Train: 100.00%, Valid: 72.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 460, Loss: 1215064.0000, Train: 100.00%, Valid: 72.20% Test: 73.50%
Split: 01, Run: 01, Epoch: 480, Loss: 1214713.7500, Train: 100.00%, Valid: 71.00% Test: 72.60%
Split: 01, Run: 01, Epoch: 500, Loss: 1214612.7500, Train: 100.00%, Valid: 72.60% Test: 73.50%
Split: 01, Run: 01
None time:  5.433502895757556
None Run 01:
Highest Train: 100.00
Highest Valid: 73.80
  Final Train: 100.00
   Final Test: 74.40
total time:  5.493770784698427
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 73.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.40 Â± nan
[32m[I 2021-07-19 19:59:02,731][0m Trial 3 finished with value: 73.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}. Best is trial 2 with value: 74.19999694824219.[0m
Study statistics: 
  Number of finished trials:  4
  Number of pruned trials:  0
  Number of complete trials:  4
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}   trial.value:  74.2    {'train': '100.00 Â± nan', 'valid': '74.20 Â± nan', 'test': '74.80 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}   trial.value:  73.8    {'train': '100.00 Â± nan', 'valid': '73.80 Â± nan', 'test': '74.40 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}   trial.value:  72.4    {'train': '100.00 Â± nan', 'valid': '72.40 Â± nan', 'test': '73.10 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}   trial.value:  69.2    {'train': '100.00 Â± nan', 'valid': '69.20 Â± nan', 'test': '70.60 Â± nan'}
test_acc
['74.80 Â± nan', '74.40 Â± nan', '73.10 Â± nan', '70.60 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}
Best trial Value:  74.19999694824219
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '74.20 Â± nan', 'test': '74.80 Â± nan'}
optuna total time:  24.140307306312025
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 20:00:25,691][0m A new study created in memory with name: no-name-1d9bda02-5db3-4121-8a00-4121e0766e48[0m
lambda1:  0.1
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1647114.3750, Train: 100.00%, Valid: 68.80% Test: 69.80%
Split: 01, Run: 01, Epoch: 40, Loss: 1604583.2500, Train: 100.00%, Valid: 68.80% Test: 70.90%
Split: 01, Run: 01, Epoch: 60, Loss: 1561952.0000, Train: 100.00%, Valid: 68.80% Test: 70.70%
Split: 01, Run: 01, Epoch: 80, Loss: 1497780.6250, Train: 100.00%, Valid: 72.60% Test: 75.20%
Split: 01, Run: 01, Epoch: 100, Loss: 1447373.7500, Train: 100.00%, Valid: 74.00% Test: 76.40%
Split: 01, Run: 01, Epoch: 120, Loss: 1400342.0000, Train: 100.00%, Valid: 74.20% Test: 76.90%
Split: 01, Run: 01, Epoch: 140, Loss: 1367111.2500, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 160, Loss: 1347534.3750, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 180, Loss: 1353461.5000, Train: 100.00%, Valid: 75.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 200, Loss: 1309885.2500, Train: 100.00%, Valid: 75.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 220, Loss: 1326457.5000, Train: 100.00%, Valid: 75.00% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 1285785.7500, Train: 100.00%, Valid: 75.60% Test: 77.40%
Split: 01, Run: 01, Epoch: 260, Loss: 1301501.3750, Train: 100.00%, Valid: 75.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 280, Loss: 1252103.6250, Train: 100.00%, Valid: 75.40% Test: 78.40%
Split: 01, Run: 01, Epoch: 300, Loss: 1253702.5000, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 01, Epoch: 320, Loss: 1263699.8750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 340, Loss: 1202083.8750, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 360, Loss: 1184826.1250, Train: 100.00%, Valid: 74.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 380, Loss: 1192654.8750, Train: 100.00%, Valid: 75.80% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 1184229.8750, Train: 100.00%, Valid: 75.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 420, Loss: 1170162.5000, Train: 100.00%, Valid: 74.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 440, Loss: 1158252.0000, Train: 100.00%, Valid: 76.20% Test: 77.50%
Split: 01, Run: 01, Epoch: 460, Loss: 1178744.6250, Train: 100.00%, Valid: 75.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 480, Loss: 1150219.6250, Train: 100.00%, Valid: 75.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 500, Loss: 1152485.8750, Train: 100.00%, Valid: 76.20% Test: 77.30%
Split: 01, Run: 01
None time:  3.5227896720170975
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 77.10
total time:  5.223859516903758
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.10 Â± nan
[32m[I 2021-07-19 20:00:30,923][0m Trial 0 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 76.80000305175781.[0m
lambda1:  1.0
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 157002.3594, Train: 100.00%, Valid: 41.60% Test: 46.20%
Split: 01, Run: 01, Epoch: 40, Loss: 149826.6562, Train: 100.00%, Valid: 52.20% Test: 55.30%
Split: 01, Run: 01, Epoch: 60, Loss: 142674.9062, Train: 100.00%, Valid: 60.00% Test: 64.00%
Split: 01, Run: 01, Epoch: 80, Loss: 136788.7188, Train: 100.00%, Valid: 65.20% Test: 66.60%
Split: 01, Run: 01, Epoch: 100, Loss: 132524.8438, Train: 100.00%, Valid: 67.60% Test: 69.30%
Split: 01, Run: 01, Epoch: 120, Loss: 125342.6172, Train: 100.00%, Valid: 69.80% Test: 69.60%
Split: 01, Run: 01, Epoch: 140, Loss: 119020.4531, Train: 100.00%, Valid: 68.60% Test: 68.80%
Split: 01, Run: 01, Epoch: 160, Loss: 114199.6719, Train: 100.00%, Valid: 67.40% Test: 68.30%
Split: 01, Run: 01, Epoch: 180, Loss: 110941.9141, Train: 100.00%, Valid: 66.80% Test: 67.60%
Split: 01, Run: 01, Epoch: 200, Loss: 105988.0391, Train: 100.00%, Valid: 69.60% Test: 67.70%
Split: 01, Run: 01, Epoch: 220, Loss: 106644.7188, Train: 100.00%, Valid: 67.20% Test: 67.80%
Split: 01, Run: 01, Epoch: 240, Loss: 98928.4688, Train: 100.00%, Valid: 61.80% Test: 66.50%
Split: 01, Run: 01, Epoch: 260, Loss: 98872.5156, Train: 100.00%, Valid: 62.60% Test: 67.10%
Split: 01, Run: 01, Epoch: 280, Loss: 94712.2969, Train: 100.00%, Valid: 63.60% Test: 65.80%
Split: 01, Run: 01, Epoch: 300, Loss: 93855.5234, Train: 100.00%, Valid: 59.80% Test: 64.10%
Split: 01, Run: 01, Epoch: 320, Loss: 92496.8594, Train: 100.00%, Valid: 61.40% Test: 62.40%
Split: 01, Run: 01, Epoch: 340, Loss: 84583.0078, Train: 100.00%, Valid: 62.80% Test: 63.20%
Split: 01, Run: 01, Epoch: 360, Loss: 85401.6094, Train: 100.00%, Valid: 58.40% Test: 61.30%
Split: 01, Run: 01, Epoch: 380, Loss: 83854.2188, Train: 100.00%, Valid: 57.80% Test: 59.10%
Split: 01, Run: 01, Epoch: 400, Loss: 84268.3750, Train: 100.00%, Valid: 57.80% Test: 59.40%
Split: 01, Run: 01, Epoch: 420, Loss: 82279.3750, Train: 100.00%, Valid: 58.60% Test: 60.90%
Split: 01, Run: 01, Epoch: 440, Loss: 78393.7812, Train: 100.00%, Valid: 54.80% Test: 57.90%
Split: 01, Run: 01, Epoch: 460, Loss: 83118.9922, Train: 100.00%, Valid: 57.00% Test: 56.30%
Split: 01, Run: 01, Epoch: 480, Loss: 76629.8203, Train: 100.00%, Valid: 51.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 500, Loss: 78992.1172, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01
None time:  3.274950080551207
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 69.80
total time:  3.349091346375644
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 69.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 69.80 Â± nan
[32m[I 2021-07-19 20:00:34,277][0m Trial 1 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 76.80000305175781.[0m
lambda1:  0.2
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 989221.4375, Train: 100.00%, Valid: 67.20% Test: 67.30%
Split: 01, Run: 01, Epoch: 40, Loss: 952423.2500, Train: 100.00%, Valid: 66.20% Test: 67.20%
Split: 01, Run: 01, Epoch: 60, Loss: 925208.4375, Train: 100.00%, Valid: 68.60% Test: 68.60%
Split: 01, Run: 01, Epoch: 80, Loss: 887757.8750, Train: 100.00%, Valid: 72.20% Test: 75.00%
Split: 01, Run: 01, Epoch: 100, Loss: 860481.6875, Train: 100.00%, Valid: 74.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 120, Loss: 833047.3750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 140, Loss: 812903.8750, Train: 100.00%, Valid: 74.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 160, Loss: 800717.3750, Train: 100.00%, Valid: 76.80% Test: 78.50%
Split: 01, Run: 01, Epoch: 180, Loss: 797658.0000, Train: 100.00%, Valid: 75.60% Test: 76.90%
Split: 01, Run: 01, Epoch: 200, Loss: 769796.9375, Train: 100.00%, Valid: 75.60% Test: 78.00%
Split: 01, Run: 01, Epoch: 220, Loss: 776847.6875, Train: 100.00%, Valid: 74.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 240, Loss: 754092.1875, Train: 100.00%, Valid: 77.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 260, Loss: 764205.6250, Train: 100.00%, Valid: 75.80% Test: 78.30%
Split: 01, Run: 01, Epoch: 280, Loss: 736744.7500, Train: 100.00%, Valid: 75.20% Test: 78.60%
Split: 01, Run: 01, Epoch: 300, Loss: 733630.8125, Train: 100.00%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 01, Epoch: 320, Loss: 740496.3750, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 01, Epoch: 340, Loss: 694524.5000, Train: 100.00%, Valid: 75.60% Test: 78.20%
Split: 01, Run: 01, Epoch: 360, Loss: 684537.5000, Train: 100.00%, Valid: 73.80% Test: 76.90%
Split: 01, Run: 01, Epoch: 380, Loss: 683988.4375, Train: 100.00%, Valid: 75.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 680102.0000, Train: 100.00%, Valid: 76.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 420, Loss: 670335.6875, Train: 100.00%, Valid: 74.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 440, Loss: 660795.3125, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 460, Loss: 681954.6875, Train: 100.00%, Valid: 75.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 480, Loss: 647462.3750, Train: 100.00%, Valid: 75.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 500, Loss: 659670.2500, Train: 100.00%, Valid: 76.20% Test: 77.60%
Split: 01, Run: 01
None time:  3.2823766693472862
None Run 01:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 77.60
total time:  3.339962058700621
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 77.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.60 Â± nan
[32m[I 2021-07-19 20:00:37,621][0m Trial 2 finished with value: 77.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}. Best is trial 2 with value: 77.80000305175781.[0m
lambda1:  0.5
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 391033.8125, Train: 100.00%, Valid: 56.00% Test: 58.30%
Split: 01, Run: 01, Epoch: 40, Loss: 374644.7812, Train: 100.00%, Valid: 58.60% Test: 60.80%
Split: 01, Run: 01, Epoch: 60, Loss: 360502.7500, Train: 100.00%, Valid: 68.20% Test: 69.50%
Split: 01, Run: 01, Epoch: 80, Loss: 345255.8125, Train: 100.00%, Valid: 68.80% Test: 72.80%
Split: 01, Run: 01, Epoch: 100, Loss: 336563.2812, Train: 100.00%, Valid: 71.20% Test: 73.80%
Split: 01, Run: 01, Epoch: 120, Loss: 321853.0312, Train: 100.00%, Valid: 74.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 140, Loss: 310838.7812, Train: 100.00%, Valid: 73.40% Test: 75.70%
Split: 01, Run: 01, Epoch: 160, Loss: 304054.5625, Train: 100.00%, Valid: 75.40% Test: 73.80%
Split: 01, Run: 01, Epoch: 180, Loss: 300046.6562, Train: 100.00%, Valid: 72.40% Test: 72.90%
Split: 01, Run: 01, Epoch: 200, Loss: 288485.5625, Train: 100.00%, Valid: 72.80% Test: 75.90%
Split: 01, Run: 01, Epoch: 220, Loss: 287600.2500, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 01, Epoch: 240, Loss: 273046.3125, Train: 100.00%, Valid: 71.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 260, Loss: 277843.9062, Train: 100.00%, Valid: 69.60% Test: 74.40%
Split: 01, Run: 01, Epoch: 280, Loss: 266921.5625, Train: 100.00%, Valid: 72.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 300, Loss: 264175.4688, Train: 100.00%, Valid: 70.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 320, Loss: 265979.0000, Train: 100.00%, Valid: 71.40% Test: 72.70%
Split: 01, Run: 01, Epoch: 340, Loss: 243437.4688, Train: 100.00%, Valid: 72.00% Test: 74.30%
Split: 01, Run: 01, Epoch: 360, Loss: 246851.9844, Train: 100.00%, Valid: 71.40% Test: 72.10%
Split: 01, Run: 01, Epoch: 380, Loss: 239862.0312, Train: 100.00%, Valid: 70.00% Test: 70.50%
Split: 01, Run: 01, Epoch: 400, Loss: 237070.4219, Train: 100.00%, Valid: 70.20% Test: 72.90%
Split: 01, Run: 01, Epoch: 420, Loss: 234759.0000, Train: 100.00%, Valid: 68.60% Test: 71.70%
Split: 01, Run: 01, Epoch: 440, Loss: 226197.3906, Train: 100.00%, Valid: 68.60% Test: 71.80%
Split: 01, Run: 01, Epoch: 460, Loss: 235076.4531, Train: 100.00%, Valid: 70.00% Test: 71.30%
Split: 01, Run: 01, Epoch: 480, Loss: 221644.9219, Train: 100.00%, Valid: 69.40% Test: 70.00%
Split: 01, Run: 01, Epoch: 500, Loss: 224447.5625, Train: 100.00%, Valid: 69.60% Test: 72.70%
Split: 01, Run: 01
None time:  3.2934709461405873
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 74.60
total time:  3.35843363404274
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.60 Â± nan
[32m[I 2021-07-19 20:00:40,984][0m Trial 3 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}. Best is trial 2 with value: 77.80000305175781.[0m
Study statistics: 
  Number of finished trials:  4
  Number of pruned trials:  0
  Number of complete trials:  4
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '77.10 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}   trial.value:  77.8    {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '74.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}   trial.value:  69.8    {'train': '100.00 Â± nan', 'valid': '69.80 Â± nan', 'test': '69.80 Â± nan'}
test_acc
['77.10 Â± nan', '77.60 Â± nan', '74.60 Â± nan', '69.80 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}
Best trial Value:  77.80000305175781
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
optuna total time:  15.299370156601071
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 20:01:18,628][0m A new study created in memory with name: no-name-e6c7cbb1-6e3f-4bb6-aa27-e5b669f293e0[0m
lambda1:  0.2
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 989221.4375, Train: 100.00%, Valid: 67.20% Test: 67.30%
Split: 01, Run: 01, Epoch: 40, Loss: 952423.2500, Train: 100.00%, Valid: 66.20% Test: 67.20%
Split: 01, Run: 01, Epoch: 60, Loss: 925208.4375, Train: 100.00%, Valid: 68.60% Test: 68.60%
Split: 01, Run: 01, Epoch: 80, Loss: 887757.8750, Train: 100.00%, Valid: 72.20% Test: 75.00%
Split: 01, Run: 01, Epoch: 100, Loss: 860481.6875, Train: 100.00%, Valid: 74.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 120, Loss: 833047.3750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 140, Loss: 812903.8750, Train: 100.00%, Valid: 74.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 160, Loss: 800717.3750, Train: 100.00%, Valid: 76.80% Test: 78.50%
Split: 01, Run: 01, Epoch: 180, Loss: 797658.0000, Train: 100.00%, Valid: 75.60% Test: 76.90%
Split: 01, Run: 01, Epoch: 200, Loss: 769796.9375, Train: 100.00%, Valid: 75.60% Test: 78.00%
Split: 01, Run: 01, Epoch: 220, Loss: 776847.6875, Train: 100.00%, Valid: 74.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 240, Loss: 754092.1875, Train: 100.00%, Valid: 77.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 260, Loss: 764205.6250, Train: 100.00%, Valid: 75.80% Test: 78.30%
Split: 01, Run: 01, Epoch: 280, Loss: 736744.7500, Train: 100.00%, Valid: 75.20% Test: 78.60%
Split: 01, Run: 01, Epoch: 300, Loss: 733630.8125, Train: 100.00%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 01, Epoch: 320, Loss: 740496.3750, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 01, Epoch: 340, Loss: 694524.5000, Train: 100.00%, Valid: 75.60% Test: 78.20%
Split: 01, Run: 01, Epoch: 360, Loss: 684537.5000, Train: 100.00%, Valid: 73.80% Test: 76.90%
Split: 01, Run: 01, Epoch: 380, Loss: 683988.4375, Train: 100.00%, Valid: 75.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 680102.0000, Train: 100.00%, Valid: 76.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 420, Loss: 670335.6875, Train: 100.00%, Valid: 74.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 440, Loss: 660795.3125, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 460, Loss: 681954.6875, Train: 100.00%, Valid: 75.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 480, Loss: 647462.3750, Train: 100.00%, Valid: 75.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 500, Loss: 659670.2500, Train: 100.00%, Valid: 76.20% Test: 77.60%
Split: 01, Run: 01
None time:  3.509074456989765
None Run 01:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 77.60
total time:  5.293061337433755
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 77.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.60 Â± nan
[32m[I 2021-07-19 20:01:23,929][0m Trial 0 finished with value: 77.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 77.80000305175781.[0m
lambda1:  1.0
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 157002.3594, Train: 100.00%, Valid: 41.60% Test: 46.20%
Split: 01, Run: 01, Epoch: 40, Loss: 149826.6562, Train: 100.00%, Valid: 52.20% Test: 55.30%
Split: 01, Run: 01, Epoch: 60, Loss: 142674.9062, Train: 100.00%, Valid: 60.00% Test: 64.00%
Split: 01, Run: 01, Epoch: 80, Loss: 136788.7188, Train: 100.00%, Valid: 65.20% Test: 66.60%
Split: 01, Run: 01, Epoch: 100, Loss: 132524.8438, Train: 100.00%, Valid: 67.60% Test: 69.30%
Split: 01, Run: 01, Epoch: 120, Loss: 125342.6172, Train: 100.00%, Valid: 69.80% Test: 69.60%
Split: 01, Run: 01, Epoch: 140, Loss: 119020.4531, Train: 100.00%, Valid: 68.60% Test: 68.80%
Split: 01, Run: 01, Epoch: 160, Loss: 114199.6719, Train: 100.00%, Valid: 67.40% Test: 68.30%
Split: 01, Run: 01, Epoch: 180, Loss: 110941.9141, Train: 100.00%, Valid: 66.80% Test: 67.60%
Split: 01, Run: 01, Epoch: 200, Loss: 105988.0391, Train: 100.00%, Valid: 69.60% Test: 67.70%
Split: 01, Run: 01, Epoch: 220, Loss: 106644.7188, Train: 100.00%, Valid: 67.20% Test: 67.80%
Split: 01, Run: 01, Epoch: 240, Loss: 98928.4688, Train: 100.00%, Valid: 61.80% Test: 66.50%
Split: 01, Run: 01, Epoch: 260, Loss: 98872.5156, Train: 100.00%, Valid: 62.60% Test: 67.10%
Split: 01, Run: 01, Epoch: 280, Loss: 94712.2969, Train: 100.00%, Valid: 63.60% Test: 65.80%
Split: 01, Run: 01, Epoch: 300, Loss: 93855.5234, Train: 100.00%, Valid: 59.80% Test: 64.10%
Split: 01, Run: 01, Epoch: 320, Loss: 92496.8594, Train: 100.00%, Valid: 61.40% Test: 62.40%
Split: 01, Run: 01, Epoch: 340, Loss: 84583.0078, Train: 100.00%, Valid: 62.80% Test: 63.20%
Split: 01, Run: 01, Epoch: 360, Loss: 85401.6094, Train: 100.00%, Valid: 58.40% Test: 61.30%
Split: 01, Run: 01, Epoch: 380, Loss: 83854.2188, Train: 100.00%, Valid: 57.80% Test: 59.10%
Split: 01, Run: 01, Epoch: 400, Loss: 84268.3750, Train: 100.00%, Valid: 57.80% Test: 59.40%
Split: 01, Run: 01, Epoch: 420, Loss: 82279.3750, Train: 100.00%, Valid: 58.60% Test: 60.90%
Split: 01, Run: 01, Epoch: 440, Loss: 78393.7812, Train: 100.00%, Valid: 54.80% Test: 57.90%
Split: 01, Run: 01, Epoch: 460, Loss: 83118.9922, Train: 100.00%, Valid: 57.00% Test: 56.30%
Split: 01, Run: 01, Epoch: 480, Loss: 76629.8203, Train: 100.00%, Valid: 51.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 500, Loss: 78992.1172, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01
None time:  3.24535839445889
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 69.80
total time:  3.3143261317163706
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 69.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 69.80 Â± nan
[32m[I 2021-07-19 20:01:27,247][0m Trial 1 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 77.80000305175781.[0m
lambda1:  0.1
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1647114.3750, Train: 100.00%, Valid: 68.80% Test: 69.80%
Split: 01, Run: 01, Epoch: 40, Loss: 1604583.2500, Train: 100.00%, Valid: 68.80% Test: 70.90%
Split: 01, Run: 01, Epoch: 60, Loss: 1561952.0000, Train: 100.00%, Valid: 68.80% Test: 70.70%
Split: 01, Run: 01, Epoch: 80, Loss: 1497780.6250, Train: 100.00%, Valid: 72.60% Test: 75.20%
Split: 01, Run: 01, Epoch: 100, Loss: 1447373.7500, Train: 100.00%, Valid: 74.00% Test: 76.40%
Split: 01, Run: 01, Epoch: 120, Loss: 1400342.0000, Train: 100.00%, Valid: 74.20% Test: 76.90%
Split: 01, Run: 01, Epoch: 140, Loss: 1367111.2500, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 160, Loss: 1347534.3750, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 180, Loss: 1353461.5000, Train: 100.00%, Valid: 75.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 200, Loss: 1309885.2500, Train: 100.00%, Valid: 75.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 220, Loss: 1326457.5000, Train: 100.00%, Valid: 75.00% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 1285785.7500, Train: 100.00%, Valid: 75.60% Test: 77.40%
Split: 01, Run: 01, Epoch: 260, Loss: 1301501.3750, Train: 100.00%, Valid: 75.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 280, Loss: 1252103.6250, Train: 100.00%, Valid: 75.40% Test: 78.40%
Split: 01, Run: 01, Epoch: 300, Loss: 1253702.5000, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 01, Epoch: 320, Loss: 1263699.8750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 340, Loss: 1202083.8750, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 360, Loss: 1184826.1250, Train: 100.00%, Valid: 74.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 380, Loss: 1192654.8750, Train: 100.00%, Valid: 75.80% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 1184229.8750, Train: 100.00%, Valid: 75.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 420, Loss: 1170162.5000, Train: 100.00%, Valid: 74.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 440, Loss: 1158252.0000, Train: 100.00%, Valid: 76.20% Test: 77.50%
Split: 01, Run: 01, Epoch: 460, Loss: 1178744.6250, Train: 100.00%, Valid: 75.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 480, Loss: 1150219.6250, Train: 100.00%, Valid: 75.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 500, Loss: 1152485.8750, Train: 100.00%, Valid: 76.20% Test: 77.30%
Split: 01, Run: 01
None time:  3.2082193261012435
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 77.10
total time:  3.266882781870663
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.10 Â± nan
[32m[I 2021-07-19 20:01:30,518][0m Trial 2 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 77.80000305175781.[0m
lambda1:  0.5
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 391033.8125, Train: 100.00%, Valid: 56.00% Test: 58.30%
Split: 01, Run: 01, Epoch: 40, Loss: 374644.7812, Train: 100.00%, Valid: 58.60% Test: 60.80%
Split: 01, Run: 01, Epoch: 60, Loss: 360502.7500, Train: 100.00%, Valid: 68.20% Test: 69.50%
Split: 01, Run: 01, Epoch: 80, Loss: 345255.8125, Train: 100.00%, Valid: 68.80% Test: 72.80%
Split: 01, Run: 01, Epoch: 100, Loss: 336563.2812, Train: 100.00%, Valid: 71.20% Test: 73.80%
Split: 01, Run: 01, Epoch: 120, Loss: 321853.0312, Train: 100.00%, Valid: 74.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 140, Loss: 310838.7812, Train: 100.00%, Valid: 73.40% Test: 75.70%
Split: 01, Run: 01, Epoch: 160, Loss: 304054.5625, Train: 100.00%, Valid: 75.40% Test: 73.80%
Split: 01, Run: 01, Epoch: 180, Loss: 300046.6562, Train: 100.00%, Valid: 72.40% Test: 72.90%
Split: 01, Run: 01, Epoch: 200, Loss: 288485.5625, Train: 100.00%, Valid: 72.80% Test: 75.90%
Split: 01, Run: 01, Epoch: 220, Loss: 287600.2500, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 01, Epoch: 240, Loss: 273046.3125, Train: 100.00%, Valid: 71.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 260, Loss: 277843.9062, Train: 100.00%, Valid: 69.60% Test: 74.40%
Split: 01, Run: 01, Epoch: 280, Loss: 266921.5625, Train: 100.00%, Valid: 72.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 300, Loss: 264175.4688, Train: 100.00%, Valid: 70.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 320, Loss: 265979.0000, Train: 100.00%, Valid: 71.40% Test: 72.70%
Split: 01, Run: 01, Epoch: 340, Loss: 243437.4688, Train: 100.00%, Valid: 72.00% Test: 74.30%
Split: 01, Run: 01, Epoch: 360, Loss: 246851.9844, Train: 100.00%, Valid: 71.40% Test: 72.10%
Split: 01, Run: 01, Epoch: 380, Loss: 239862.0312, Train: 100.00%, Valid: 70.00% Test: 70.50%
Split: 01, Run: 01, Epoch: 400, Loss: 237070.4219, Train: 100.00%, Valid: 70.20% Test: 72.90%
Split: 01, Run: 01, Epoch: 420, Loss: 234759.0000, Train: 100.00%, Valid: 68.60% Test: 71.70%
Split: 01, Run: 01, Epoch: 440, Loss: 226197.3906, Train: 100.00%, Valid: 68.60% Test: 71.80%
Split: 01, Run: 01, Epoch: 460, Loss: 235076.4531, Train: 100.00%, Valid: 70.00% Test: 71.30%
Split: 01, Run: 01, Epoch: 480, Loss: 221644.9219, Train: 100.00%, Valid: 69.40% Test: 70.00%
Split: 01, Run: 01, Epoch: 500, Loss: 224447.5625, Train: 100.00%, Valid: 69.60% Test: 72.70%
Split: 01, Run: 01
None time:  3.176914632320404
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 74.60
total time:  3.237077893689275
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.60 Â± nan
[32m[I 2021-07-19 20:01:33,759][0m Trial 3 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 77.80000305175781.[0m
Study statistics: 
  Number of finished trials:  4
  Number of pruned trials:  0
  Number of complete trials:  4
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '77.10 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}   trial.value:  77.8    {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '74.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}   trial.value:  69.8    {'train': '100.00 Â± nan', 'valid': '69.80 Â± nan', 'test': '69.80 Â± nan'}
test_acc
['77.10 Â± nan', '77.60 Â± nan', '74.60 Â± nan', '69.80 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}
Best trial Value:  77.80000305175781
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
optuna total time:  15.137206079438329
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.5, 1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  4
[32m[I 2021-07-19 20:37:53,735][0m A new study created in memory with name: no-name-1b8422be-aa83-4364-ab3d-599b436e945b[0m
lambda1:  1.0
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 157002.3594, Train: 100.00%, Valid: 41.60% Test: 46.20%
Split: 01, Run: 01, Epoch: 40, Loss: 149826.6562, Train: 100.00%, Valid: 52.20% Test: 55.30%
Split: 01, Run: 01, Epoch: 60, Loss: 142674.9062, Train: 100.00%, Valid: 60.00% Test: 64.00%
Split: 01, Run: 01, Epoch: 80, Loss: 136788.7188, Train: 100.00%, Valid: 65.20% Test: 66.60%
Split: 01, Run: 01, Epoch: 100, Loss: 132524.8438, Train: 100.00%, Valid: 67.60% Test: 69.30%
Split: 01, Run: 01, Epoch: 120, Loss: 125342.6172, Train: 100.00%, Valid: 69.80% Test: 69.60%
Split: 01, Run: 01, Epoch: 140, Loss: 119020.4531, Train: 100.00%, Valid: 68.60% Test: 68.80%
Split: 01, Run: 01, Epoch: 160, Loss: 114199.6719, Train: 100.00%, Valid: 67.40% Test: 68.30%
Split: 01, Run: 01, Epoch: 180, Loss: 110941.9141, Train: 100.00%, Valid: 66.80% Test: 67.60%
Split: 01, Run: 01, Epoch: 200, Loss: 105988.0391, Train: 100.00%, Valid: 69.60% Test: 67.70%
Split: 01, Run: 01, Epoch: 220, Loss: 106644.7188, Train: 100.00%, Valid: 67.20% Test: 67.80%
Split: 01, Run: 01, Epoch: 240, Loss: 98928.4688, Train: 100.00%, Valid: 61.80% Test: 66.50%
Split: 01, Run: 01, Epoch: 260, Loss: 98872.5156, Train: 100.00%, Valid: 62.60% Test: 67.10%
Split: 01, Run: 01, Epoch: 280, Loss: 94712.2969, Train: 100.00%, Valid: 63.60% Test: 65.80%
Split: 01, Run: 01, Epoch: 300, Loss: 93855.5234, Train: 100.00%, Valid: 59.80% Test: 64.10%
Split: 01, Run: 01, Epoch: 320, Loss: 92496.8594, Train: 100.00%, Valid: 61.40% Test: 62.40%
Split: 01, Run: 01, Epoch: 340, Loss: 84583.0078, Train: 100.00%, Valid: 62.80% Test: 63.20%
Split: 01, Run: 01, Epoch: 360, Loss: 85401.6094, Train: 100.00%, Valid: 58.40% Test: 61.30%
Split: 01, Run: 01, Epoch: 380, Loss: 83854.2188, Train: 100.00%, Valid: 57.80% Test: 59.10%
Split: 01, Run: 01, Epoch: 400, Loss: 84268.3750, Train: 100.00%, Valid: 57.80% Test: 59.40%
Split: 01, Run: 01, Epoch: 420, Loss: 82279.3750, Train: 100.00%, Valid: 58.60% Test: 60.90%
Split: 01, Run: 01, Epoch: 440, Loss: 78393.7812, Train: 100.00%, Valid: 54.80% Test: 57.90%
Split: 01, Run: 01, Epoch: 460, Loss: 83118.9922, Train: 100.00%, Valid: 57.00% Test: 56.30%
Split: 01, Run: 01, Epoch: 480, Loss: 76629.8203, Train: 100.00%, Valid: 51.20% Test: 57.10%
Split: 01, Run: 01, Epoch: 500, Loss: 78992.1172, Train: 100.00%, Valid: 56.20% Test: 57.10%
Split: 01, Run: 01
None time:  3.888809161260724
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 69.80
total time:  10.76219293475151
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 69.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 69.80 Â± nan
[32m[I 2021-07-19 20:38:04,549][0m Trial 0 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}. Best is trial 0 with value: 69.80000305175781.[0m
lambda1:  0.5
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 391033.8125, Train: 100.00%, Valid: 56.00% Test: 58.30%
Split: 01, Run: 01, Epoch: 40, Loss: 374644.7812, Train: 100.00%, Valid: 58.60% Test: 60.80%
Split: 01, Run: 01, Epoch: 60, Loss: 360502.7500, Train: 100.00%, Valid: 68.20% Test: 69.50%
Split: 01, Run: 01, Epoch: 80, Loss: 345255.8125, Train: 100.00%, Valid: 68.80% Test: 72.80%
Split: 01, Run: 01, Epoch: 100, Loss: 336563.2812, Train: 100.00%, Valid: 71.20% Test: 73.80%
Split: 01, Run: 01, Epoch: 120, Loss: 321853.0312, Train: 100.00%, Valid: 74.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 140, Loss: 310838.7812, Train: 100.00%, Valid: 73.40% Test: 75.70%
Split: 01, Run: 01, Epoch: 160, Loss: 304054.5625, Train: 100.00%, Valid: 75.40% Test: 73.80%
Split: 01, Run: 01, Epoch: 180, Loss: 300046.6562, Train: 100.00%, Valid: 72.40% Test: 72.90%
Split: 01, Run: 01, Epoch: 200, Loss: 288485.5625, Train: 100.00%, Valid: 72.80% Test: 75.90%
Split: 01, Run: 01, Epoch: 220, Loss: 287600.2500, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 01, Epoch: 240, Loss: 273046.3125, Train: 100.00%, Valid: 71.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 260, Loss: 277843.9062, Train: 100.00%, Valid: 69.60% Test: 74.40%
Split: 01, Run: 01, Epoch: 280, Loss: 266921.5625, Train: 100.00%, Valid: 72.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 300, Loss: 264175.4688, Train: 100.00%, Valid: 70.00% Test: 73.40%
Split: 01, Run: 01, Epoch: 320, Loss: 265979.0000, Train: 100.00%, Valid: 71.40% Test: 72.70%
Split: 01, Run: 01, Epoch: 340, Loss: 243437.4688, Train: 100.00%, Valid: 72.00% Test: 74.30%
Split: 01, Run: 01, Epoch: 360, Loss: 246851.9844, Train: 100.00%, Valid: 71.40% Test: 72.10%
Split: 01, Run: 01, Epoch: 380, Loss: 239862.0312, Train: 100.00%, Valid: 70.00% Test: 70.50%
Split: 01, Run: 01, Epoch: 400, Loss: 237070.4219, Train: 100.00%, Valid: 70.20% Test: 72.90%
Split: 01, Run: 01, Epoch: 420, Loss: 234759.0000, Train: 100.00%, Valid: 68.60% Test: 71.70%
Split: 01, Run: 01, Epoch: 440, Loss: 226197.3906, Train: 100.00%, Valid: 68.60% Test: 71.80%
Split: 01, Run: 01, Epoch: 460, Loss: 235076.4531, Train: 100.00%, Valid: 70.00% Test: 71.30%
Split: 01, Run: 01, Epoch: 480, Loss: 221644.9219, Train: 100.00%, Valid: 69.40% Test: 70.00%
Split: 01, Run: 01, Epoch: 500, Loss: 224447.5625, Train: 100.00%, Valid: 69.60% Test: 72.70%
Split: 01, Run: 01
None time:  3.308846483938396
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 74.60
total time:  3.382522244937718
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 74.60 Â± nan
[32m[I 2021-07-19 20:38:07,937][0m Trial 1 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}. Best is trial 1 with value: 76.80000305175781.[0m
lambda1:  0.1
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1647114.3750, Train: 100.00%, Valid: 68.80% Test: 69.80%
Split: 01, Run: 01, Epoch: 40, Loss: 1604583.2500, Train: 100.00%, Valid: 68.80% Test: 70.90%
Split: 01, Run: 01, Epoch: 60, Loss: 1561952.0000, Train: 100.00%, Valid: 68.80% Test: 70.70%
Split: 01, Run: 01, Epoch: 80, Loss: 1497780.6250, Train: 100.00%, Valid: 72.60% Test: 75.20%
Split: 01, Run: 01, Epoch: 100, Loss: 1447373.7500, Train: 100.00%, Valid: 74.00% Test: 76.40%
Split: 01, Run: 01, Epoch: 120, Loss: 1400342.0000, Train: 100.00%, Valid: 74.20% Test: 76.90%
Split: 01, Run: 01, Epoch: 140, Loss: 1367111.2500, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 160, Loss: 1347534.3750, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 180, Loss: 1353461.5000, Train: 100.00%, Valid: 75.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 200, Loss: 1309885.2500, Train: 100.00%, Valid: 75.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 220, Loss: 1326457.5000, Train: 100.00%, Valid: 75.00% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 1285785.7500, Train: 100.00%, Valid: 75.60% Test: 77.40%
Split: 01, Run: 01, Epoch: 260, Loss: 1301501.3750, Train: 100.00%, Valid: 75.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 280, Loss: 1252103.6250, Train: 100.00%, Valid: 75.40% Test: 78.40%
Split: 01, Run: 01, Epoch: 300, Loss: 1253702.5000, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 01, Epoch: 320, Loss: 1263699.8750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 340, Loss: 1202083.8750, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 360, Loss: 1184826.1250, Train: 100.00%, Valid: 74.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 380, Loss: 1192654.8750, Train: 100.00%, Valid: 75.80% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 1184229.8750, Train: 100.00%, Valid: 75.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 420, Loss: 1170162.5000, Train: 100.00%, Valid: 74.60% Test: 77.60%
Split: 01, Run: 01, Epoch: 440, Loss: 1158252.0000, Train: 100.00%, Valid: 76.20% Test: 77.50%
Split: 01, Run: 01, Epoch: 460, Loss: 1178744.6250, Train: 100.00%, Valid: 75.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 480, Loss: 1150219.6250, Train: 100.00%, Valid: 75.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 500, Loss: 1152485.8750, Train: 100.00%, Valid: 76.20% Test: 77.30%
Split: 01, Run: 01
None time:  3.3173357117921114
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 77.10
total time:  3.406997046433389
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 76.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.10 Â± nan
[32m[I 2021-07-19 20:38:11,348][0m Trial 2 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}. Best is trial 1 with value: 76.80000305175781.[0m
lambda1:  0.2
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 989221.4375, Train: 100.00%, Valid: 67.20% Test: 67.30%
Split: 01, Run: 01, Epoch: 40, Loss: 952423.2500, Train: 100.00%, Valid: 66.20% Test: 67.20%
Split: 01, Run: 01, Epoch: 60, Loss: 925208.4375, Train: 100.00%, Valid: 68.60% Test: 68.60%
Split: 01, Run: 01, Epoch: 80, Loss: 887757.8750, Train: 100.00%, Valid: 72.20% Test: 75.00%
Split: 01, Run: 01, Epoch: 100, Loss: 860481.6875, Train: 100.00%, Valid: 74.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 120, Loss: 833047.3750, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 140, Loss: 812903.8750, Train: 100.00%, Valid: 74.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 160, Loss: 800717.3750, Train: 100.00%, Valid: 76.80% Test: 78.50%
Split: 01, Run: 01, Epoch: 180, Loss: 797658.0000, Train: 100.00%, Valid: 75.60% Test: 76.90%
Split: 01, Run: 01, Epoch: 200, Loss: 769796.9375, Train: 100.00%, Valid: 75.60% Test: 78.00%
Split: 01, Run: 01, Epoch: 220, Loss: 776847.6875, Train: 100.00%, Valid: 74.60% Test: 78.10%
Split: 01, Run: 01, Epoch: 240, Loss: 754092.1875, Train: 100.00%, Valid: 77.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 260, Loss: 764205.6250, Train: 100.00%, Valid: 75.80% Test: 78.30%
Split: 01, Run: 01, Epoch: 280, Loss: 736744.7500, Train: 100.00%, Valid: 75.20% Test: 78.60%
Split: 01, Run: 01, Epoch: 300, Loss: 733630.8125, Train: 100.00%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 01, Epoch: 320, Loss: 740496.3750, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 01, Epoch: 340, Loss: 694524.5000, Train: 100.00%, Valid: 75.60% Test: 78.20%
Split: 01, Run: 01, Epoch: 360, Loss: 684537.5000, Train: 100.00%, Valid: 73.80% Test: 76.90%
Split: 01, Run: 01, Epoch: 380, Loss: 683988.4375, Train: 100.00%, Valid: 75.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 680102.0000, Train: 100.00%, Valid: 76.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 420, Loss: 670335.6875, Train: 100.00%, Valid: 74.40% Test: 77.40%
Split: 01, Run: 01, Epoch: 440, Loss: 660795.3125, Train: 100.00%, Valid: 75.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 460, Loss: 681954.6875, Train: 100.00%, Valid: 75.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 480, Loss: 647462.3750, Train: 100.00%, Valid: 75.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 500, Loss: 659670.2500, Train: 100.00%, Valid: 76.20% Test: 77.60%
Split: 01, Run: 01
None time:  3.263585248030722
None Run 01:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 77.60
total time:  3.324026634916663
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 77.80 Â± nan
  Final Train: 100.00 Â± nan
   Final Test: 77.60 Â± nan
[32m[I 2021-07-19 20:38:14,677][0m Trial 3 finished with value: 77.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}. Best is trial 3 with value: 77.80000305175781.[0m
Study statistics: 
  Number of finished trials:  4
  Number of pruned trials:  0
  Number of complete trials:  4
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '77.10 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}   trial.value:  77.8    {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 100.0, 'K': 1}   trial.value:  76.8    {'train': '100.00 Â± nan', 'valid': '76.80 Â± nan', 'test': '74.60 Â± nan'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 1}   trial.value:  69.8    {'train': '100.00 Â± nan', 'valid': '69.80 Â± nan', 'test': '69.80 Â± nan'}
test_acc
['77.10 Â± nan', '77.60 Â± nan', '74.60 Â± nan', '69.80 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 100.0, 'K': 1}
Best trial Value:  77.80000305175781
Best trial Acc:  {'train': '100.00 Â± nan', 'valid': '77.80 Â± nan', 'test': '77.60 Â± nan'}
optuna total time:  20.947967066429555
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [None], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:39:02,253][0m A new study created in memory with name: no-name-cdaec1c1-fe3e-4f16-a258-48a9f9bf2a8b[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=1, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 1 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 4375.0303, Train: 99.29%, Valid: 75.60% Test: 77.50%
Split: 01, Run: 01, Epoch: 40, Loss: 4366.6201, Train: 100.00%, Valid: 75.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 60, Loss: 4364.3818, Train: 100.00%, Valid: 71.80% Test: 74.40%
Split: 01, Run: 01, Epoch: 80, Loss: 4364.0645, Train: 100.00%, Valid: 71.60% Test: 73.30%
Split: 01, Run: 01, Epoch: 100, Loss: 4364.0493, Train: 100.00%, Valid: 71.60% Test: 73.10%
Split: 01, Run: 01, Epoch: 120, Loss: 4362.9639, Train: 100.00%, Valid: 71.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 140, Loss: 4363.8193, Train: 100.00%, Valid: 70.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 160, Loss: 4363.6489, Train: 100.00%, Valid: 72.20% Test: 74.10%
Split: 01, Run: 01, Epoch: 180, Loss: 4363.1494, Train: 100.00%, Valid: 72.00% Test: 72.50%
Split: 01, Run: 01, Epoch: 200, Loss: 4363.1602, Train: 100.00%, Valid: 69.20% Test: 71.00%
Split: 01, Run: 01, Epoch: 220, Loss: 4362.9800, Train: 100.00%, Valid: 71.40% Test: 71.70%
Split: 01, Run: 01, Epoch: 240, Loss: 4363.7197, Train: 100.00%, Valid: 70.60% Test: 70.30%
Split: 01, Run: 01, Epoch: 260, Loss: 4363.0029, Train: 100.00%, Valid: 72.00% Test: 73.00%
Split: 01, Run: 01, Epoch: 280, Loss: 4362.8286, Train: 100.00%, Valid: 70.80% Test: 71.50%
Split: 01, Run: 01, Epoch: 300, Loss: 4362.6914, Train: 100.00%, Valid: 70.20% Test: 70.50%
Split: 01, Run: 01, Epoch: 320, Loss: 4362.6694, Train: 100.00%, Valid: 70.00% Test: 69.90%
Split: 01, Run: 01, Epoch: 340, Loss: 4362.4268, Train: 100.00%, Valid: 70.60% Test: 70.70%
Split: 01, Run: 01, Epoch: 360, Loss: 4362.6646, Train: 100.00%, Valid: 71.60% Test: 70.20%
Split: 01, Run: 01, Epoch: 380, Loss: 4362.8257, Train: 100.00%, Valid: 69.80% Test: 70.40%
Split: 01, Run: 01, Epoch: 400, Loss: 4362.5366, Train: 100.00%, Valid: 69.60% Test: 71.40%
Split: 01, Run: 01, Epoch: 420, Loss: 4362.8418, Train: 100.00%, Valid: 70.60% Test: 70.90%
Split: 01, Run: 01, Epoch: 440, Loss: 4362.2168, Train: 100.00%, Valid: 72.00% Test: 71.50%
Split: 01, Run: 01, Epoch: 460, Loss: 4362.7622, Train: 100.00%, Valid: 72.80% Test: 72.50%
Split: 01, Run: 01, Epoch: 480, Loss: 4362.9204, Train: 100.00%, Valid: 72.20% Test: 72.10%
Split: 01, Run: 01, Epoch: 500, Loss: 4362.4639, Train: 100.00%, Valid: 71.20% Test: 72.90%
Split: 01, Run: 01
None time:  7.308872361667454
None Run 01:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 99.29
   Final Test: 78.40
total time:  9.09096694830805
None All runs:
Highest Train: 100.00 Â± nan
Highest Valid: 77.20 Â± nan
  Final Train: 99.29 Â± nan
   Final Test: 78.40 Â± nan
[32m[I 2021-07-19 20:39:11,354][0m Trial 0 finished with value: 77.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 77.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  77.2    {'train': '99.29 Â± nan', 'valid': '77.20 Â± nan', 'test': '78.40 Â± nan'}
test_acc
['78.40 Â± nan']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  77.20000457763672
Best trial Acc:  {'train': '99.29 Â± nan', 'valid': '77.20 Â± nan', 'test': '78.40 Â± nan'}
optuna total time:  9.105046858079731
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [None], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:41:38,066][0m A new study created in memory with name: no-name-bbea38b2-982d-4073-b836-63496eca103f[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 4375.0303, Train: 99.29%, Valid: 75.60% Test: 77.50%
Split: 01, Run: 01, Epoch: 40, Loss: 4366.6201, Train: 100.00%, Valid: 75.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 60, Loss: 4364.3818, Train: 100.00%, Valid: 71.80% Test: 74.40%
Split: 01, Run: 01, Epoch: 80, Loss: 4364.0645, Train: 100.00%, Valid: 71.60% Test: 73.30%
Split: 01, Run: 01, Epoch: 100, Loss: 4364.0493, Train: 100.00%, Valid: 71.60% Test: 73.10%
Split: 01, Run: 01, Epoch: 120, Loss: 4362.9639, Train: 100.00%, Valid: 71.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 140, Loss: 4363.8193, Train: 100.00%, Valid: 70.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 160, Loss: 4363.6489, Train: 100.00%, Valid: 72.20% Test: 74.10%
Split: 01, Run: 01, Epoch: 180, Loss: 4363.1494, Train: 100.00%, Valid: 72.00% Test: 72.50%
Split: 01, Run: 01, Epoch: 200, Loss: 4363.1602, Train: 100.00%, Valid: 69.20% Test: 71.00%
Split: 01, Run: 01, Epoch: 220, Loss: 4362.9800, Train: 100.00%, Valid: 71.40% Test: 71.70%
Split: 01, Run: 01, Epoch: 240, Loss: 4363.7197, Train: 100.00%, Valid: 70.60% Test: 70.30%
Split: 01, Run: 01, Epoch: 260, Loss: 4363.0029, Train: 100.00%, Valid: 72.00% Test: 73.00%
Split: 01, Run: 01, Epoch: 280, Loss: 4362.8286, Train: 100.00%, Valid: 70.80% Test: 71.50%
Split: 01, Run: 01, Epoch: 300, Loss: 4362.6914, Train: 100.00%, Valid: 70.20% Test: 70.50%
Split: 01, Run: 01, Epoch: 320, Loss: 4362.6694, Train: 100.00%, Valid: 70.00% Test: 69.90%
Split: 01, Run: 01, Epoch: 340, Loss: 4362.4268, Train: 100.00%, Valid: 70.60% Test: 70.70%
Split: 01, Run: 01, Epoch: 360, Loss: 4362.6646, Train: 100.00%, Valid: 71.60% Test: 70.20%
Split: 01, Run: 01, Epoch: 380, Loss: 4362.8257, Train: 100.00%, Valid: 69.80% Test: 70.40%
Split: 01, Run: 01, Epoch: 400, Loss: 4362.5366, Train: 100.00%, Valid: 69.60% Test: 71.40%
Split: 01, Run: 01, Epoch: 420, Loss: 4362.8418, Train: 100.00%, Valid: 70.60% Test: 70.90%
Split: 01, Run: 01, Epoch: 440, Loss: 4362.2168, Train: 100.00%, Valid: 72.00% Test: 71.50%
Split: 01, Run: 01, Epoch: 460, Loss: 4362.7622, Train: 100.00%, Valid: 72.80% Test: 72.50%
Split: 01, Run: 01, Epoch: 480, Loss: 4362.9204, Train: 100.00%, Valid: 72.20% Test: 72.10%
Split: 01, Run: 01, Epoch: 500, Loss: 4362.4639, Train: 100.00%, Valid: 71.20% Test: 72.90%
Split: 01, Run: 01
None time:  6.427151563577354
None Run 01:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 99.29
   Final Test: 78.40
Split: 01, Run: 02, Epoch: 20, Loss: 4369.7505, Train: 99.29%, Valid: 71.20% Test: 73.80%
Split: 01, Run: 02, Epoch: 40, Loss: 4366.1260, Train: 100.00%, Valid: 70.60% Test: 74.10%
Split: 01, Run: 02, Epoch: 60, Loss: 4364.2144, Train: 100.00%, Valid: 71.20% Test: 72.80%
Split: 01, Run: 02, Epoch: 80, Loss: 4363.7368, Train: 100.00%, Valid: 71.40% Test: 72.20%
Split: 01, Run: 02, Epoch: 100, Loss: 4363.7944, Train: 100.00%, Valid: 70.60% Test: 72.50%
Split: 01, Run: 02, Epoch: 120, Loss: 4363.6118, Train: 100.00%, Valid: 71.00% Test: 70.90%
Split: 01, Run: 02, Epoch: 140, Loss: 4363.5723, Train: 100.00%, Valid: 70.40% Test: 73.30%
Split: 01, Run: 02, Epoch: 160, Loss: 4363.2300, Train: 100.00%, Valid: 71.40% Test: 74.40%
Split: 01, Run: 02, Epoch: 180, Loss: 4362.8994, Train: 100.00%, Valid: 71.00% Test: 73.50%
Split: 01, Run: 02, Epoch: 200, Loss: 4363.0806, Train: 100.00%, Valid: 71.00% Test: 72.50%
Split: 01, Run: 02, Epoch: 220, Loss: 4362.3550, Train: 100.00%, Valid: 71.00% Test: 72.90%
Split: 01, Run: 02, Epoch: 240, Loss: 4362.2520, Train: 100.00%, Valid: 71.00% Test: 72.90%
Split: 01, Run: 02, Epoch: 260, Loss: 4362.5161, Train: 100.00%, Valid: 71.20% Test: 71.00%
Split: 01, Run: 02, Epoch: 280, Loss: 4362.8086, Train: 100.00%, Valid: 71.60% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 4362.3369, Train: 100.00%, Valid: 72.60% Test: 73.40%
Split: 01, Run: 02, Epoch: 320, Loss: 4362.9658, Train: 100.00%, Valid: 72.40% Test: 72.70%
Split: 01, Run: 02, Epoch: 340, Loss: 4362.3779, Train: 100.00%, Valid: 70.20% Test: 70.80%
Split: 01, Run: 02, Epoch: 360, Loss: 4362.3662, Train: 100.00%, Valid: 71.20% Test: 72.30%
Split: 01, Run: 02, Epoch: 380, Loss: 4363.0068, Train: 100.00%, Valid: 70.60% Test: 70.90%
Split: 01, Run: 02, Epoch: 400, Loss: 4362.7168, Train: 100.00%, Valid: 70.60% Test: 72.00%
Split: 01, Run: 02, Epoch: 420, Loss: 4362.5596, Train: 100.00%, Valid: 71.00% Test: 71.90%
Split: 01, Run: 02, Epoch: 440, Loss: 4362.7529, Train: 100.00%, Valid: 70.60% Test: 71.10%
Split: 01, Run: 02, Epoch: 460, Loss: 4362.3179, Train: 100.00%, Valid: 70.20% Test: 73.10%
Split: 01, Run: 02, Epoch: 480, Loss: 4362.0850, Train: 100.00%, Valid: 70.60% Test: 71.80%
Split: 01, Run: 02, Epoch: 500, Loss: 4362.5708, Train: 100.00%, Valid: 69.80% Test: 71.80%
Split: 01, Run: 02
None time:  5.840069914236665
None Run 02:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 99.29
   Final Test: 77.10
Split: 01, Run: 03, Epoch: 20, Loss: 4371.4736, Train: 100.00%, Valid: 77.80% Test: 78.70%
Split: 01, Run: 03, Epoch: 40, Loss: 4366.1182, Train: 100.00%, Valid: 75.20% Test: 77.30%
Split: 01, Run: 03, Epoch: 60, Loss: 4364.0986, Train: 100.00%, Valid: 72.00% Test: 74.20%
Split: 01, Run: 03, Epoch: 80, Loss: 4364.1089, Train: 100.00%, Valid: 72.60% Test: 74.50%
Split: 01, Run: 03, Epoch: 100, Loss: 4363.9648, Train: 100.00%, Valid: 72.40% Test: 73.60%
Split: 01, Run: 03, Epoch: 120, Loss: 4363.3276, Train: 100.00%, Valid: 72.60% Test: 74.80%
Split: 01, Run: 03, Epoch: 140, Loss: 4363.1450, Train: 100.00%, Valid: 72.60% Test: 73.70%
Split: 01, Run: 03, Epoch: 160, Loss: 4363.4707, Train: 100.00%, Valid: 72.20% Test: 73.40%
Split: 01, Run: 03, Epoch: 180, Loss: 4362.8599, Train: 100.00%, Valid: 71.20% Test: 73.50%
Split: 01, Run: 03, Epoch: 200, Loss: 4363.0986, Train: 100.00%, Valid: 71.80% Test: 73.90%
Split: 01, Run: 03, Epoch: 220, Loss: 4362.9526, Train: 100.00%, Valid: 69.60% Test: 73.00%
Split: 01, Run: 03, Epoch: 240, Loss: 4362.6089, Train: 100.00%, Valid: 69.60% Test: 73.20%
Split: 01, Run: 03, Epoch: 260, Loss: 4362.9092, Train: 100.00%, Valid: 70.80% Test: 73.10%
Split: 01, Run: 03, Epoch: 280, Loss: 4362.8296, Train: 100.00%, Valid: 71.00% Test: 74.70%
Split: 01, Run: 03, Epoch: 300, Loss: 4362.8198, Train: 100.00%, Valid: 70.40% Test: 73.90%
Split: 01, Run: 03, Epoch: 320, Loss: 4362.7510, Train: 100.00%, Valid: 72.40% Test: 73.30%
Split: 01, Run: 03, Epoch: 340, Loss: 4363.0483, Train: 100.00%, Valid: 71.20% Test: 72.70%
Split: 01, Run: 03, Epoch: 360, Loss: 4362.9146, Train: 100.00%, Valid: 71.60% Test: 73.50%
Split: 01, Run: 03, Epoch: 380, Loss: 4363.2900, Train: 100.00%, Valid: 72.00% Test: 74.00%
Split: 01, Run: 03, Epoch: 400, Loss: 4362.5806, Train: 100.00%, Valid: 71.60% Test: 74.00%
Split: 01, Run: 03, Epoch: 420, Loss: 4362.5889, Train: 100.00%, Valid: 71.60% Test: 72.80%
Split: 01, Run: 03, Epoch: 440, Loss: 4363.0068, Train: 100.00%, Valid: 72.80% Test: 73.50%
Split: 01, Run: 03, Epoch: 460, Loss: 4362.7148, Train: 100.00%, Valid: 73.20% Test: 73.30%
Split: 01, Run: 03, Epoch: 480, Loss: 4362.5254, Train: 100.00%, Valid: 73.60% Test: 73.10%
Split: 01, Run: 03, Epoch: 500, Loss: 4362.7036, Train: 100.00%, Valid: 72.40% Test: 73.30%
Split: 01, Run: 03
None time:  6.0344959469512105
None Run 03:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 100.00
   Final Test: 78.80
total time:  20.055423690006137
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.40 Â± 0.92
  Final Train: 99.52 Â± 0.41
   Final Test: 78.10 Â± 0.89
[32m[I 2021-07-19 20:41:58,131][0m Trial 0 finished with value: 77.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 77.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  77.4    {'train': '99.52 Â± 0.41', 'valid': '77.40 Â± 0.92', 'test': '78.10 Â± 0.89'}
test_acc
['78.10 Â± 0.89']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  77.4000015258789
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '77.40 Â± 0.92', 'test': '78.10 Â± 0.89'}
optuna total time:  20.068321416154504
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [None], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:42:25,573][0m A new study created in memory with name: no-name-1a2c652b-561a-4760-ab94-27b954464c49[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6600, Train: 92.14%, Valid: 57.00% Test: 58.60%
Split: 01, Run: 01, Epoch: 40, Loss: 0.9356, Train: 99.29%, Valid: 76.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4751, Train: 99.29%, Valid: 77.80% Test: 79.10%
Split: 01, Run: 01, Epoch: 80, Loss: 0.3433, Train: 100.00%, Valid: 77.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3013, Train: 100.00%, Valid: 76.80% Test: 78.10%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2429, Train: 100.00%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2602, Train: 100.00%, Valid: 77.00% Test: 77.40%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2063, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2358, Train: 100.00%, Valid: 77.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2084, Train: 100.00%, Valid: 76.40% Test: 76.60%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2075, Train: 100.00%, Valid: 76.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2084, Train: 100.00%, Valid: 76.00% Test: 76.90%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2131, Train: 100.00%, Valid: 76.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2030, Train: 100.00%, Valid: 76.00% Test: 77.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1855, Train: 100.00%, Valid: 76.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 320, Loss: 0.1835, Train: 100.00%, Valid: 77.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.1539, Train: 100.00%, Valid: 77.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1454, Train: 100.00%, Valid: 75.40% Test: 77.00%
Split: 01, Run: 01, Epoch: 380, Loss: 0.1993, Train: 100.00%, Valid: 76.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1669, Train: 100.00%, Valid: 76.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1812, Train: 100.00%, Valid: 74.40% Test: 76.20%
Split: 01, Run: 01, Epoch: 440, Loss: 0.1661, Train: 100.00%, Valid: 78.00% Test: 78.80%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1816, Train: 100.00%, Valid: 76.20% Test: 78.30%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2042, Train: 100.00%, Valid: 77.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1846, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01
None time:  6.2979934345930815
None Run 01:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 02, Epoch: 20, Loss: 1.6338, Train: 97.86%, Valid: 71.40% Test: 74.10%
Split: 01, Run: 02, Epoch: 40, Loss: 0.8782, Train: 99.29%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 02, Epoch: 60, Loss: 0.4303, Train: 99.29%, Valid: 77.60% Test: 77.60%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3292, Train: 99.29%, Valid: 75.80% Test: 77.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3135, Train: 100.00%, Valid: 78.00% Test: 78.60%
Split: 01, Run: 02, Epoch: 120, Loss: 0.2849, Train: 100.00%, Valid: 77.40% Test: 77.30%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2348, Train: 100.00%, Valid: 77.60% Test: 78.10%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2276, Train: 100.00%, Valid: 77.80% Test: 78.10%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2253, Train: 99.29%, Valid: 74.40% Test: 77.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1752, Train: 100.00%, Valid: 76.20% Test: 77.80%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1961, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 02, Epoch: 240, Loss: 0.1916, Train: 100.00%, Valid: 78.20% Test: 76.90%
Split: 01, Run: 02, Epoch: 260, Loss: 0.1912, Train: 100.00%, Valid: 77.00% Test: 77.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1906, Train: 100.00%, Valid: 79.00% Test: 78.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1759, Train: 100.00%, Valid: 74.40% Test: 75.10%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1757, Train: 100.00%, Valid: 75.80% Test: 76.60%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1586, Train: 100.00%, Valid: 74.60% Test: 77.50%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1722, Train: 100.00%, Valid: 76.40% Test: 77.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.1893, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1588, Train: 100.00%, Valid: 75.60% Test: 77.10%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1956, Train: 100.00%, Valid: 75.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1525, Train: 100.00%, Valid: 75.40% Test: 75.90%
Split: 01, Run: 02, Epoch: 460, Loss: 0.1613, Train: 100.00%, Valid: 78.60% Test: 79.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1619, Train: 100.00%, Valid: 77.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1613, Train: 100.00%, Valid: 76.60% Test: 77.70%
Split: 01, Run: 02
None time:  5.75355192553252
None Run 02:
Highest Train: 100.00
Highest Valid: 79.60
  Final Train: 100.00
   Final Test: 79.20
Split: 01, Run: 03, Epoch: 20, Loss: 1.6158, Train: 95.71%, Valid: 63.40% Test: 67.20%
Split: 01, Run: 03, Epoch: 40, Loss: 0.9170, Train: 99.29%, Valid: 76.60% Test: 76.90%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4520, Train: 99.29%, Valid: 78.00% Test: 78.30%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3935, Train: 99.29%, Valid: 78.20% Test: 78.30%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3224, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.2534, Train: 100.00%, Valid: 78.00% Test: 79.00%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2228, Train: 100.00%, Valid: 74.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2194, Train: 100.00%, Valid: 76.20% Test: 76.30%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2167, Train: 100.00%, Valid: 77.40% Test: 79.20%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2137, Train: 100.00%, Valid: 77.80% Test: 78.20%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2107, Train: 100.00%, Valid: 77.20% Test: 77.20%
Split: 01, Run: 03, Epoch: 240, Loss: 0.1866, Train: 100.00%, Valid: 75.80% Test: 77.30%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1818, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1957, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2013, Train: 100.00%, Valid: 76.20% Test: 78.10%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1965, Train: 100.00%, Valid: 77.20% Test: 78.20%
Split: 01, Run: 03, Epoch: 360, Loss: 0.2044, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1936, Train: 100.00%, Valid: 76.60% Test: 77.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2287, Train: 100.00%, Valid: 77.60% Test: 79.00%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1648, Train: 100.00%, Valid: 76.40% Test: 77.20%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1708, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1945, Train: 100.00%, Valid: 77.40% Test: 79.10%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1512, Train: 100.00%, Valid: 77.80% Test: 78.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1699, Train: 100.00%, Valid: 77.00% Test: 78.90%
Split: 01, Run: 03
None time:  5.753459678031504
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.50
total time:  19.576904052868485
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.07 Â± 0.46
  Final Train: 100.00 Â± 0.00
   Final Test: 78.77 Â± 0.38
[32m[I 2021-07-19 20:42:45,161][0m Trial 0 finished with value: 79.06665802001953 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 79.06665802001953.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  79.067    {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
test_acc
['78.77 Â± 0.38']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  79.06665802001953
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
optuna total time:  19.592842894606292
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ElasticGNN', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [None], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:43:04,699][0m A new study created in memory with name: no-name-1a98c2c7-88ab-4b82-b017-cb213c161e83[0m
[33m[W 2021-07-19 20:43:04,701][0m Trial 0 failed because of the following error: TypeError("'<=' not supported between instances of 'int' and 'NoneType'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 58, in objective
    args = set_up_trial(trial, args)
  File "main_optuna.py", line 186, in set_up_trial
    args.lambda1 = trial.suggest_uniform('lambda1', 0, 1000)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/trial/_trial.py", line 231, in suggest_uniform
    return self._suggest(name, distribution)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/trial/_trial.py", line 693, in _suggest
    study, trial, name, distribution
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/samplers/_grid.py", line 160, in sample_independent
    contains = param_distribution._contains(param_distribution.to_internal_repr(param_value))
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/distributions.py", line 134, in _contains
    return self.low <= value < self.high
TypeError: '<=' not supported between instances of 'int' and 'NoneType'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 322, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 58, in objective
    args = set_up_trial(trial, args)
  File "main_optuna.py", line 186, in set_up_trial
    args.lambda1 = trial.suggest_uniform('lambda1', 0, 1000)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/trial/_trial.py", line 231, in suggest_uniform
    return self._suggest(name, distribution)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/trial/_trial.py", line 693, in _suggest
    study, trial, name, distribution
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/samplers/_grid.py", line 160, in sample_independent
    contains = param_distribution._contains(param_distribution.to_internal_repr(param_value))
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/distributions.py", line 134, in _contains
    return self.low <= value < self.high
TypeError: '<=' not supported between instances of 'int' and 'NoneType'
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ElasticGNN', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:43:33,467][0m A new study created in memory with name: no-name-87585ddf-86df-459f-9eb4-10d151a21d71[0m
lambda1:  1.0
lambda2:  100.0
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ElasticGNN', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
[33m[W 2021-07-19 20:43:35,208][0m Trial 0 failed because of the following error: NameError("name 'ElasticGNN' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 86, in get_model
    model = ElasticGNN(in_channels=data.num_features,
NameError: name 'ElasticGNN' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 322, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 86, in get_model
    model = ElasticGNN(in_channels=data.num_features, 
NameError: name 'ElasticGNN' is not defined
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:44:24,545][0m A new study created in memory with name: no-name-ee6dfc72-af07-4130-9ec9-222869232058[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6600, Train: 92.14%, Valid: 57.00% Test: 58.60%
Split: 01, Run: 01, Epoch: 40, Loss: 0.9356, Train: 99.29%, Valid: 76.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4751, Train: 99.29%, Valid: 77.80% Test: 79.10%
Split: 01, Run: 01, Epoch: 80, Loss: 0.3433, Train: 100.00%, Valid: 77.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3013, Train: 100.00%, Valid: 76.80% Test: 78.10%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2429, Train: 100.00%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2602, Train: 100.00%, Valid: 77.00% Test: 77.40%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2063, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2358, Train: 100.00%, Valid: 77.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2084, Train: 100.00%, Valid: 76.40% Test: 76.60%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2075, Train: 100.00%, Valid: 76.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2084, Train: 100.00%, Valid: 76.00% Test: 76.90%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2131, Train: 100.00%, Valid: 76.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2030, Train: 100.00%, Valid: 76.00% Test: 77.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1855, Train: 100.00%, Valid: 76.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 320, Loss: 0.1835, Train: 100.00%, Valid: 77.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 340, Loss: 0.1539, Train: 100.00%, Valid: 77.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1454, Train: 100.00%, Valid: 75.40% Test: 77.00%
Split: 01, Run: 01, Epoch: 380, Loss: 0.1993, Train: 100.00%, Valid: 76.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1669, Train: 100.00%, Valid: 76.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1812, Train: 100.00%, Valid: 74.40% Test: 76.20%
Split: 01, Run: 01, Epoch: 440, Loss: 0.1661, Train: 100.00%, Valid: 78.00% Test: 78.80%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1816, Train: 100.00%, Valid: 76.20% Test: 78.30%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2042, Train: 100.00%, Valid: 77.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1846, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01
None time:  6.20551660656929
None Run 01:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 02, Epoch: 20, Loss: 1.6338, Train: 97.86%, Valid: 71.40% Test: 74.10%
Split: 01, Run: 02, Epoch: 40, Loss: 0.8782, Train: 99.29%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 02, Epoch: 60, Loss: 0.4303, Train: 99.29%, Valid: 77.60% Test: 77.60%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3292, Train: 99.29%, Valid: 75.80% Test: 77.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3135, Train: 100.00%, Valid: 78.00% Test: 78.60%
Split: 01, Run: 02, Epoch: 120, Loss: 0.2849, Train: 100.00%, Valid: 77.40% Test: 77.30%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2348, Train: 100.00%, Valid: 77.60% Test: 78.10%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2276, Train: 100.00%, Valid: 77.80% Test: 78.10%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2253, Train: 99.29%, Valid: 74.40% Test: 77.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1752, Train: 100.00%, Valid: 76.20% Test: 77.80%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1961, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 02, Epoch: 240, Loss: 0.1916, Train: 100.00%, Valid: 78.20% Test: 76.90%
Split: 01, Run: 02, Epoch: 260, Loss: 0.1912, Train: 100.00%, Valid: 77.00% Test: 77.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1906, Train: 100.00%, Valid: 79.00% Test: 78.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1759, Train: 100.00%, Valid: 74.40% Test: 75.10%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1757, Train: 100.00%, Valid: 75.80% Test: 76.60%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1586, Train: 100.00%, Valid: 74.60% Test: 77.50%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1722, Train: 100.00%, Valid: 76.40% Test: 77.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.1893, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1588, Train: 100.00%, Valid: 75.60% Test: 77.10%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1956, Train: 100.00%, Valid: 75.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1525, Train: 100.00%, Valid: 75.40% Test: 75.90%
Split: 01, Run: 02, Epoch: 460, Loss: 0.1613, Train: 100.00%, Valid: 78.60% Test: 79.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1619, Train: 100.00%, Valid: 77.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1613, Train: 100.00%, Valid: 76.60% Test: 77.70%
Split: 01, Run: 02
None time:  5.591815257444978
None Run 02:
Highest Train: 100.00
Highest Valid: 79.60
  Final Train: 100.00
   Final Test: 79.20
Split: 01, Run: 03, Epoch: 20, Loss: 1.6158, Train: 95.71%, Valid: 63.40% Test: 67.20%
Split: 01, Run: 03, Epoch: 40, Loss: 0.9170, Train: 99.29%, Valid: 76.60% Test: 76.90%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4520, Train: 99.29%, Valid: 78.00% Test: 78.30%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3935, Train: 99.29%, Valid: 78.20% Test: 78.30%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3224, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.2534, Train: 100.00%, Valid: 78.00% Test: 79.00%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2228, Train: 100.00%, Valid: 74.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2194, Train: 100.00%, Valid: 76.20% Test: 76.30%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2167, Train: 100.00%, Valid: 77.40% Test: 79.20%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2137, Train: 100.00%, Valid: 77.80% Test: 78.20%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2107, Train: 100.00%, Valid: 77.20% Test: 77.20%
Split: 01, Run: 03, Epoch: 240, Loss: 0.1866, Train: 100.00%, Valid: 75.80% Test: 77.30%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1818, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1957, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2013, Train: 100.00%, Valid: 76.20% Test: 78.10%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1965, Train: 100.00%, Valid: 77.20% Test: 78.20%
Split: 01, Run: 03, Epoch: 360, Loss: 0.2044, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1936, Train: 100.00%, Valid: 76.60% Test: 77.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2287, Train: 100.00%, Valid: 77.60% Test: 79.00%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1648, Train: 100.00%, Valid: 76.40% Test: 77.20%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1708, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1945, Train: 100.00%, Valid: 77.40% Test: 79.10%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1512, Train: 100.00%, Valid: 77.80% Test: 78.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1699, Train: 100.00%, Valid: 77.00% Test: 78.90%
Split: 01, Run: 03
None time:  5.541698384098709
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.50
total time:  19.046638142317533
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.07 Â± 0.46
  Final Train: 100.00 Â± 0.00
   Final Test: 78.77 Â± 0.38
[32m[I 2021-07-19 20:44:43,602][0m Trial 0 finished with value: 79.06665802001953 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 79.06665802001953.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  79.067    {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
test_acc
['78.77 Â± 0.38']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  79.06665802001953
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
optuna total time:  19.0608138628304
Using backend: pytorch
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 20:47:55,908][0m A new study created in memory with name: no-name-d89c7a1a-1c1f-42cc-8e15-ea6b47346c58[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Using backend: pytorch
main:  Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [1]}
num_trial:  1
[32m[I 2021-07-19 20:48:07,037][0m A new study created in memory with name: no-name-6b2a70ea-6a92-43e8-a2c1-92353ccfe97a[0m
K:  1
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=1, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=1, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.6600, Train: 92.14%, Valid: 57.00% Test: 58.60%
Split: 01, Run: 01, Epoch: 40, Loss: 0.9356, Train: 99.29%, Valid: 76.00% Test: 77.20%
Split: 01, Run: 01, Epoch: 60, Loss: 0.4751, Train: 99.29%, Valid: 77.80% Test: 79.10%
Split: 01, Run: 01, Epoch: 80, Loss: 0.3433, Train: 100.00%, Valid: 77.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3013, Train: 100.00%, Valid: 76.80% Test: 78.10%
Split: 01, Run: 01, Epoch: 120, Loss: 0.2429, Train: 100.00%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 01, Epoch: 140, Loss: 0.2602, Train: 100.00%, Valid: 77.00% Test: 77.40%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2063, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2358, Train: 100.00%, Valid: 77.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2084, Train: 100.00%, Valid: 76.40% Test: 76.60%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2075, Train: 100.00%, Valid: 76.40% Test: 77.80%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2084, Train: 100.00%, Valid: 76.00% Test: 76.90%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2131, Train: 100.00%, Valid: 76.80% Test: 77.40%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2030, Train: 100.00%, Valid: 76.00% Test: 77.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1855, Train: 100.00%, Valid: 76.60% Test: 77.20%
Split: 01, Run: 01, Epoch: 320, Loss: 0.1835, Train: 100.00%, Valid: 77.60% Test: 77.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  16.18353536631912
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 01, Epoch: 340, Loss: 0.1539, Train: 100.00%, Valid: 77.00% Test: 76.70%
Split: 01, Run: 01, Epoch: 360, Loss: 0.1454, Train: 100.00%, Valid: 75.40% Test: 77.00%
Split: 01, Run: 01, Epoch: 380, Loss: 0.1993, Train: 100.00%, Valid: 76.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1669, Train: 100.00%, Valid: 76.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 420, Loss: 0.1812, Train: 100.00%, Valid: 74.40% Test: 76.20%
Split: 01, Run: 01, Epoch: 440, Loss: 0.1661, Train: 100.00%, Valid: 78.00% Test: 78.80%
Split: 01, Run: 01, Epoch: 460, Loss: 0.1816, Train: 100.00%, Valid: 76.20% Test: 78.30%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2042, Train: 100.00%, Valid: 77.00% Test: 77.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1846, Train: 100.00%, Valid: 75.40% Test: 77.40%
Split: 01, Run: 01
None time:  7.304263080470264
None Run 01:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 02, Epoch: 20, Loss: 1.6338, Train: 97.86%, Valid: 71.40% Test: 74.10%
Split: 01, Run: 02, Epoch: 40, Loss: 0.8782, Train: 99.29%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 02, Epoch: 60, Loss: 0.4303, Train: 99.29%, Valid: 77.60% Test: 77.60%
Split: 01, Run: 02, Epoch: 80, Loss: 0.3292, Train: 99.29%, Valid: 75.80% Test: 77.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3135, Train: 100.00%, Valid: 78.00% Test: 78.60%
Split: 01, Run: 02, Epoch: 120, Loss: 0.2849, Train: 100.00%, Valid: 77.40% Test: 77.30%
Split: 01, Run: 02, Epoch: 140, Loss: 0.2348, Train: 100.00%, Valid: 77.60% Test: 78.10%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2276, Train: 100.00%, Valid: 77.80% Test: 78.10%
Split: 01, Run: 02, Epoch: 180, Loss: 0.2253, Train: 99.29%, Valid: 74.40% Test: 77.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.1752, Train: 100.00%, Valid: 76.20% Test: 77.80%
Split: 01, Run: 02, Epoch: 220, Loss: 0.1961, Train: 100.00%, Valid: 76.00% Test: 77.70%
Split: 01, Run: 02, Epoch: 240, Loss: 0.1916, Train: 100.00%, Valid: 78.20% Test: 76.90%
Split: 01, Run: 02, Epoch: 260, Loss: 0.1912, Train: 100.00%, Valid: 77.00% Test: 77.10%
Split: 01, Run: 02, Epoch: 280, Loss: 0.1906, Train: 100.00%, Valid: 79.00% Test: 78.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1759, Train: 100.00%, Valid: 74.40% Test: 75.10%
Split: 01, Run: 02, Epoch: 320, Loss: 0.1757, Train: 100.00%, Valid: 75.80% Test: 76.60%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 340, Loss: 0.1586, Train: 100.00%, Valid: 74.60% Test: 77.50%
Split: 01, Run: 02, Epoch: 360, Loss: 0.1722, Train: 100.00%, Valid: 76.40% Test: 77.40%
Split: 01, Run: 02, Epoch: 380, Loss: 0.1893, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1588, Train: 100.00%, Valid: 75.60% Test: 77.10%
Split: 01, Run: 02, Epoch: 420, Loss: 0.1956, Train: 100.00%, Valid: 75.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 440, Loss: 0.1525, Train: 100.00%, Valid: 75.40% Test: 75.90%
Split: 01, Run: 02, Epoch: 460, Loss: 0.1613, Train: 100.00%, Valid: 78.60% Test: 79.30%
Split: 01, Run: 02, Epoch: 480, Loss: 0.1619, Train: 100.00%, Valid: 77.00% Test: 76.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1613, Train: 100.00%, Valid: 76.60% Test: 77.70%
Split: 01, Run: 02
None time:  6.734761883504689
None Run 02:
Highest Train: 100.00
Highest Valid: 79.60
  Final Train: 100.00
   Final Test: 79.20
Split: 01, Run: 03, Epoch: 20, Loss: 1.6158, Train: 95.71%, Valid: 63.40% Test: 67.20%
Split: 01, Run: 03, Epoch: 40, Loss: 0.9170, Train: 99.29%, Valid: 76.60% Test: 76.90%
Split: 01, Run: 03, Epoch: 60, Loss: 0.4520, Train: 99.29%, Valid: 78.00% Test: 78.30%
Split: 01, Run: 03, Epoch: 80, Loss: 0.3935, Train: 99.29%, Valid: 78.20% Test: 78.30%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3224, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 120, Loss: 0.2534, Train: 100.00%, Valid: 78.00% Test: 79.00%
Split: 01, Run: 03, Epoch: 140, Loss: 0.2228, Train: 100.00%, Valid: 74.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 160, Loss: 0.2194, Train: 100.00%, Valid: 76.20% Test: 76.30%
Split: 01, Run: 03, Epoch: 180, Loss: 0.2167, Train: 100.00%, Valid: 77.40% Test: 79.20%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2137, Train: 100.00%, Valid: 77.80% Test: 78.20%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2107, Train: 100.00%, Valid: 77.20% Test: 77.20%
Split: 01, Run: 03, Epoch: 240, Loss: 0.1866, Train: 100.00%, Valid: 75.80% Test: 77.30%
Split: 01, Run: 03, Epoch: 260, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 76.90%
Split: 01, Run: 03, Epoch: 280, Loss: 0.1818, Train: 100.00%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.1957, Train: 100.00%, Valid: 76.00% Test: 77.00%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2013, Train: 100.00%, Valid: 76.20% Test: 78.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 03, Epoch: 340, Loss: 0.1965, Train: 100.00%, Valid: 77.20% Test: 78.20%
Split: 01, Run: 03, Epoch: 360, Loss: 0.2044, Train: 100.00%, Valid: 76.00% Test: 76.80%
Split: 01, Run: 03, Epoch: 380, Loss: 0.1936, Train: 100.00%, Valid: 76.60% Test: 77.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2287, Train: 100.00%, Valid: 77.60% Test: 79.00%
Split: 01, Run: 03, Epoch: 420, Loss: 0.1648, Train: 100.00%, Valid: 76.40% Test: 77.20%
Split: 01, Run: 03, Epoch: 440, Loss: 0.1708, Train: 100.00%, Valid: 77.60% Test: 78.40%
Split: 01, Run: 03, Epoch: 460, Loss: 0.1945, Train: 100.00%, Valid: 77.40% Test: 79.10%
Split: 01, Run: 03, Epoch: 480, Loss: 0.1512, Train: 100.00%, Valid: 77.80% Test: 78.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1699, Train: 100.00%, Valid: 77.00% Test: 78.90%
Split: 01, Run: 03
None time:  6.733198854140937
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.50
total time:  22.561401098035276
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.07 Â± 0.46
  Final Train: 100.00 Â± 0.00
   Final Test: 78.77 Â± 0.38
[32m[I 2021-07-19 20:48:29,608][0m Trial 0 finished with value: 79.06665802001953 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}. Best is trial 0 with value: 79.06665802001953.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}   trial.value:  79.067    {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
test_acc
['78.77 Â± 0.38']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 1}
Best trial Value:  79.06665802001953
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '79.07 Â± 0.46', 'test': '78.77 Â± 0.38'}
optuna total time:  22.576220449991524
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  16.633112798444927
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 20:48:41,727][0m A new study created in memory with name: no-name-c4a19b7b-6629-44d7-b409-635b9a467fab[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 1.7092, Train: 84.29%, Valid: 58.40% Test: 59.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  14.703908771276474
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  49.34353129938245
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-19 20:48:45,303][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  49.398534967564046
Split: 01, Run: 01, Epoch: 40, Loss: 1.0790, Train: 94.29%, Valid: 78.60% Test: 80.20%
Split: 01, Run: 01, Epoch: 60, Loss: 0.6131, Train: 95.71%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 01, Epoch: 80, Loss: 0.4464, Train: 97.14%, Valid: 79.40% Test: 83.10%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 120, Loss: 0.3556, Train: 98.57%, Valid: 79.20% Test: 82.90%
Split: 01, Run: 01, Epoch: 140, Loss: 0.3285, Train: 99.29%, Valid: 79.80% Test: 83.10%
Split: 01, Run: 01, Epoch: 160, Loss: 0.2790, Train: 99.29%, Valid: 79.40% Test: 82.40%
Split: 01, Run: 01, Epoch: 180, Loss: 0.2849, Train: 99.29%, Valid: 80.40% Test: 83.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 220, Loss: 0.2622, Train: 100.00%, Valid: 78.60% Test: 81.70%
Split: 01, Run: 01, Epoch: 240, Loss: 0.2473, Train: 99.29%, Valid: 79.20% Test: 83.20%
Split: 01, Run: 01, Epoch: 260, Loss: 0.2660, Train: 99.29%, Valid: 78.60% Test: 83.60%
Split: 01, Run: 01, Epoch: 280, Loss: 0.2751, Train: 99.29%, Valid: 79.40% Test: 82.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 320, Loss: 0.2426, Train: 100.00%, Valid: 79.40% Test: 82.20%
Split: 01, Run: 01, Epoch: 340, Loss: 0.2268, Train: 100.00%, Valid: 80.00% Test: 83.30%
Split: 01, Run: 01, Epoch: 360, Loss: 0.2095, Train: 99.29%, Valid: 79.60% Test: 82.10%
Split: 01, Run: 01, Epoch: 380, Loss: 0.2250, Train: 99.29%, Valid: 79.60% Test: 83.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 420, Loss: 0.2112, Train: 100.00%, Valid: 79.40% Test: 82.30%
Split: 01, Run: 01, Epoch: 440, Loss: 0.2220, Train: 99.29%, Valid: 79.20% Test: 82.50%
Split: 01, Run: 01, Epoch: 460, Loss: 0.2422, Train: 100.00%, Valid: 80.00% Test: 82.80%
Split: 01, Run: 01, Epoch: 480, Loss: 0.2248, Train: 99.29%, Valid: 78.60% Test: 82.50%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  15.121726934798062
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 20, Loss: 1.6931, Train: 87.14%, Valid: 75.20% Test: 78.70%
Split: 01, Run: 02, Epoch: 40, Loss: 1.0466, Train: 93.57%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 60, Loss: 0.5792, Train: 95.71%, Valid: 79.60% Test: 81.10%
Split: 01, Run: 02, Epoch: 80, Loss: 0.4393, Train: 97.86%, Valid: 78.60% Test: 82.80%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 120, Loss: 0.3790, Train: 98.57%, Valid: 79.40% Test: 82.30%
Split: 01, Run: 02, Epoch: 140, Loss: 0.3177, Train: 99.29%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 02, Epoch: 160, Loss: 0.2975, Train: 99.29%, Valid: 78.40% Test: 82.50%
Split: 01, Run: 02, Epoch: 180, Loss: 0.3074, Train: 99.29%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 220, Loss: 0.2328, Train: 99.29%, Valid: 80.00% Test: 82.20%
Split: 01, Run: 02, Epoch: 240, Loss: 0.2679, Train: 99.29%, Valid: 79.00% Test: 83.40%
Split: 01, Run: 02, Epoch: 260, Loss: 0.2469, Train: 100.00%, Valid: 79.40% Test: 82.60%
Split: 01, Run: 02, Epoch: 280, Loss: 0.2275, Train: 100.00%, Valid: 79.80% Test: 82.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 320, Loss: 0.2226, Train: 99.29%, Valid: 79.00% Test: 82.60%
Split: 01, Run: 02, Epoch: 340, Loss: 0.2254, Train: 99.29%, Valid: 78.60% Test: 80.90%
Split: 01, Run: 02, Epoch: 360, Loss: 0.2170, Train: 100.00%, Valid: 79.20% Test: 81.80%
Split: 01, Run: 02, Epoch: 380, Loss: 0.2212, Train: 100.00%, Valid: 78.80% Test: 83.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 420, Loss: 0.2432, Train: 99.29%, Valid: 78.40% Test: 82.10%
Split: 01, Run: 02, Epoch: 440, Loss: 0.2190, Train: 99.29%, Valid: 79.60% Test: 82.10%
Split: 01, Run: 02, Epoch: 460, Loss: 0.2035, Train: 99.29%, Valid: 79.00% Test: 82.80%
Split: 01, Run: 02, Epoch: 480, Loss: 0.2217, Train: 100.00%, Valid: 79.40% Test: 83.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  14.292594343423843
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 20, Loss: 1.6925, Train: 87.14%, Valid: 69.60% Test: 68.20%
Split: 01, Run: 03, Epoch: 40, Loss: 1.1037, Train: 93.57%, Valid: 79.00% Test: 79.60%
Split: 01, Run: 03, Epoch: 60, Loss: 0.6006, Train: 96.43%, Valid: 79.60% Test: 83.10%
Split: 01, Run: 03, Epoch: 80, Loss: 0.4876, Train: 97.14%, Valid: 79.40% Test: 81.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 120, Loss: 0.3219, Train: 98.57%, Valid: 80.80% Test: 83.20%
Split: 01, Run: 03, Epoch: 140, Loss: 0.3089, Train: 99.29%, Valid: 79.40% Test: 81.90%
Split: 01, Run: 03, Epoch: 160, Loss: 0.3108, Train: 98.57%, Valid: 78.60% Test: 81.30%
Split: 01, Run: 03, Epoch: 180, Loss: 0.3099, Train: 99.29%, Valid: 78.80% Test: 82.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 220, Loss: 0.2730, Train: 98.57%, Valid: 78.20% Test: 82.10%
Split: 01, Run: 03, Epoch: 240, Loss: 0.2491, Train: 99.29%, Valid: 78.40% Test: 82.50%
Split: 01, Run: 03, Epoch: 260, Loss: 0.2644, Train: 99.29%, Valid: 79.00% Test: 82.10%
Split: 01, Run: 03, Epoch: 280, Loss: 0.2515, Train: 100.00%, Valid: 79.60% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 320, Loss: 0.2594, Train: 99.29%, Valid: 78.80% Test: 82.70%
Split: 01, Run: 03, Epoch: 340, Loss: 0.2357, Train: 100.00%, Valid: 80.20% Test: 83.10%
Split: 01, Run: 03, Epoch: 360, Loss: 0.2581, Train: 100.00%, Valid: 80.00% Test: 82.20%
Split: 01, Run: 03, Epoch: 380, Loss: 0.2576, Train: 99.29%, Valid: 79.20% Test: 82.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 420, Loss: 0.2108, Train: 100.00%, Valid: 79.80% Test: 81.70%
Split: 01, Run: 03, Epoch: 440, Loss: 0.2262, Train: 100.00%, Valid: 79.20% Test: 82.60%
Split: 01, Run: 03, Epoch: 460, Loss: 0.2405, Train: 99.29%, Valid: 79.60% Test: 83.40%
Split: 01, Run: 03, Epoch: 480, Loss: 0.2113, Train: 99.29%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  14.252369972877204
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  45.45376476459205
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-19 20:49:27,190][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  45.46726776845753
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 20:49:51,235][0m A new study created in memory with name: no-name-eb3580ed-4ca3-4e86-8c99-480caf51605a[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 156144.7031, Train: 100.00%, Valid: 46.40% Test: 50.70%
Split: 01, Run: 01, Epoch: 40, Loss: 150508.0938, Train: 100.00%, Valid: 49.40% Test: 53.60%
Split: 01, Run: 01, Epoch: 60, Loss: 144389.4062, Train: 100.00%, Valid: 60.00% Test: 60.90%
Split: 01, Run: 01, Epoch: 80, Loss: 138705.2344, Train: 100.00%, Valid: 62.20% Test: 65.40%
Split: 01, Run: 01, Epoch: 100, Loss: 134513.5938, Train: 100.00%, Valid: 64.80% Test: 66.70%
Split: 01, Run: 01, Epoch: 120, Loss: 128769.3828, Train: 100.00%, Valid: 66.40% Test: 65.80%
Split: 01, Run: 01, Epoch: 140, Loss: 122258.1094, Train: 100.00%, Valid: 64.60% Test: 66.90%
Split: 01, Run: 01, Epoch: 160, Loss: 117182.4844, Train: 100.00%, Valid: 64.20% Test: 67.40%
Split: 01, Run: 01, Epoch: 180, Loss: 115108.5156, Train: 100.00%, Valid: 65.40% Test: 63.50%
Split: 01, Run: 01, Epoch: 200, Loss: 108335.1172, Train: 100.00%, Valid: 64.60% Test: 63.80%
Split: 01, Run: 01, Epoch: 220, Loss: 108358.1016, Train: 100.00%, Valid: 62.60% Test: 64.70%
Split: 01, Run: 01, Epoch: 240, Loss: 102908.6953, Train: 100.00%, Valid: 60.80% Test: 63.40%
Split: 01, Run: 01, Epoch: 260, Loss: 102079.8438, Train: 100.00%, Valid: 60.40% Test: 63.30%
Split: 01, Run: 01, Epoch: 280, Loss: 99129.4922, Train: 100.00%, Valid: 60.80% Test: 63.10%
Split: 01, Run: 01, Epoch: 300, Loss: 98190.8047, Train: 100.00%, Valid: 60.00% Test: 62.70%
Split: 01, Run: 01, Epoch: 320, Loss: 97039.3438, Train: 100.00%, Valid: 60.80% Test: 61.90%
Split: 01, Run: 01, Epoch: 340, Loss: 88754.7734, Train: 100.00%, Valid: 58.80% Test: 61.10%
Split: 01, Run: 01, Epoch: 360, Loss: 88322.1484, Train: 100.00%, Valid: 61.00% Test: 61.00%
Split: 01, Run: 01, Epoch: 380, Loss: 86902.5156, Train: 100.00%, Valid: 58.60% Test: 61.00%
Split: 01, Run: 01, Epoch: 400, Loss: 87561.4688, Train: 100.00%, Valid: 56.80% Test: 57.30%
Split: 01, Run: 01, Epoch: 420, Loss: 84822.1875, Train: 100.00%, Valid: 59.60% Test: 60.40%
Split: 01, Run: 01, Epoch: 440, Loss: 82102.8672, Train: 100.00%, Valid: 56.60% Test: 59.30%
Split: 01, Run: 01, Epoch: 460, Loss: 83566.3906, Train: 100.00%, Valid: 55.00% Test: 56.30%
Split: 01, Run: 01, Epoch: 480, Loss: 79887.3281, Train: 100.00%, Valid: 51.40% Test: 57.00%
Split: 01, Run: 01, Epoch: 500, Loss: 79834.1016, Train: 100.00%, Valid: 57.80% Test: 58.40%
Split: 01, Run: 01
None time:  8.246712328866124
None Run 01:
Highest Train: 100.00
Highest Valid: 69.20
  Final Train: 100.00
   Final Test: 64.90
Split: 01, Run: 02, Epoch: 20, Loss: 156123.5781, Train: 100.00%, Valid: 54.80% Test: 57.80%
Split: 01, Run: 02, Epoch: 40, Loss: 150482.5000, Train: 100.00%, Valid: 57.40% Test: 64.40%
Split: 01, Run: 02, Epoch: 60, Loss: 143374.1562, Train: 100.00%, Valid: 62.80% Test: 62.40%
Split: 01, Run: 02, Epoch: 80, Loss: 135925.4375, Train: 100.00%, Valid: 60.00% Test: 62.00%
Split: 01, Run: 02, Epoch: 100, Loss: 133422.5625, Train: 100.00%, Valid: 61.20% Test: 62.60%
Split: 01, Run: 02, Epoch: 120, Loss: 128976.0469, Train: 100.00%, Valid: 64.80% Test: 66.60%
Split: 01, Run: 02, Epoch: 140, Loss: 122871.3672, Train: 100.00%, Valid: 62.80% Test: 64.80%
Split: 01, Run: 02, Epoch: 160, Loss: 115995.7031, Train: 100.00%, Valid: 62.40% Test: 66.70%
Split: 01, Run: 02, Epoch: 180, Loss: 116116.1797, Train: 100.00%, Valid: 60.40% Test: 63.60%
Split: 01, Run: 02, Epoch: 200, Loss: 108516.6641, Train: 100.00%, Valid: 62.20% Test: 63.90%
Split: 01, Run: 02, Epoch: 220, Loss: 100353.3203, Train: 100.00%, Valid: 62.20% Test: 65.10%
Split: 01, Run: 02, Epoch: 240, Loss: 103144.0234, Train: 100.00%, Valid: 60.40% Test: 64.00%
Split: 01, Run: 02, Epoch: 260, Loss: 102864.6875, Train: 100.00%, Valid: 62.80% Test: 65.50%
Split: 01, Run: 02, Epoch: 280, Loss: 97315.7812, Train: 100.00%, Valid: 66.20% Test: 67.30%
Split: 01, Run: 02, Epoch: 300, Loss: 93828.6719, Train: 100.00%, Valid: 64.20% Test: 65.40%
Split: 01, Run: 02, Epoch: 320, Loss: 90821.2656, Train: 100.00%, Valid: 61.40% Test: 62.80%
Split: 01, Run: 02, Epoch: 340, Loss: 92491.4922, Train: 100.00%, Valid: 65.20% Test: 66.80%
Split: 01, Run: 02, Epoch: 360, Loss: 87993.6875, Train: 100.00%, Valid: 65.00% Test: 65.40%
Split: 01, Run: 02, Epoch: 380, Loss: 82697.2266, Train: 100.00%, Valid: 63.00% Test: 68.50%
Split: 01, Run: 02, Epoch: 400, Loss: 87513.5391, Train: 100.00%, Valid: 65.40% Test: 65.50%
Split: 01, Run: 02, Epoch: 420, Loss: 85634.6953, Train: 100.00%, Valid: 62.40% Test: 66.90%
Split: 01, Run: 02, Epoch: 440, Loss: 80062.3828, Train: 100.00%, Valid: 62.80% Test: 63.20%
Split: 01, Run: 02, Epoch: 460, Loss: 83186.9688, Train: 100.00%, Valid: 61.00% Test: 61.80%
Split: 01, Run: 02, Epoch: 480, Loss: 78745.2656, Train: 100.00%, Valid: 61.20% Test: 62.70%
Split: 01, Run: 02, Epoch: 500, Loss: 79706.6016, Train: 100.00%, Valid: 58.80% Test: 60.30%
Split: 01, Run: 02
None time:  7.9632988246157765
None Run 02:
Highest Train: 100.00
Highest Valid: 68.60
  Final Train: 100.00
   Final Test: 64.70
Split: 01, Run: 03, Epoch: 20, Loss: 155369.4531, Train: 100.00%, Valid: 54.20% Test: 57.40%
Split: 01, Run: 03, Epoch: 40, Loss: 150671.9531, Train: 100.00%, Valid: 52.80% Test: 57.40%
Split: 01, Run: 03, Epoch: 60, Loss: 141349.9844, Train: 100.00%, Valid: 61.60% Test: 65.00%
Split: 01, Run: 03, Epoch: 80, Loss: 138118.2969, Train: 100.00%, Valid: 64.40% Test: 63.30%
Split: 01, Run: 03, Epoch: 100, Loss: 133782.1719, Train: 100.00%, Valid: 62.60% Test: 64.70%
Split: 01, Run: 03, Epoch: 120, Loss: 130002.4141, Train: 100.00%, Valid: 62.40% Test: 66.90%
Split: 01, Run: 03, Epoch: 140, Loss: 121908.1719, Train: 100.00%, Valid: 64.00% Test: 66.30%
Split: 01, Run: 03, Epoch: 160, Loss: 120431.6406, Train: 100.00%, Valid: 62.00% Test: 65.60%
Split: 01, Run: 03, Epoch: 180, Loss: 116510.8047, Train: 100.00%, Valid: 62.20% Test: 65.10%
Split: 01, Run: 03, Epoch: 200, Loss: 113107.0000, Train: 100.00%, Valid: 60.40% Test: 66.10%
Split: 01, Run: 03, Epoch: 220, Loss: 107664.8125, Train: 100.00%, Valid: 65.80% Test: 66.30%
Split: 01, Run: 03, Epoch: 240, Loss: 108243.3281, Train: 100.00%, Valid: 62.80% Test: 64.60%
Split: 01, Run: 03, Epoch: 260, Loss: 96743.0781, Train: 100.00%, Valid: 63.40% Test: 65.20%
Split: 01, Run: 03, Epoch: 280, Loss: 97833.2578, Train: 100.00%, Valid: 61.00% Test: 64.20%
Split: 01, Run: 03, Epoch: 300, Loss: 104666.7812, Train: 100.00%, Valid: 64.20% Test: 65.40%
Split: 01, Run: 03, Epoch: 320, Loss: 99218.3281, Train: 100.00%, Valid: 64.60% Test: 63.90%
Split: 01, Run: 03, Epoch: 340, Loss: 92328.4766, Train: 100.00%, Valid: 59.60% Test: 62.20%
Split: 01, Run: 03, Epoch: 360, Loss: 91583.5000, Train: 100.00%, Valid: 61.60% Test: 63.70%
Split: 01, Run: 03, Epoch: 380, Loss: 88524.8594, Train: 100.00%, Valid: 59.80% Test: 61.90%
Split: 01, Run: 03, Epoch: 400, Loss: 87902.4062, Train: 100.00%, Valid: 60.20% Test: 58.70%
Split: 01, Run: 03, Epoch: 420, Loss: 86251.5078, Train: 100.00%, Valid: 56.60% Test: 59.70%
Split: 01, Run: 03, Epoch: 440, Loss: 87985.0859, Train: 100.00%, Valid: 56.20% Test: 59.10%
Split: 01, Run: 03, Epoch: 460, Loss: 82414.0625, Train: 100.00%, Valid: 57.60% Test: 56.40%
Split: 01, Run: 03, Epoch: 480, Loss: 87889.1797, Train: 100.00%, Valid: 55.00% Test: 58.40%
Split: 01, Run: 03, Epoch: 500, Loss: 85472.1641, Train: 100.00%, Valid: 55.20% Test: 57.30%
Split: 01, Run: 03
None time:  7.734061422757804
None Run 03:
Highest Train: 100.00
Highest Valid: 68.00
  Final Train: 100.00
   Final Test: 66.70
total time:  25.697028482332826
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 68.60 Â± 0.60
  Final Train: 100.00 Â± 0.00
   Final Test: 65.43 Â± 1.10
[32m[I 2021-07-19 20:50:16,948][0m Trial 0 finished with value: 68.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 10}. Best is trial 0 with value: 68.5999984741211.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 10}   trial.value:  68.6    {'train': '100.00 Â± 0.00', 'valid': '68.60 Â± 0.60', 'test': '65.43 Â± 1.10'}
test_acc
['65.43 Â± 1.10']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 100.0, 'K': 10}
Best trial Value:  68.5999984741211
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '68.60 Â± 0.60', 'test': '65.43 Â± 1.10'}
optuna total time:  25.717633990570903
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-19 20:55:22,840][0m A new study created in memory with name: no-name-9dbdc325-ea88-467e-b559-5d743e72ba02[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
[33m[W 2021-07-19 20:55:24,771][0m Trial 0 failed because of the following error: AttributeError("'NoneType' object has no attribute 'argmax'")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 41, in test_altopt
    y_pred = out.argmax(dim=-1, keepdim=True)
AttributeError: 'NoneType' object has no attribute 'argmax'[0m
Traceback (most recent call last):
  File "main_optuna.py", line 325, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 41, in test_altopt
    y_pred = out.argmax(dim=-1, keepdim=True)
AttributeError: 'NoneType' object has no attribute 'argmax'
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:00:41,288][0m A new study created in memory with name: no-name-ae7a05e9-7be0-46f2-961f-dcef74007f9b[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
[33m[W 2021-07-20 00:00:48,375][0m Trial 0 failed because of the following error: IndentationError('expected an indented block', ('/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py', 81, 16, "            self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')\n"))
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 19, in get_model
    from model_ALTOPT import ALTOPT
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 81
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
       ^
IndentationError: expected an indented block[0m
Traceback (most recent call last):
  File "main_optuna.py", line 325, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 111, in objective
    model = get_model(args, dataset)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/get_model.py", line 19, in get_model
    from model_ALTOPT import ALTOPT
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 81
    self.FF = self.prop(x=self.mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
       ^
IndentationError: expected an indented block
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:02:59,059][0m A new study created in memory with name: no-name-2c356a56-196f-447f-9569-695ad7b450b9[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py[0m(102)[0;36minit_label[0;34m()[0m
[0;32m    101 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 102 [0;31m        [0mlabel[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mzeros[0m[0;34m([0m[0mnodes[0m[0;34m,[0m [0mclasses[0m[0;34m)[0m[0;34m.[0m[0mcuda[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    103 [0;31m        [0mlabel[0m[0;34m[[0m[0mmask[0m[0;34m,[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m[[0m[0mmask[0m[0;34m][0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
ipdb> 2708
ipdb> 2708
ipdb> tensor([3, 4, 4,  ..., 3, 3, 3], device='cuda:0')
ipdb> tensor(6, device='cuda:0')
ipdb> ipdb> ipdb> tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
ipdb> torch.Size([2708, 7])
ipdb> torch.Size([2708, 7])
ipdb> *** NameError: name 'label' is not defined
ipdb> [33m[W 2021-07-20 00:05:01,516][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 76, in propagate_update
    self.FF = self.prop.init_label(data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 102, in init_label
    classes = data.y.max() + 1
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 102, in init_label
    classes = data.y.max() + 1
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:05:19,266][0m A new study created in memory with name: no-name-772ecf53-cbfd-4f1a-9b47-ddf8e648d225[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:06:59,223][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:07:10,911][0m A new study created in memory with name: no-name-5327930c-cce9-4f1e-91cd-9d97d1f324e0[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-20 00:07:12,631][0m Trial 0 failed because of the following error: NameError("name 'FF' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 81, in propagate_update
    zero_mlp = torch.zeros_like(FF)
NameError: name 'FF' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 325, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 81, in propagate_update
    zero_mlp = torch.zeros_like(FF)
NameError: name 'FF' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:07:28,062][0m A new study created in memory with name: no-name-4cd208b7-2ac6-4987-ae21-23105ab931e6[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
[33m[W 2021-07-20 00:07:29,916][0m Trial 0 failed because of the following error: NameError("name 'mask' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in propagate_update
    self.FF = self.prop(x=zero_mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 116, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
NameError: name 'mask' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 325, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 124, in objective
    model.propagate_update(data, K=args.K)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 82, in propagate_update
    self.FF = self.prop(x=zero_mlp, edge_index=data.adj_t, data=data, FF=self.FF, mode='ALTOPT')
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 92, in forward
    x = self.apt_forward(mlp=x, FF=FF, edge_index=edge_index, K=self.K, alpha=self.alpha, data=data)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/prop.py", line 116, in apt_forward
    FF[mask] = 1/(lambda1+lambda2+1) * AF[mask] + lambda1/(lambda1+lambda2+1) * mlp[mask] + lambda2/(lambda1+lambda1+1) * label[mask]  ## for labeled nodes
NameError: name 'mask' is not defined
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [1.0], 'lambda2': [100.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:08:21,153][0m A new study created in memory with name: no-name-c0ed5fa7-4124-488e-8123-aec9d2f503f6[0m
lambda1:  1.0
lambda2:  100.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=100.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.713
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:08:35,125][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:09:17,131][0m A new study created in memory with name: no-name-d3b0efdb-1fb0-40fc-af81-ff58d3b8b807[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 20, Loss: 1398.6722, Train: 100.00%, Valid: 75.00% Test: 76.10%
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 40, Loss: 1168.8636, Train: 100.00%, Valid: 75.00% Test: 76.10%
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** ValueError: source code string cannot contain null bytes
ipdb> [33m[W 2021-07-20 00:17:17,062][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 127, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 127, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:17:48,039][0m A new study created in memory with name: no-name-ffc552ee-b710-41ea-9dc6-fda48acf76ac[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:18:15,766][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:19:49,201][0m A new study created in memory with name: no-name-9149c6fb-c50b-4e50-8f03-1a8c92cae10f[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:19:58,405][0m Trial 0 failed because of the following error: NameError("name 'labbel' is not defined")
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 26, in train_altopt
    diff = out - labbel
NameError: name 'labbel' is not defined[0m
Traceback (most recent call last):
  File "main_optuna.py", line 325, in <module>
    study.optimize(objective, n_trials=num_trial)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/study.py", line 315, in optimize
    show_progress_bar=show_progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 65, in _optimize
    progress_bar=progress_bar,
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 156, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 26, in train_altopt
    diff = out - labbel
NameError: name 'labbel' is not defined

If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:20:30,036][0m A new study created in memory with name: no-name-f20065aa-2fdc-4a7c-b4ce-c9e3c35d89b6[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(127)[0;36mobjective[0;34m()[0m
[0;32m    126 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:26:26,684][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:26:56,381][0m A new study created in memory with name: no-name-dd6d3710-42d7-44db-a85a-33f2498b520a[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(128)[0;36mobjective[0;34m()[0m
[0;32m    127 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m                    [0mloss[0m [0;34m=[0m [0mtrain_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0mtrain_idx[0m[0;34m,[0m [0moptimizer[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> [33m[W 2021-07-20 00:27:54,699][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "main_optuna.py", line 128, in objective
    loss = train_altopt(model, data, train_idx, optimizer, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:28:02,184][0m A new study created in memory with name: no-name-e36104d8-04a5-4ed4-9cb2-f7aa81a68f1b[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 1., 0., 0.],
        [0., 0., 0.,  ..., 1., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
ipdb> tensor([[0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> *** NameError: name 'train_mask' is not defined
ipdb> tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
ipdb> tensor(0., device='cuda:0')
ipdb> tensor([[0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> torch.Size([140, 7])
ipdb> tensor(140., device='cuda:0')
ipdb> tensor(140., device='cuda:0')
ipdb> tensor(140., device='cuda:0')
ipdb> [33m[W 2021-07-20 00:29:43,695][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 55, in test_altopt
    if len(data.y.shape) == 1:
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 55, in test_altopt
    if len(data.y.shape) == 1:
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:30:26,478][0m A new study created in memory with name: no-name-5230fcd9-b832-4dc1-a085-060d18f6cad8[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> tensor([[3],
        [4],
        [4],
        ...,
        [3],
        [3],
        [3]], device='cuda:0')
ipdb> torch.Size([2708, 1])
ipdb> tensor([[3],
        [4],
        [4],
        [0],
        [3],
        [2],
        [0],
        [3],
        [3],
        [2],
        [0],
        [0],
        [4],
        [3],
        [3],
        [3],
        [2],
        [3],
        [1],
        [3],
        [5],
        [3],
        [4],
        [6],
        [3],
        [3],
        [6],
        [3],
        [2],
        [4],
        [3],
        [6],
        [0],
        [4],
        [2],
        [0],
        [1],
        [5],
        [4],
        [4],
        [3],
        [6],
        [6],
        [4],
        [3],
        [3],
        [2],
        [5],
        [3],
        [4],
        [5],
        [3],
        [0],
        [2],
        [1],
        [4],
        [6],
        [3],
        [2],
        [2],
        [0],
        [0],
        [0],
        [4],
        [2],
        [0],
        [4],
        [5],
        [2],
        [6],
        [5],
        [2],
        [2],
        [2],
        [0],
        [4],
        [5],
        [6],
        [4],
        [0],
        [0],
        [0],
        [4],
        [2],
        [4],
        [1],
        [4],
        [6],
        [0],
        [4],
        [2],
        [4],
        [6],
        [6],
        [0],
        [0],
        [6],
        [5],
        [0],
        [6],
        [0],
        [2],
        [1],
        [1],
        [1],
        [2],
        [6],
        [5],
        [6],
        [1],
        [2],
        [2],
        [1],
        [5],
        [5],
        [5],
        [6],
        [5],
        [6],
        [5],
        [5],
        [1],
        [6],
        [6],
        [1],
        [5],
        [1],
        [6],
        [5],
        [5],
        [5],
        [1],
        [5],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
ipdb> *** SyntaxError: invalid syntax
ipdb> tensor([[0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> tensor([[0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')
ipdb> *** AttributeError: 'Data' object has no attribute 'train_mas'
ipdb> tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')
ipdb> tensor([[3],
        [3],
        [3],
        ...,
        [3],
        [3],
        [3]], device='cuda:0')
ipdb> torch.Size([2568, 1])
ipdb> tensor(7704, device='cuda:0')
ipdb> tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
ipdb> torch.Size([2568, 7])
ipdb> tensor([0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
ipdb> tensor([0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
ipdb> ipdb> tensor(3, device='cuda:0')
ipdb> [33m[W 2021-07-20 00:33:11,998][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 126, in objective
    result = test_altopt(model, data, split_idx, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 55, in test_altopt
    if len(data.y.shape) == 1:
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 55, in test_altopt
    if len(data.y.shape) == 1:
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:34:53,063][0m A new study created in memory with name: no-name-7f11c5df-c3d6-4f86-ae22-046acd01c2f3[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                        [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                        [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                        [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                        [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py[0m(55)[0;36mtest_altopt[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mif[0m [0mlen[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0mshape[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        [0my[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0my[0m[0;34m.[0m[0munsqueeze[0m[0;34m([0m[0mdim[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# for non ogb datas[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:35:22,493][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:35:49,532][0m A new study created in memory with name: no-name-926256d8-fb14-461c-812f-9b25b3b9785d[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:35:55,080][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:36:02,081][0m A new study created in memory with name: no-name-00141c45-192a-4acf-83a9-cb340ee300a1[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:37:42,835][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:39:26,709][0m A new study created in memory with name: no-name-63d33d97-654d-4bdd-beba-3b2d6525f756[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 5 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 6 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 7 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 8 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 9 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 10 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 11 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 12 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 13 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 14 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'self' is not defined
ipdb> ipdb> ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
ipdb> [33m[W 2021-07-20 00:40:25,351][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:40:32,516][0m A new study created in memory with name: no-name-9e578d38-8e3e-42c1-9ec5-42b52b8499cd[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:41:15,446][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "main_optuna.py", line 137, in objective
    logger.add_result(runs_overall, result)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:41:22,723][0m A new study created in memory with name: no-name-612788b1-ac5b-407e-9cab-a96ff5ae960d[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(93)[0;36mforward[0;34m()[0m
[0;32m     92 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 93 [0;31m        [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mtraining[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     94 [0;31m            [0;31m## there is no dropout in test[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(93)[0;36mforward[0;34m()[0m
[0;32m     92 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 93 [0;31m        [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mtraining[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     94 [0;31m            [0;31m## there is no dropout in test[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 2 test pseudo label FF
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(93)[0;36mforward[0;34m()[0m
[0;32m     92 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 93 [0;31m        [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mtraining[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     94 [0;31m            [0;31m## there is no dropout in test[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py[0m(93)[0;36mforward[0;34m()[0m
[0;32m     92 [0;31m        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 93 [0;31m        [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mtraining[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     94 [0;31m            [0;31m## there is no dropout in test[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> [33m[W 2021-07-20 00:42:24,331][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 129, in objective
    result = test_altopt(model, data, split_idx, args=args)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py", line 49, in test_altopt
    out = model(data=data)  ## still forward to update mlp output, or move it to propagate_update
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 93, in forward
    if not self.training:
  File "/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/model_ALTOPT.py", line 93, in forward
    if not self.training:
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-20 00:42:31,948][0m A new study created in memory with name: no-name-52dfac5e-15f4-4d90-a10b-bf78457e6bb6[0m
lambda1:  0.0
lambda2:  1.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.73
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 5 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 6 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 7 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 8 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 9 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 10 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 11 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 12 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 13 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 14 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:42:52,711][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:43:07,201][0m A new study created in memory with name: no-name-4bce4a50-8484-4df6-bbb0-c8fddfda71af[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 5 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 6 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> epoch 7 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> epoch 8 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:43:27,940][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:44:01,726][0m A new study created in memory with name: no-name-d70b4bdf-45b3-4c75-b4d0-b99d80e6138e[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> with mlp
epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> [33m[W 2021-07-20 00:44:44,435][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:44:51,469][0m A new study created in memory with name: no-name-1e55799d-cdbc-40bb-8e4e-15fb369fd86e[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
train_acc: 1.0, valid_acc: 0.316, test_acc: 0.319
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> with mlp
epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:45:26,318][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:45:33,391][0m A new study created in memory with name: no-name-d5f41752-2b73-4d63-8686-2d8ab3f77ef2[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> with mlp
epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 5 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 6 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 7 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 8 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:46:00,578][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:46:08,139][0m A new study created in memory with name: no-name-25cf8d2a-bd4e-4651-829b-614241251d9c[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
epoch 1 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 2 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 3 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 4 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 5 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> with mlp
epoch 6 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 7 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 8 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:46:17,671][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 130, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:47:15,817][0m A new study created in memory with name: no-name-e51f516c-5b72-4397-b813-0f778343dbdf[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
epoch 1 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 2 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 3 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 4 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 5 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 6 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 7 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> ipdb> with mlp
epoch 8 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 9 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 10 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 11 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.103
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 12 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.103
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.104
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 13 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.104
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.105
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 14 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.105
train_acc: 0.25, valid_acc: 0.19, test_acc: 0.191
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 15 test pseudo label FF
train_acc: 0.25, valid_acc: 0.19, test_acc: 0.191
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 16 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 17 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
train_acc: 0.2, valid_acc: 0.132, test_acc: 0.14
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 18 test pseudo label FF
train_acc: 0.2, valid_acc: 0.132, test_acc: 0.14
train_acc: 0.4857142857142857, valid_acc: 0.45, test_acc: 0.466
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 19 test pseudo label FF
train_acc: 0.4857142857142857, valid_acc: 0.45, test_acc: 0.466
train_acc: 0.37142857142857144, valid_acc: 0.458, test_acc: 0.454
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 20 test pseudo label FF
train_acc: 0.37142857142857144, valid_acc: 0.458, test_acc: 0.454
train_acc: 0.39285714285714285, valid_acc: 0.492, test_acc: 0.49
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 20, Loss: 1887.6927, Train: 39.29%, Valid: 49.20% Test: 49.00%
with mlp
epoch 21 test pseudo label FF
train_acc: 0.39285714285714285, valid_acc: 0.492, test_acc: 0.49
train_acc: 0.40714285714285714, valid_acc: 0.498, test_acc: 0.493
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 22 test pseudo label FF
train_acc: 0.40714285714285714, valid_acc: 0.498, test_acc: 0.493
train_acc: 0.42142857142857143, valid_acc: 0.486, test_acc: 0.478
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 23 test pseudo label FF
train_acc: 0.42142857142857143, valid_acc: 0.486, test_acc: 0.478
train_acc: 0.42142857142857143, valid_acc: 0.484, test_acc: 0.468
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 24 test pseudo label FF
train_acc: 0.42142857142857143, valid_acc: 0.484, test_acc: 0.468
train_acc: 0.45714285714285713, valid_acc: 0.502, test_acc: 0.478
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 25 test pseudo label FF
train_acc: 0.45714285714285713, valid_acc: 0.502, test_acc: 0.478
train_acc: 0.5928571428571429, valid_acc: 0.562, test_acc: 0.532
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(137)[0;36mobjective[0;34m()[0m
[0;32m    136 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 26 test pseudo label FF
train_acc: 0.5928571428571429, valid_acc: 0.562, test_acc: 0.532
train_acc: 0.6785714285714286, valid_acc: 0.594, test_acc: 0.574
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(130)[0;36mobjective[0;34m()[0m
[0;32m    129 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest_altopt[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m,[0m [0margs[0m[0;34m=[0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m                    [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> [33m[W 2021-07-20 00:48:13,986][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 130, in objective
    if epoch % 10 == 0:
  File "main_optuna.py", line 130, in objective
    if epoch % 10 == 0:
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:48:21,600][0m A new study created in memory with name: no-name-fbb80fbb-fa3b-4f02-8409-e53a37a6b524[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
epoch 1 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 2 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 3 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 4 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 5 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 6 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 7 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 8 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 9 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
with mlp
epoch 10 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 11 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.102
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.103
with mlp
epoch 12 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.103
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.104
with mlp
epoch 13 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.104
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.105
with mlp
epoch 14 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.114, test_acc: 0.105
train_acc: 0.25, valid_acc: 0.19, test_acc: 0.191
with mlp
epoch 15 test pseudo label FF
train_acc: 0.25, valid_acc: 0.19, test_acc: 0.191
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
with mlp
epoch 16 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
with mlp
epoch 17 test pseudo label FF
train_acc: 0.14285714285714285, valid_acc: 0.122, test_acc: 0.13
train_acc: 0.2, valid_acc: 0.132, test_acc: 0.14
with mlp
epoch 18 test pseudo label FF
train_acc: 0.2, valid_acc: 0.132, test_acc: 0.14
train_acc: 0.4857142857142857, valid_acc: 0.45, test_acc: 0.466
with mlp
epoch 19 test pseudo label FF
train_acc: 0.4857142857142857, valid_acc: 0.45, test_acc: 0.466
train_acc: 0.37142857142857144, valid_acc: 0.458, test_acc: 0.454
with mlp
epoch 20 test pseudo label FF
train_acc: 0.37142857142857144, valid_acc: 0.458, test_acc: 0.454
train_acc: 0.39285714285714285, valid_acc: 0.492, test_acc: 0.49
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 20, Loss: 1887.6927, Train: 39.29%, Valid: 49.20% Test: 49.00%
with mlp
epoch 21 test pseudo label FF
train_acc: 0.39285714285714285, valid_acc: 0.492, test_acc: 0.49
train_acc: 0.40714285714285714, valid_acc: 0.498, test_acc: 0.493
with mlp
epoch 22 test pseudo label FF
train_acc: 0.40714285714285714, valid_acc: 0.498, test_acc: 0.493
train_acc: 0.42142857142857143, valid_acc: 0.486, test_acc: 0.478
with mlp
epoch 23 test pseudo label FF
train_acc: 0.42142857142857143, valid_acc: 0.486, test_acc: 0.478
train_acc: 0.42142857142857143, valid_acc: 0.484, test_acc: 0.468
with mlp
epoch 24 test pseudo label FF
train_acc: 0.42142857142857143, valid_acc: 0.484, test_acc: 0.468
train_acc: 0.45714285714285713, valid_acc: 0.502, test_acc: 0.478
with mlp
epoch 25 test pseudo label FF
train_acc: 0.45714285714285713, valid_acc: 0.502, test_acc: 0.478
train_acc: 0.5928571428571429, valid_acc: 0.562, test_acc: 0.532
with mlp
epoch 26 test pseudo label FF
train_acc: 0.5928571428571429, valid_acc: 0.562, test_acc: 0.532
train_acc: 0.6785714285714286, valid_acc: 0.594, test_acc: 0.574
with mlp
epoch 27 test pseudo label FF
train_acc: 0.6785714285714286, valid_acc: 0.594, test_acc: 0.574
train_acc: 0.7428571428571429, valid_acc: 0.606, test_acc: 0.596
with mlp
epoch 28 test pseudo label FF
train_acc: 0.7428571428571429, valid_acc: 0.606, test_acc: 0.596
train_acc: 0.7571428571428571, valid_acc: 0.586, test_acc: 0.575
with mlp
epoch 29 test pseudo label FF
train_acc: 0.7571428571428571, valid_acc: 0.586, test_acc: 0.575
train_acc: 0.7285714285714285, valid_acc: 0.542, test_acc: 0.549
with mlp
epoch 30 test pseudo label FF
train_acc: 0.7285714285714285, valid_acc: 0.542, test_acc: 0.549
train_acc: 0.7357142857142858, valid_acc: 0.522, test_acc: 0.526
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 31 test pseudo label FF
train_acc: 0.7357142857142858, valid_acc: 0.522, test_acc: 0.526
train_acc: 0.7285714285714285, valid_acc: 0.518, test_acc: 0.517
with mlp
epoch 32 test pseudo label FF
train_acc: 0.7285714285714285, valid_acc: 0.518, test_acc: 0.517
train_acc: 0.7142857142857143, valid_acc: 0.524, test_acc: 0.544
with mlp
epoch 33 test pseudo label FF
train_acc: 0.7142857142857143, valid_acc: 0.524, test_acc: 0.544
train_acc: 0.75, valid_acc: 0.574, test_acc: 0.576
with mlp
epoch 34 test pseudo label FF
train_acc: 0.75, valid_acc: 0.574, test_acc: 0.576
train_acc: 0.7857142857142857, valid_acc: 0.63, test_acc: 0.627
with mlp
epoch 35 test pseudo label FF
train_acc: 0.7857142857142857, valid_acc: 0.63, test_acc: 0.627
train_acc: 0.8357142857142857, valid_acc: 0.664, test_acc: 0.673
with mlp
epoch 36 test pseudo label FF
train_acc: 0.8357142857142857, valid_acc: 0.664, test_acc: 0.673
train_acc: 0.85, valid_acc: 0.68, test_acc: 0.683
with mlp
epoch 37 test pseudo label FF
train_acc: 0.85, valid_acc: 0.68, test_acc: 0.683
train_acc: 0.8428571428571429, valid_acc: 0.694, test_acc: 0.675
with mlp
epoch 38 test pseudo label FF
train_acc: 0.8428571428571429, valid_acc: 0.694, test_acc: 0.675
train_acc: 0.85, valid_acc: 0.696, test_acc: 0.674
with mlp
epoch 39 test pseudo label FF
train_acc: 0.85, valid_acc: 0.696, test_acc: 0.674
train_acc: 0.85, valid_acc: 0.704, test_acc: 0.681
with mlp
epoch 40 test pseudo label FF
train_acc: 0.85, valid_acc: 0.704, test_acc: 0.681
train_acc: 0.8571428571428571, valid_acc: 0.694, test_acc: 0.681
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 40, Loss: 1389.1696, Train: 85.71%, Valid: 69.40% Test: 68.10%
with mlp
epoch 41 test pseudo label FF
train_acc: 0.8571428571428571, valid_acc: 0.694, test_acc: 0.681
train_acc: 0.8714285714285714, valid_acc: 0.678, test_acc: 0.684
with mlp
epoch 42 test pseudo label FF
train_acc: 0.8714285714285714, valid_acc: 0.678, test_acc: 0.684
train_acc: 0.9, valid_acc: 0.67, test_acc: 0.68
with mlp
epoch 43 test pseudo label FF
train_acc: 0.9, valid_acc: 0.67, test_acc: 0.68
train_acc: 0.8928571428571429, valid_acc: 0.678, test_acc: 0.676
with mlp
epoch 44 test pseudo label FF
train_acc: 0.8928571428571429, valid_acc: 0.678, test_acc: 0.676
train_acc: 0.9071428571428571, valid_acc: 0.68, test_acc: 0.686
with mlp
epoch 45 test pseudo label FF
train_acc: 0.9071428571428571, valid_acc: 0.68, test_acc: 0.686
train_acc: 0.9142857142857143, valid_acc: 0.678, test_acc: 0.684
with mlp
epoch 46 test pseudo label FF
train_acc: 0.9142857142857143, valid_acc: 0.678, test_acc: 0.684
train_acc: 0.9214285714285714, valid_acc: 0.676, test_acc: 0.687
with mlp
epoch 47 test pseudo label FF
train_acc: 0.9214285714285714, valid_acc: 0.676, test_acc: 0.687
train_acc: 0.9142857142857143, valid_acc: 0.678, test_acc: 0.689
with mlp
epoch 48 test pseudo label FF
train_acc: 0.9142857142857143, valid_acc: 0.678, test_acc: 0.689
train_acc: 0.9142857142857143, valid_acc: 0.682, test_acc: 0.695
with mlp
epoch 49 test pseudo label FF
train_acc: 0.9142857142857143, valid_acc: 0.682, test_acc: 0.695
train_acc: 0.9357142857142857, valid_acc: 0.686, test_acc: 0.699
with mlp
epoch 50 test pseudo label FF
train_acc: 0.9357142857142857, valid_acc: 0.686, test_acc: 0.699
train_acc: 0.9428571428571428, valid_acc: 0.698, test_acc: 0.71
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 51 test pseudo label FF
train_acc: 0.9428571428571428, valid_acc: 0.698, test_acc: 0.71
train_acc: 0.9428571428571428, valid_acc: 0.712, test_acc: 0.717
with mlp
epoch 52 test pseudo label FF
train_acc: 0.9428571428571428, valid_acc: 0.712, test_acc: 0.717
train_acc: 0.9571428571428572, valid_acc: 0.71, test_acc: 0.722
with mlp
epoch 53 test pseudo label FF
train_acc: 0.9571428571428572, valid_acc: 0.71, test_acc: 0.722
train_acc: 0.9571428571428572, valid_acc: 0.71, test_acc: 0.726
with mlp
epoch 54 test pseudo label FF
train_acc: 0.9571428571428572, valid_acc: 0.71, test_acc: 0.726
train_acc: 0.9571428571428572, valid_acc: 0.714, test_acc: 0.729
with mlp
epoch 55 test pseudo label FF
train_acc: 0.9571428571428572, valid_acc: 0.714, test_acc: 0.729
train_acc: 0.9714285714285714, valid_acc: 0.714, test_acc: 0.732
with mlp
epoch 56 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.714, test_acc: 0.732
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.733
with mlp
epoch 57 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.733
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.731
with mlp
epoch 58 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.731
train_acc: 0.9714285714285714, valid_acc: 0.704, test_acc: 0.726
with mlp
epoch 59 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.704, test_acc: 0.726
train_acc: 0.9642857142857143, valid_acc: 0.696, test_acc: 0.725
with mlp
epoch 60 test pseudo label FF
train_acc: 0.9642857142857143, valid_acc: 0.696, test_acc: 0.725
train_acc: 0.9714285714285714, valid_acc: 0.7, test_acc: 0.726
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 60, Loss: 1181.9622, Train: 97.14%, Valid: 70.00% Test: 72.60%
with mlp
epoch 61 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.7, test_acc: 0.726
train_acc: 0.9714285714285714, valid_acc: 0.702, test_acc: 0.723
with mlp
epoch 62 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.702, test_acc: 0.723
train_acc: 0.9714285714285714, valid_acc: 0.702, test_acc: 0.724
with mlp
epoch 63 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.702, test_acc: 0.724
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.725
with mlp
epoch 64 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.71, test_acc: 0.725
train_acc: 0.9714285714285714, valid_acc: 0.718, test_acc: 0.724
with mlp
epoch 65 test pseudo label FF
train_acc: 0.9714285714285714, valid_acc: 0.718, test_acc: 0.724
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.728
with mlp
epoch 66 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.728
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.737
with mlp
epoch 67 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.737
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.742
with mlp
epoch 68 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.746
with mlp
epoch 69 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.746
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.745
with mlp
epoch 70 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.745
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 71 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.742
with mlp
epoch 72 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.744
with mlp
epoch 73 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
with mlp
epoch 74 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.741
with mlp
epoch 75 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.741
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.742
with mlp
epoch 76 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.743
with mlp
epoch 77 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.743
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.748
with mlp
epoch 78 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.748
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.751
with mlp
epoch 79 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.751
train_acc: 0.9857142857142858, valid_acc: 0.75, test_acc: 0.753
with mlp
epoch 80 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.75, test_acc: 0.753
train_acc: 0.9857142857142858, valid_acc: 0.754, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 80, Loss: 1075.5554, Train: 98.57%, Valid: 75.40% Test: 75.20%
with mlp
epoch 81 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.754, test_acc: 0.752
train_acc: 0.9857142857142858, valid_acc: 0.754, test_acc: 0.754
with mlp
epoch 82 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.754, test_acc: 0.754
train_acc: 0.9857142857142858, valid_acc: 0.75, test_acc: 0.754
with mlp
epoch 83 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.75, test_acc: 0.754
train_acc: 0.9857142857142858, valid_acc: 0.748, test_acc: 0.751
with mlp
epoch 84 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.748, test_acc: 0.751
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.749
with mlp
epoch 85 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.749
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.75
with mlp
epoch 86 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.75
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.747
with mlp
epoch 87 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.747
train_acc: 0.9857142857142858, valid_acc: 0.748, test_acc: 0.745
with mlp
epoch 88 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.748, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.743
with mlp
epoch 89 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.743
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.744
with mlp
epoch 90 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.745
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 91 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.742, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.745
with mlp
epoch 92 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.744
with mlp
epoch 93 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.743
with mlp
epoch 94 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.743
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.747
with mlp
epoch 95 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.747
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.748
with mlp
epoch 96 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.744, test_acc: 0.748
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.746
with mlp
epoch 97 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.746, test_acc: 0.746
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.746
with mlp
epoch 98 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.746
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.746
with mlp
epoch 99 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.746
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.745
with mlp
epoch 100 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.745
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 100, Loss: 999.0941, Train: 98.57%, Valid: 74.00% Test: 74.50%
with mlp
epoch 101 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
with mlp
epoch 102 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.742
with mlp
epoch 103 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.744
with mlp
epoch 104 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
with mlp
epoch 105 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
with mlp
epoch 106 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.741
with mlp
epoch 107 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.74, test_acc: 0.741
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.739
with mlp
epoch 108 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.739
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.734
with mlp
epoch 109 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.738, test_acc: 0.734
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.733
with mlp
epoch 110 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.736, test_acc: 0.733
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.731
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 111 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.731
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.734
with mlp
epoch 112 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.734
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.734
with mlp
epoch 113 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.734
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.735
with mlp
epoch 114 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.735
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.739
with mlp
epoch 115 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.739
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.742
with mlp
epoch 116 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.745
with mlp
epoch 117 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.744
with mlp
epoch 118 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.745
with mlp
epoch 119 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.749
with mlp
epoch 120 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.749
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.747
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 120, Loss: 957.9139, Train: 98.57%, Valid: 73.00% Test: 74.70%
with mlp
epoch 121 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.747
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.746
with mlp
epoch 122 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.746
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.745
with mlp
epoch 123 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.745
with mlp
epoch 124 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.745
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.744
with mlp
epoch 125 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.744
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.742
with mlp
epoch 126 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.742
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.743
with mlp
epoch 127 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.743
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.741
with mlp
epoch 128 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.741
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.738
with mlp
epoch 129 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.738
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.74
with mlp
epoch 130 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.724, test_acc: 0.74
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.741
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 131 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.726, test_acc: 0.741
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.741
with mlp
epoch 132 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.741
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.74
with mlp
epoch 133 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.728, test_acc: 0.74
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.74
with mlp
epoch 134 test pseudo label FF
train_acc: 0.9857142857142858, valid_acc: 0.73, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.73, test_acc: 0.736
with mlp
epoch 135 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.73, test_acc: 0.736
train_acc: 0.9928571428571429, valid_acc: 0.732, test_acc: 0.739
with mlp
epoch 136 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.732, test_acc: 0.739
train_acc: 0.9928571428571429, valid_acc: 0.732, test_acc: 0.741
with mlp
epoch 137 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.732, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.743
with mlp
epoch 138 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.745
with mlp
epoch 139 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.745
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.744
with mlp
epoch 140 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.744
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.742
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 140, Loss: 938.4604, Train: 99.29%, Valid: 73.60% Test: 74.20%
with mlp
epoch 141 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.742
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.739
with mlp
epoch 142 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.739
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.736
with mlp
epoch 143 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.736
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.738
with mlp
epoch 144 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.738
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.738
with mlp
epoch 145 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.734, test_acc: 0.738
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
with mlp
epoch 146 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.744
with mlp
epoch 147 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.744
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.74
with mlp
epoch 148 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.739
with mlp
epoch 149 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.739
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.737
with mlp
epoch 150 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.737
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.738
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 151 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.738
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
with mlp
epoch 152 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
with mlp
epoch 153 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.736, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.746
with mlp
epoch 154 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.746
with mlp
epoch 155 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.738, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.746
with mlp
epoch 156 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.746
with mlp
epoch 157 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.745
with mlp
epoch 158 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.745
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.747
with mlp
epoch 159 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.747
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.747
with mlp
epoch 160 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.747
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.745
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 160, Loss: 893.0950, Train: 99.29%, Valid: 74.00% Test: 74.50%
with mlp
epoch 161 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.745
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.741
with mlp
epoch 162 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.741
with mlp
epoch 163 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.738
with mlp
epoch 164 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.738
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.737
with mlp
epoch 165 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.74, test_acc: 0.737
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.739
with mlp
epoch 166 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.739
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.74
with mlp
epoch 167 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.74
with mlp
epoch 168 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.74
with mlp
epoch 169 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.74
with mlp
epoch 170 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.741
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 171 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.741
with mlp
epoch 172 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.739
with mlp
epoch 173 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.739
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.742
with mlp
epoch 174 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.742
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.743
with mlp
epoch 175 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.745
with mlp
epoch 176 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.742, test_acc: 0.745
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.746
with mlp
epoch 177 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.747
with mlp
epoch 178 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.747
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.747
with mlp
epoch 179 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.747
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.746
with mlp
epoch 180 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.746
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.743
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 180, Loss: 891.7512, Train: 99.29%, Valid: 74.80% Test: 74.30%
with mlp
epoch 181 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.741
with mlp
epoch 182 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.752, test_acc: 0.74
with mlp
epoch 183 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.752, test_acc: 0.74
train_acc: 0.9928571428571429, valid_acc: 0.752, test_acc: 0.741
with mlp
epoch 184 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.752, test_acc: 0.741
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.743
with mlp
epoch 185 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.744
with mlp
epoch 186 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.744
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.743
with mlp
epoch 187 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.75, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.742
with mlp
epoch 188 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.748, test_acc: 0.742
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.743
with mlp
epoch 189 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.746, test_acc: 0.743
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.743
with mlp
epoch 190 test pseudo label FF
train_acc: 0.9928571428571429, valid_acc: 0.744, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.742
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 191 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.742
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.743
with mlp
epoch 192 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
with mlp
epoch 193 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
with mlp
epoch 194 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.744
with mlp
epoch 195 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.744
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
with mlp
epoch 196 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.745
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.743
with mlp
epoch 197 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.742
with mlp
epoch 198 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.742
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.743
with mlp
epoch 199 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.743
with mlp
epoch 200 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.747
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 200, Loss: 839.3170, Train: 100.00%, Valid: 74.00% Test: 74.70%
with mlp
epoch 201 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.747
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.748
with mlp
epoch 202 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.748
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
with mlp
epoch 203 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
with mlp
epoch 204 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.753
with mlp
epoch 205 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
with mlp
epoch 206 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.751
with mlp
epoch 207 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.749
with mlp
epoch 208 test pseudo label FF
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.749
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.75
with mlp
epoch 209 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.75
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.748
with mlp
epoch 210 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.748
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.745
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 211 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.745
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.742
with mlp
epoch 212 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.742
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.738
with mlp
epoch 213 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.738
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.738
with mlp
epoch 214 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.738
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.734
with mlp
epoch 215 test pseudo label FF
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.734
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.737
with mlp
epoch 216 test pseudo label FF
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.737
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.743
with mlp
epoch 217 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.746
with mlp
epoch 218 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.746
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.747
with mlp
epoch 219 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.747
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.754
with mlp
epoch 220 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.754
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 220, Loss: 856.9798, Train: 100.00%, Valid: 74.80% Test: 75.40%
with mlp
epoch 221 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.756
with mlp
epoch 222 test pseudo label FF
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.756
with mlp
epoch 223 test pseudo label FF
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.758
with mlp
epoch 224 test pseudo label FF
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.754
with mlp
epoch 225 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
with mlp
epoch 226 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
with mlp
epoch 227 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
with mlp
epoch 228 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.75
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
with mlp
epoch 229 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.753
with mlp
epoch 230 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 231 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.753
with mlp
epoch 232 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.755
with mlp
epoch 233 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.757
with mlp
epoch 234 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.757
with mlp
epoch 235 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.758
with mlp
epoch 236 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.753
with mlp
epoch 237 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.754
with mlp
epoch 238 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
with mlp
epoch 239 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
with mlp
epoch 240 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 240, Loss: 839.2628, Train: 100.00%, Valid: 74.40% Test: 75.20%
with mlp
epoch 241 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
with mlp
epoch 242 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.754
with mlp
epoch 243 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.756
with mlp
epoch 244 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
with mlp
epoch 245 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.763
with mlp
epoch 246 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.762
with mlp
epoch 247 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.762
with mlp
epoch 248 test pseudo label FF
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
with mlp
epoch 249 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.76
with mlp
epoch 250 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.756
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 251 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
with mlp
epoch 252 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.752
with mlp
epoch 253 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.747
with mlp
epoch 254 test pseudo label FF
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.747
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.747
with mlp
epoch 255 test pseudo label FF
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.747
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.748
with mlp
epoch 256 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.748
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.749
with mlp
epoch 257 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.749
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.751
with mlp
epoch 258 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.751
with mlp
epoch 259 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.751
with mlp
epoch 260 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 260, Loss: 846.6912, Train: 100.00%, Valid: 74.60% Test: 75.20%
with mlp
epoch 261 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.752
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.753
with mlp
epoch 262 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.755
with mlp
epoch 263 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.756
with mlp
epoch 264 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.751
with mlp
epoch 265 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.751
with mlp
epoch 266 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
epoch 267 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.751
with mlp
epoch 268 test pseudo label FF
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
epoch 269 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.753
with mlp
epoch 270 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.753
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.754
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 271 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.754
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.755
with mlp
epoch 272 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
epoch 273 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.76
with mlp
epoch 274 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.761
with mlp
epoch 275 test pseudo label FF
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.76
with mlp
epoch 276 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.762
with mlp
epoch 277 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
epoch 278 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.756
with mlp
epoch 279 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.755
with mlp
epoch 280 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.755
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 280, Loss: 848.3898, Train: 100.00%, Valid: 74.40% Test: 75.50%
with mlp
epoch 281 test pseudo label FF
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.755
with mlp
epoch 282 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.755
with mlp
epoch 283 test pseudo label FF
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.758
with mlp
epoch 284 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
with mlp
epoch 285 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
with mlp
epoch 286 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
epoch 287 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
epoch 288 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
epoch 289 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.758
with mlp
epoch 290 test pseudo label FF
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.76
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 291 test pseudo label FF
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.759
with mlp
epoch 292 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
with mlp
epoch 293 test pseudo label FF
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.763
with mlp
epoch 294 test pseudo label FF
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
with mlp
epoch 295 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
with mlp
epoch 296 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
with mlp
epoch 297 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.769
with mlp
epoch 298 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.769
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.767
with mlp
epoch 299 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.767
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
with mlp
epoch 300 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.768
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.766
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m10[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [33m[W 2021-07-20 00:49:17,199][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 131, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 131, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [1.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:49:41,605][0m A new study created in memory with name: no-name-a9749543-3132-4dc0-be0f-4c6b1ec60a08[0m
lambda1:  0.0
lambda2:  1.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=1.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
epoch 1 test pseudo label FF
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
with mlp
epoch 2 test pseudo label FF
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.759
with mlp
epoch 3 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
with mlp
epoch 4 test pseudo label FF
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.763
with mlp
epoch 5 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
with mlp
epoch 6 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.762
with mlp
epoch 7 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 8 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 9 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 10 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 11 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 12 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 13 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 14 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 15 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 16 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 17 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 18 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 19 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 20 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 20, Loss: 1887.6927, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 21 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 22 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 23 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 24 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 25 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 26 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 27 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 28 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 29 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 30 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 31 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 32 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 33 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 34 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 35 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 36 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 37 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 38 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 39 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 40 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 40, Loss: 1389.1696, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 41 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 42 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 43 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 44 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 45 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 46 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 47 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 48 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 49 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 50 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 51 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 52 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 53 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 54 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 55 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 56 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 57 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 58 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 59 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 60 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 60, Loss: 1181.9622, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 61 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 62 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 63 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 64 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 65 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 66 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 67 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 68 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 69 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 70 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 71 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 72 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 73 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 74 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 75 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 76 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 77 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 78 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 79 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 80 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 80, Loss: 1075.5554, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 81 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 82 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 83 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 84 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 85 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 86 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 87 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 88 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 89 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 90 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 91 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 92 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 93 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 94 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 95 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 96 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 97 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 98 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 99 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 100 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m50[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 100, Loss: 999.0941, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 101 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 102 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 103 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 104 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 105 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 106 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 107 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 108 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 109 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 110 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 111 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 112 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 113 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 114 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 115 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 116 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 117 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 118 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 119 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 120 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 120, Loss: 957.9139, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 121 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 122 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 123 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 124 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 125 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 126 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 127 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 128 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 129 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 130 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 131 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 132 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 133 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 134 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 135 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 136 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 137 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 138 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 139 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 140 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 140, Loss: 938.4604, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 141 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 142 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 143 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 144 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 145 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 146 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 147 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 148 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 149 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 150 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> with mlp
epoch 151 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 152 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 153 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 154 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 155 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 156 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 157 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 158 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 159 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 160 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 160, Loss: 893.0950, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 161 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 162 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 163 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 164 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 165 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 166 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 167 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 168 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 169 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 170 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 171 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 172 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 173 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 174 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 175 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 176 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 177 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 178 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 179 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 180 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 180, Loss: 891.7512, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 181 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 182 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 183 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 184 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 185 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 186 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 187 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 188 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 189 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 190 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 191 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 192 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 193 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 194 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 195 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 196 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 197 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 198 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 199 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 200 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m50[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 200, Loss: 839.3170, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 201 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 202 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 203 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 204 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 205 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 206 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 207 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 208 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 209 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 210 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 211 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 212 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 213 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 214 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 215 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 216 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 217 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 218 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 219 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 220 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 220, Loss: 856.9798, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 221 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 222 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 223 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 224 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 225 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 226 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 227 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 228 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 229 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 230 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 231 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 232 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 233 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 234 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 235 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 236 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 237 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 238 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 239 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 240 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 240, Loss: 839.2628, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 241 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 242 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 243 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 244 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 245 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 246 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 247 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 248 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 249 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 250 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'cc' is not defined
ipdb> with mlp
epoch 251 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 252 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 253 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 254 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 255 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 256 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 257 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 258 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 259 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 260 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 260, Loss: 846.6912, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 261 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 262 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 263 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 264 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 265 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 266 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 267 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 268 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 269 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 270 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 271 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 272 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 273 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 274 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 275 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 276 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 277 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 278 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 279 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 280 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 280, Loss: 848.3898, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 281 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 282 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 283 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 284 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 285 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 286 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 287 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 288 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 289 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 290 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 291 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 292 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 293 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 294 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 295 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 296 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 297 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 298 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 299 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 300 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m50[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Split: 01, Run: 01, Epoch: 300, Loss: 816.6421, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 301 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 302 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 303 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 304 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 305 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 306 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 307 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 308 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 309 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 310 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 311 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 312 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 313 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 314 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 315 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 316 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 317 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 318 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 319 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 320 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 320, Loss: 791.3780, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 321 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 322 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 323 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 324 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 325 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 326 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 327 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 328 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 329 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 330 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 331 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 332 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 333 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 334 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 335 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 336 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 337 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 338 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 339 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 340 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 340, Loss: 808.2175, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 341 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 342 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 343 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 344 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 345 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 346 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 347 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 348 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 349 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 350 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(138)[0;36mobjective[0;34m()[0m
[0;32m    137 [0;31m                    [0mresult[0m [0;34m=[0m [0mtest[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata[0m[0;34m,[0m [0msplit_idx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 138 [0;31m                [0mlogger[0m[0;34m.[0m[0madd_result[0m[0;34m([0m[0mruns_overall[0m[0;34m,[0m [0mresult[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    139 [0;31m[0;34m[0m[0m
[0m
ipdb> ipdb> with mlp
epoch 351 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 352 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 353 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 354 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 355 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 356 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 357 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 358 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 359 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 360 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 360, Loss: 792.3744, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 361 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 362 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 363 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 364 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 365 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 366 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 367 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 368 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 369 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 370 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 371 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 372 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 373 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 374 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 375 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 376 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 377 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 378 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 379 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 380 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
Split: 01, Run: 01, Epoch: 380, Loss: 786.3751, Train: 100.00%, Valid: 75.00% Test: 76.10%
with mlp
epoch 381 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 382 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 383 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 384 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 385 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 386 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 387 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 388 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 389 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 390 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 391 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 392 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 393 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 394 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 395 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 396 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 397 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 398 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 399 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
with mlp
epoch 400 test pseudo label FF
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.761
> [0;32m/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/main_optuna.py[0m(131)[0;36mobjective[0;34m()[0m
[0;32m    130 [0;31m                    [0;32mif[0m [0mepoch[0m [0;34m%[0m [0;36m50[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m                        [0;32mimport[0m [0mipdb[0m[0;34m;[0m [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m                    [0;31m# loss = train(model, data, train_idx, optimizer, args=args)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ipdb> [33m[W 2021-07-20 00:50:25,279][0m Trial 0 failed because of the following error: BdbQuit()
Traceback (most recent call last):
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/site-packages/optuna/_optimize.py", line 189, in _run_trial
    value = func(trial)
  File "main_optuna.py", line 131, in objective
    import ipdb; ipdb.set_trace()
  File "main_optuna.py", line 131, in objective
    import ipdb; ipdb.set_trace()
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/mnt/home/xiaorui/Anaconda3/envs/GNN/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit[0m
Exiting Debugger.
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [10.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:51:18,127][0m A new study created in memory with name: no-name-40ff258b-292b-4472-b44f-e3345ebb9f37[0m
lambda1:  0.1
lambda2:  10.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
no mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.667
with mlp
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.666
with mlp
train_acc: 1.0, valid_acc: 0.658, test_acc: 0.666
with mlp
train_acc: 1.0, valid_acc: 0.658, test_acc: 0.666
with mlp
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.667
with mlp
train_acc: 1.0, valid_acc: 0.656, test_acc: 0.67
with mlp
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.673
with mlp
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.674
with mlp
train_acc: 1.0, valid_acc: 0.662, test_acc: 0.676
with mlp
train_acc: 1.0, valid_acc: 0.666, test_acc: 0.684
with mlp
train_acc: 1.0, valid_acc: 0.672, test_acc: 0.691
with mlp
train_acc: 1.0, valid_acc: 0.674, test_acc: 0.694
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.696
with mlp
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.699
with mlp
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.699
with mlp
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.701
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.701
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.701
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.705
Split: 01, Run: 01, Epoch: 20, Loss: 17798.7012, Train: 100.00%, Valid: 67.80% Test: 70.50%
with mlp
train_acc: 1.0, valid_acc: 0.682, test_acc: 0.706
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.709
with mlp
train_acc: 1.0, valid_acc: 0.688, test_acc: 0.709
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.738
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.763
Split: 01, Run: 01, Epoch: 40, Loss: 16188.4990, Train: 100.00%, Valid: 72.80% Test: 76.30%
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
Split: 01, Run: 01, Epoch: 60, Loss: 15479.5713, Train: 100.00%, Valid: 76.00% Test: 78.10%
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
Split: 01, Run: 01, Epoch: 80, Loss: 14668.3682, Train: 100.00%, Valid: 76.00% Test: 78.30%
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.778
Split: 01, Run: 01, Epoch: 100, Loss: 14039.0264, Train: 100.00%, Valid: 76.20% Test: 77.80%
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.784
Split: 01, Run: 01, Epoch: 120, Loss: 13316.7627, Train: 100.00%, Valid: 76.20% Test: 78.40%
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.774
Split: 01, Run: 01, Epoch: 140, Loss: 12883.5176, Train: 100.00%, Valid: 76.00% Test: 77.40%
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.768
Split: 01, Run: 01, Epoch: 160, Loss: 12577.3955, Train: 100.00%, Valid: 75.20% Test: 76.80%
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.766
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.756
Split: 01, Run: 01, Epoch: 180, Loss: 12111.5342, Train: 100.00%, Valid: 73.40% Test: 75.60%
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.756
Split: 01, Run: 01, Epoch: 200, Loss: 11795.0732, Train: 100.00%, Valid: 73.20% Test: 75.60%
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.742
Split: 01, Run: 01, Epoch: 220, Loss: 12029.6318, Train: 100.00%, Valid: 72.80% Test: 74.20%
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.742
Split: 01, Run: 01, Epoch: 240, Loss: 11897.7822, Train: 100.00%, Valid: 73.20% Test: 74.20%
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.738
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.734
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.73
Split: 01, Run: 01, Epoch: 260, Loss: 11759.0293, Train: 100.00%, Valid: 72.40% Test: 73.00%
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.736
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.735
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.733
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.734
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.725
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.731
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.73
Split: 01, Run: 01, Epoch: 280, Loss: 11200.2383, Train: 100.00%, Valid: 71.60% Test: 73.00%
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.726
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.73
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.725
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.725
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.724
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
Split: 01, Run: 01, Epoch: 300, Loss: 11411.7793, Train: 100.00%, Valid: 70.40% Test: 71.80%
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.724
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.728
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.727
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.725
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.719
Split: 01, Run: 01, Epoch: 320, Loss: 11199.8193, Train: 100.00%, Valid: 70.00% Test: 71.90%
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.722
Split: 01, Run: 01, Epoch: 340, Loss: 10493.7529, Train: 100.00%, Valid: 70.60% Test: 72.20%
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.713
Split: 01, Run: 01, Epoch: 360, Loss: 10295.7783, Train: 100.00%, Valid: 69.60% Test: 71.30%
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.714
Split: 01, Run: 01, Epoch: 380, Loss: 10438.2217, Train: 100.00%, Valid: 70.60% Test: 71.40%
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.72
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.712
Split: 01, Run: 01, Epoch: 400, Loss: 10040.3623, Train: 100.00%, Valid: 69.60% Test: 71.20%
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.69, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.714
Split: 01, Run: 01, Epoch: 420, Loss: 10315.0938, Train: 100.00%, Valid: 69.80% Test: 71.40%
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.719
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.717
Split: 01, Run: 01, Epoch: 440, Loss: 10383.5684, Train: 100.00%, Valid: 70.00% Test: 71.70%
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.712
Split: 01, Run: 01, Epoch: 460, Loss: 10590.5273, Train: 100.00%, Valid: 70.20% Test: 71.20%
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.715
Split: 01, Run: 01, Epoch: 480, Loss: 10330.2207, Train: 100.00%, Valid: 70.20% Test: 71.50%
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.7, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.717
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.715
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.709
Split: 01, Run: 01, Epoch: 500, Loss: 9783.8877, Train: 100.00%, Valid: 70.20% Test: 70.90%
Split: 01, Run: 01
None time:  18.15288472175598
None Run 01:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 78.10
no mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.687
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.689
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.688
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.69
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.696
with mlp
train_acc: 1.0, valid_acc: 0.69, test_acc: 0.697
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.702
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.704
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.706
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.704
with mlp
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.701
with mlp
train_acc: 1.0, valid_acc: 0.688, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.722
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.698, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.723
Split: 01, Run: 02, Epoch: 20, Loss: 17512.8047, Train: 100.00%, Valid: 68.60% Test: 72.30%
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.723
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.771
Split: 01, Run: 02, Epoch: 40, Loss: 16005.0234, Train: 100.00%, Valid: 75.00% Test: 77.10%
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.776
Split: 01, Run: 02, Epoch: 60, Loss: 14951.4189, Train: 100.00%, Valid: 75.40% Test: 77.60%
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.777
Split: 01, Run: 02, Epoch: 80, Loss: 13881.5684, Train: 100.00%, Valid: 75.00% Test: 77.70%
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.781
Split: 01, Run: 02, Epoch: 100, Loss: 13883.6826, Train: 100.00%, Valid: 75.40% Test: 78.10%
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.769
Split: 01, Run: 02, Epoch: 120, Loss: 13586.3916, Train: 100.00%, Valid: 75.00% Test: 76.90%
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.77
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.768
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.765
Split: 01, Run: 02, Epoch: 140, Loss: 12904.1240, Train: 100.00%, Valid: 75.60% Test: 76.50%
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.766
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.766
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.766
with mlp
train_acc: 1.0, valid_acc: 0.75, test_acc: 0.766
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.767
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.765
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.76
Split: 01, Run: 02, Epoch: 160, Loss: 12500.5938, Train: 100.00%, Valid: 73.60% Test: 76.00%
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.762
Split: 01, Run: 02, Epoch: 180, Loss: 12272.8457, Train: 100.00%, Valid: 74.00% Test: 76.20%
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.758
Split: 01, Run: 02, Epoch: 200, Loss: 11875.2480, Train: 100.00%, Valid: 73.20% Test: 75.80%
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
Split: 01, Run: 02, Epoch: 220, Loss: 11102.5371, Train: 100.00%, Valid: 72.60% Test: 75.10%
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.753
Split: 01, Run: 02, Epoch: 240, Loss: 11155.6543, Train: 100.00%, Valid: 72.60% Test: 75.30%
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.755
Split: 01, Run: 02, Epoch: 260, Loss: 11499.5352, Train: 100.00%, Valid: 73.40% Test: 75.50%
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.746
Split: 01, Run: 02, Epoch: 280, Loss: 11196.0605, Train: 100.00%, Valid: 73.00% Test: 74.60%
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.743
Split: 01, Run: 02, Epoch: 300, Loss: 10665.0732, Train: 100.00%, Valid: 73.00% Test: 74.30%
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.74
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.742
Split: 01, Run: 02, Epoch: 320, Loss: 10547.4707, Train: 100.00%, Valid: 72.40% Test: 74.20%
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.737
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.735
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.733
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.732
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.732
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.736
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.745
Split: 01, Run: 02, Epoch: 340, Loss: 10558.6738, Train: 100.00%, Valid: 72.80% Test: 74.50%
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.745
Split: 01, Run: 02, Epoch: 360, Loss: 10181.7021, Train: 100.00%, Valid: 71.60% Test: 74.50%
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.75
Split: 01, Run: 02, Epoch: 380, Loss: 10208.5449, Train: 100.00%, Valid: 72.40% Test: 75.00%
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.75
Split: 01, Run: 02, Epoch: 400, Loss: 10414.0977, Train: 100.00%, Valid: 72.80% Test: 75.00%
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
Split: 01, Run: 02, Epoch: 420, Loss: 10079.6436, Train: 100.00%, Valid: 72.60% Test: 75.00%
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.738
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.74
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.74
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.754
Split: 01, Run: 02, Epoch: 440, Loss: 9686.5068, Train: 100.00%, Valid: 73.20% Test: 75.40%
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.751
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.757
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.756
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.726, test_acc: 0.75
Split: 01, Run: 02, Epoch: 460, Loss: 9923.2422, Train: 100.00%, Valid: 72.60% Test: 75.00%
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.739
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.753
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.754
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.749
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.743
Split: 01, Run: 02, Epoch: 480, Loss: 9776.2344, Train: 100.00%, Valid: 71.00% Test: 74.30%
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.743
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.747
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.75
with mlp
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.755
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.758
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.762
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.732, test_acc: 0.763
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.76
Split: 01, Run: 02, Epoch: 500, Loss: 10017.8008, Train: 100.00%, Valid: 73.40% Test: 76.00%
Split: 01, Run: 02
None time:  17.907769402489066
None Run 02:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 100.00
   Final Test: 76.60
no mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.744
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.706
with mlp
train_acc: 1.0, valid_acc: 0.676, test_acc: 0.698
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.702
with mlp
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.701
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.704
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.705
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.706
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.707
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.709
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.712
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.713
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.711
with mlp
train_acc: 1.0, valid_acc: 0.686, test_acc: 0.709
with mlp
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.704
with mlp
train_acc: 1.0, valid_acc: 0.682, test_acc: 0.706
with mlp
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.708
with mlp
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.71
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.71
Split: 01, Run: 03, Epoch: 20, Loss: 17393.8418, Train: 100.00%, Valid: 69.40% Test: 71.00%
with mlp
train_acc: 1.0, valid_acc: 0.696, test_acc: 0.714
with mlp
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.716
with mlp
train_acc: 1.0, valid_acc: 0.702, test_acc: 0.721
with mlp
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.729
with mlp
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.736
with mlp
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.741
with mlp
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.742
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.746
with mlp
train_acc: 1.0, valid_acc: 0.714, test_acc: 0.745
with mlp
train_acc: 1.0, valid_acc: 0.716, test_acc: 0.748
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.752
with mlp
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.759
with mlp
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.76
with mlp
train_acc: 1.0, valid_acc: 0.73, test_acc: 0.761
with mlp
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.764
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.769
with mlp
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.771
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.775
Split: 01, Run: 03, Epoch: 40, Loss: 16000.9590, Train: 100.00%, Valid: 74.00% Test: 77.50%
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.742, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.788
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.786
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.788
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.79
Split: 01, Run: 03, Epoch: 60, Loss: 14701.0762, Train: 100.00%, Valid: 75.80% Test: 79.00%
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.766, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.793
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.793
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.795
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.793
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.792
Split: 01, Run: 03, Epoch: 80, Loss: 14573.3057, Train: 100.00%, Valid: 77.00% Test: 79.20%
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.796
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.793
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.794
with mlp
train_acc: 1.0, valid_acc: 0.78, test_acc: 0.792
with mlp
train_acc: 1.0, valid_acc: 0.78, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.768, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.766, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.768, test_acc: 0.783
Split: 01, Run: 03, Epoch: 100, Loss: 13964.1523, Train: 100.00%, Valid: 76.80% Test: 78.30%
with mlp
train_acc: 1.0, valid_acc: 0.77, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.788
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.78, test_acc: 0.791
with mlp
train_acc: 1.0, valid_acc: 0.78, test_acc: 0.793
with mlp
train_acc: 1.0, valid_acc: 0.78, test_acc: 0.79
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.776, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.778, test_acc: 0.789
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.774, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.772, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.766, test_acc: 0.785
Split: 01, Run: 03, Epoch: 120, Loss: 13656.3184, Train: 100.00%, Valid: 76.60% Test: 78.50%
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.787
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.785
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.764, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.766, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.777
Split: 01, Run: 03, Epoch: 140, Loss: 12821.0859, Train: 100.00%, Valid: 75.60% Test: 77.70%
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.776
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.784
Split: 01, Run: 03, Epoch: 160, Loss: 12703.7197, Train: 100.00%, Valid: 75.60% Test: 78.40%
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.784
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.778
Split: 01, Run: 03, Epoch: 180, Loss: 11994.4912, Train: 100.00%, Valid: 75.60% Test: 77.80%
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.778
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.782
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.783
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.781
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.777
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.775
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.772
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.773
Split: 01, Run: 03, Epoch: 200, Loss: 12012.5264, Train: 100.00%, Valid: 75.40% Test: 77.30%
with mlp
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.773
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.774
with mlp
train_acc: 1.0, valid_acc: 0.752, test_acc: 0.773
with mlp
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [10.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:52:30,406][0m A new study created in memory with name: no-name-4a93e442-d8fa-4f4d-92f4-8407836e7312[0m
lambda1:  0.1
lambda2:  10.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.744
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.667
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.666
train_acc: 1.0, valid_acc: 0.658, test_acc: 0.666
train_acc: 1.0, valid_acc: 0.658, test_acc: 0.666
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.667
train_acc: 1.0, valid_acc: 0.656, test_acc: 0.67
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.673
train_acc: 1.0, valid_acc: 0.654, test_acc: 0.674
train_acc: 1.0, valid_acc: 0.662, test_acc: 0.676
train_acc: 1.0, valid_acc: 0.666, test_acc: 0.684
train_acc: 1.0, valid_acc: 0.672, test_acc: 0.691
train_acc: 1.0, valid_acc: 0.674, test_acc: 0.694
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.696
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.699
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.699
train_acc: 1.0, valid_acc: 0.68, test_acc: 0.701
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.701
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.701
train_acc: 1.0, valid_acc: 0.678, test_acc: 0.705
Split: 01, Run: 01, Epoch: 20, Loss: 17798.7012, Train: 100.00%, Valid: 67.80% Test: 70.50%
train_acc: 1.0, valid_acc: 0.682, test_acc: 0.706
train_acc: 1.0, valid_acc: 0.684, test_acc: 0.709
train_acc: 1.0, valid_acc: 0.688, test_acc: 0.709
train_acc: 1.0, valid_acc: 0.692, test_acc: 0.712
train_acc: 1.0, valid_acc: 0.694, test_acc: 0.712
train_acc: 1.0, valid_acc: 0.704, test_acc: 0.718
train_acc: 1.0, valid_acc: 0.706, test_acc: 0.729
train_acc: 1.0, valid_acc: 0.71, test_acc: 0.738
train_acc: 1.0, valid_acc: 0.708, test_acc: 0.741
train_acc: 1.0, valid_acc: 0.712, test_acc: 0.743
train_acc: 1.0, valid_acc: 0.718, test_acc: 0.745
train_acc: 1.0, valid_acc: 0.72, test_acc: 0.75
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.755
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.757
train_acc: 1.0, valid_acc: 0.722, test_acc: 0.756
train_acc: 1.0, valid_acc: 0.724, test_acc: 0.758
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.759
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.76
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.762
train_acc: 1.0, valid_acc: 0.728, test_acc: 0.763
Split: 01, Run: 01, Epoch: 40, Loss: 16188.4990, Train: 100.00%, Valid: 72.80% Test: 76.30%
train_acc: 1.0, valid_acc: 0.736, test_acc: 0.763
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.734, test_acc: 0.761
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.767
train_acc: 1.0, valid_acc: 0.738, test_acc: 0.768
train_acc: 1.0, valid_acc: 0.74, test_acc: 0.771
train_acc: 1.0, valid_acc: 0.744, test_acc: 0.773
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.774
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
train_acc: 1.0, valid_acc: 0.746, test_acc: 0.775
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.777
train_acc: 1.0, valid_acc: 0.748, test_acc: 0.777
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
train_acc: 1.0, valid_acc: 0.754, test_acc: 0.779
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.779
train_acc: 1.0, valid_acc: 0.758, test_acc: 0.781
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.781
Split: 01, Run: 01, Epoch: 60, Loss: 15479.5713, Train: 100.00%, Valid: 76.00% Test: 78.10%
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.782
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.783
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
train_acc: 1.0, valid_acc: 0.762, test_acc: 0.785
train_acc: 1.0, valid_acc: 0.76, test_acc: 0.783
train_acc: 1.0, valid_acc: 0.756, test_acc: 0.78
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [10.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:52:41,751][0m A new study created in memory with name: no-name-36ce7225-d2cd-4b41-9ed5-9e0c6c29efe2[0m
lambda1:  0.1
lambda2:  10.0
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=10.0, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 17798.7012, Train: 100.00%, Valid: 67.80% Test: 70.50%
Split: 01, Run: 01, Epoch: 40, Loss: 16188.4990, Train: 100.00%, Valid: 72.80% Test: 76.30%
Split: 01, Run: 01, Epoch: 60, Loss: 15479.5713, Train: 100.00%, Valid: 76.00% Test: 78.10%
Split: 01, Run: 01, Epoch: 80, Loss: 14668.3682, Train: 100.00%, Valid: 76.00% Test: 78.30%
Split: 01, Run: 01, Epoch: 100, Loss: 14039.0264, Train: 100.00%, Valid: 76.20% Test: 77.80%
Split: 01, Run: 01, Epoch: 120, Loss: 13316.7627, Train: 100.00%, Valid: 76.20% Test: 78.40%
Split: 01, Run: 01, Epoch: 140, Loss: 12883.5176, Train: 100.00%, Valid: 76.00% Test: 77.40%
Split: 01, Run: 01, Epoch: 160, Loss: 12577.3955, Train: 100.00%, Valid: 75.20% Test: 76.80%
Split: 01, Run: 01, Epoch: 180, Loss: 12111.5342, Train: 100.00%, Valid: 73.40% Test: 75.60%
Split: 01, Run: 01, Epoch: 200, Loss: 11795.0732, Train: 100.00%, Valid: 73.20% Test: 75.60%
Split: 01, Run: 01, Epoch: 220, Loss: 12029.6318, Train: 100.00%, Valid: 72.80% Test: 74.20%
Split: 01, Run: 01, Epoch: 240, Loss: 11897.7822, Train: 100.00%, Valid: 73.20% Test: 74.20%
Split: 01, Run: 01, Epoch: 260, Loss: 11759.0293, Train: 100.00%, Valid: 72.40% Test: 73.00%
Split: 01, Run: 01, Epoch: 280, Loss: 11200.2383, Train: 100.00%, Valid: 71.60% Test: 73.00%
Split: 01, Run: 01, Epoch: 300, Loss: 11411.7793, Train: 100.00%, Valid: 70.40% Test: 71.80%
Split: 01, Run: 01, Epoch: 320, Loss: 11199.8193, Train: 100.00%, Valid: 70.00% Test: 71.90%
Split: 01, Run: 01, Epoch: 340, Loss: 10493.7529, Train: 100.00%, Valid: 70.60% Test: 72.20%
Split: 01, Run: 01, Epoch: 360, Loss: 10295.7783, Train: 100.00%, Valid: 69.60% Test: 71.30%
Split: 01, Run: 01, Epoch: 380, Loss: 10438.2217, Train: 100.00%, Valid: 70.60% Test: 71.40%
Split: 01, Run: 01, Epoch: 400, Loss: 10040.3623, Train: 100.00%, Valid: 69.60% Test: 71.20%
Split: 01, Run: 01, Epoch: 420, Loss: 10315.0938, Train: 100.00%, Valid: 69.80% Test: 71.40%
Split: 01, Run: 01, Epoch: 440, Loss: 10383.5684, Train: 100.00%, Valid: 70.00% Test: 71.70%
Split: 01, Run: 01, Epoch: 460, Loss: 10590.5273, Train: 100.00%, Valid: 70.20% Test: 71.20%
Split: 01, Run: 01, Epoch: 480, Loss: 10330.2207, Train: 100.00%, Valid: 70.20% Test: 71.50%
Split: 01, Run: 01, Epoch: 500, Loss: 9783.8877, Train: 100.00%, Valid: 70.20% Test: 70.90%
Split: 01, Run: 01
None time:  17.697602285072207
None Run 01:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 78.10
Split: 01, Run: 02, Epoch: 20, Loss: 17512.8047, Train: 100.00%, Valid: 68.60% Test: 72.30%
Split: 01, Run: 02, Epoch: 40, Loss: 16005.0234, Train: 100.00%, Valid: 75.00% Test: 77.10%
Split: 01, Run: 02, Epoch: 60, Loss: 14951.4189, Train: 100.00%, Valid: 75.40% Test: 77.60%
Split: 01, Run: 02, Epoch: 80, Loss: 13881.5684, Train: 100.00%, Valid: 75.00% Test: 77.70%
Split: 01, Run: 02, Epoch: 100, Loss: 13883.6826, Train: 100.00%, Valid: 75.40% Test: 78.10%
Split: 01, Run: 02, Epoch: 120, Loss: 13586.3916, Train: 100.00%, Valid: 75.00% Test: 76.90%
Split: 01, Run: 02, Epoch: 140, Loss: 12904.1240, Train: 100.00%, Valid: 75.60% Test: 76.50%
Split: 01, Run: 02, Epoch: 160, Loss: 12500.5938, Train: 100.00%, Valid: 73.60% Test: 76.00%
Split: 01, Run: 02, Epoch: 180, Loss: 12272.8457, Train: 100.00%, Valid: 74.00% Test: 76.20%
Split: 01, Run: 02, Epoch: 200, Loss: 11875.2480, Train: 100.00%, Valid: 73.20% Test: 75.80%
Split: 01, Run: 02, Epoch: 220, Loss: 11102.5371, Train: 100.00%, Valid: 72.60% Test: 75.10%
Split: 01, Run: 02, Epoch: 240, Loss: 11155.6543, Train: 100.00%, Valid: 72.60% Test: 75.30%
Split: 01, Run: 02, Epoch: 260, Loss: 11499.5352, Train: 100.00%, Valid: 73.40% Test: 75.50%
Split: 01, Run: 02, Epoch: 280, Loss: 11196.0605, Train: 100.00%, Valid: 73.00% Test: 74.60%
Split: 01, Run: 02, Epoch: 300, Loss: 10665.0732, Train: 100.00%, Valid: 73.00% Test: 74.30%
Split: 01, Run: 02, Epoch: 320, Loss: 10547.4707, Train: 100.00%, Valid: 72.40% Test: 74.20%
Split: 01, Run: 02, Epoch: 340, Loss: 10558.6738, Train: 100.00%, Valid: 72.80% Test: 74.50%
Split: 01, Run: 02, Epoch: 360, Loss: 10181.7021, Train: 100.00%, Valid: 71.60% Test: 74.50%
Split: 01, Run: 02, Epoch: 380, Loss: 10208.5449, Train: 100.00%, Valid: 72.40% Test: 75.00%
Split: 01, Run: 02, Epoch: 400, Loss: 10414.0977, Train: 100.00%, Valid: 72.80% Test: 75.00%
Split: 01, Run: 02, Epoch: 420, Loss: 10079.6436, Train: 100.00%, Valid: 72.60% Test: 75.00%
Split: 01, Run: 02, Epoch: 440, Loss: 9686.5068, Train: 100.00%, Valid: 73.20% Test: 75.40%
Split: 01, Run: 02, Epoch: 460, Loss: 9923.2422, Train: 100.00%, Valid: 72.60% Test: 75.00%
Split: 01, Run: 02, Epoch: 480, Loss: 9776.2344, Train: 100.00%, Valid: 71.00% Test: 74.30%
Split: 01, Run: 02, Epoch: 500, Loss: 10017.8008, Train: 100.00%, Valid: 73.40% Test: 76.00%
Split: 01, Run: 02
None time:  17.702625057660043
None Run 02:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 100.00
   Final Test: 76.60
Split: 01, Run: 03, Epoch: 20, Loss: 17393.8418, Train: 100.00%, Valid: 69.40% Test: 71.00%
Split: 01, Run: 03, Epoch: 40, Loss: 16000.9590, Train: 100.00%, Valid: 74.00% Test: 77.50%
Split: 01, Run: 03, Epoch: 60, Loss: 14701.0762, Train: 100.00%, Valid: 75.80% Test: 79.00%
Split: 01, Run: 03, Epoch: 80, Loss: 14573.3057, Train: 100.00%, Valid: 77.00% Test: 79.20%
Split: 01, Run: 03, Epoch: 100, Loss: 13964.1523, Train: 100.00%, Valid: 76.80% Test: 78.30%
Split: 01, Run: 03, Epoch: 120, Loss: 13656.3184, Train: 100.00%, Valid: 76.60% Test: 78.50%
Split: 01, Run: 03, Epoch: 140, Loss: 12821.0859, Train: 100.00%, Valid: 75.60% Test: 77.70%
Split: 01, Run: 03, Epoch: 160, Loss: 12703.7197, Train: 100.00%, Valid: 75.60% Test: 78.40%
Split: 01, Run: 03, Epoch: 180, Loss: 11994.4912, Train: 100.00%, Valid: 75.60% Test: 77.80%
Split: 01, Run: 03, Epoch: 200, Loss: 12012.5264, Train: 100.00%, Valid: 75.40% Test: 77.30%
Split: 01, Run: 03, Epoch: 220, Loss: 11763.5381, Train: 100.00%, Valid: 75.00% Test: 77.10%
Split: 01, Run: 03, Epoch: 240, Loss: 11695.0283, Train: 100.00%, Valid: 75.00% Test: 76.60%
Split: 01, Run: 03, Epoch: 260, Loss: 11003.0615, Train: 100.00%, Valid: 74.00% Test: 76.10%
Split: 01, Run: 03, Epoch: 280, Loss: 11039.8047, Train: 100.00%, Valid: 73.40% Test: 74.70%
Split: 01, Run: 03, Epoch: 300, Loss: 11289.0234, Train: 100.00%, Valid: 73.80% Test: 75.70%
Split: 01, Run: 03, Epoch: 320, Loss: 10932.9043, Train: 100.00%, Valid: 73.60% Test: 75.90%
Split: 01, Run: 03, Epoch: 340, Loss: 10694.5840, Train: 100.00%, Valid: 73.60% Test: 75.50%
Split: 01, Run: 03, Epoch: 360, Loss: 10557.0078, Train: 100.00%, Valid: 71.80% Test: 74.20%
Split: 01, Run: 03, Epoch: 380, Loss: 10306.0713, Train: 100.00%, Valid: 72.00% Test: 74.30%
Split: 01, Run: 03, Epoch: 400, Loss: 10486.8525, Train: 100.00%, Valid: 71.60% Test: 74.30%
Split: 01, Run: 03, Epoch: 420, Loss: 9913.6426, Train: 100.00%, Valid: 71.80% Test: 72.60%
Split: 01, Run: 03, Epoch: 440, Loss: 10293.6729, Train: 100.00%, Valid: 71.80% Test: 72.70%
Split: 01, Run: 03, Epoch: 460, Loss: 10110.8047, Train: 100.00%, Valid: 71.40% Test: 72.60%
Split: 01, Run: 03, Epoch: 480, Loss: 9961.1846, Train: 100.00%, Valid: 70.40% Test: 72.60%
Split: 01, Run: 03, Epoch: 500, Loss: 9826.1816, Train: 100.00%, Valid: 71.20% Test: 72.80%
Split: 01, Run: 03
None time:  17.418563356623054
None Run 03:
Highest Train: 100.00
Highest Valid: 78.00
  Final Train: 100.00
   Final Test: 79.00
total time:  54.56028751749545
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.67 Â± 1.22
  Final Train: 100.00 Â± 0.00
   Final Test: 77.90 Â± 1.21
[32m[I 2021-07-20 00:53:36,322][0m Trial 0 finished with value: 76.66666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 10.0, 'K': 30}. Best is trial 0 with value: 76.66666412353516.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 10.0, 'K': 30}   trial.value:  76.667    {'train': '100.00 Â± 0.00', 'valid': '76.67 Â± 1.22', 'test': '77.90 Â± 1.21'}
test_acc
['77.90 Â± 1.21']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 10.0, 'K': 30}
Best trial Value:  76.66666412353516
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '76.67 Â± 1.22', 'test': '77.90 Â± 1.21'}
optuna total time:  54.575594327412546
Using backend: pytorch
main:  Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1], 'lambda2': [0.1], 'alpha': [0.1], 'dropout': [0.5], 'K': [30]}
num_trial:  1
[32m[I 2021-07-20 00:54:01,182][0m A new study created in memory with name: no-name-1bb637bf-412a-400c-a411-4c3fc6ccbea8[0m
lambda1:  0.1
lambda2:  0.1
K:  30
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=30, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=20, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=30, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 20, Loss: 11.6911, Train: 99.29%, Valid: 29.60% Test: 32.80%
Split: 01, Run: 01, Epoch: 40, Loss: 10.5790, Train: 100.00%, Valid: 34.20% Test: 37.80%
Split: 01, Run: 01, Epoch: 60, Loss: 10.5412, Train: 100.00%, Valid: 39.80% Test: 43.50%
Split: 01, Run: 01, Epoch: 80, Loss: 10.3912, Train: 100.00%, Valid: 43.80% Test: 47.30%
Split: 01, Run: 01, Epoch: 100, Loss: 9.9739, Train: 100.00%, Valid: 45.00% Test: 48.10%
Split: 01, Run: 01, Epoch: 120, Loss: 9.4815, Train: 100.00%, Valid: 46.40% Test: 49.00%
Split: 01, Run: 01, Epoch: 140, Loss: 10.4711, Train: 100.00%, Valid: 48.20% Test: 50.30%
Split: 01, Run: 01, Epoch: 160, Loss: 10.0971, Train: 100.00%, Valid: 49.40% Test: 50.30%
Split: 01, Run: 01, Epoch: 180, Loss: 9.6094, Train: 100.00%, Valid: 50.60% Test: 50.90%
Split: 01, Run: 01, Epoch: 200, Loss: 9.9166, Train: 100.00%, Valid: 51.00% Test: 50.70%
Split: 01, Run: 01, Epoch: 220, Loss: 10.1994, Train: 100.00%, Valid: 50.80% Test: 50.70%
Split: 01, Run: 01, Epoch: 240, Loss: 10.8756, Train: 100.00%, Valid: 51.00% Test: 52.00%
Split: 01, Run: 01, Epoch: 260, Loss: 10.0200, Train: 100.00%, Valid: 51.60% Test: 53.40%
Split: 01, Run: 01, Epoch: 280, Loss: 9.7105, Train: 100.00%, Valid: 51.80% Test: 53.80%
Split: 01, Run: 01, Epoch: 300, Loss: 10.1261, Train: 100.00%, Valid: 51.40% Test: 53.00%
Split: 01, Run: 01, Epoch: 320, Loss: 9.8927, Train: 100.00%, Valid: 54.40% Test: 55.30%
Split: 01, Run: 01, Epoch: 340, Loss: 9.1539, Train: 100.00%, Valid: 54.20% Test: 54.30%
Split: 01, Run: 01, Epoch: 360, Loss: 10.4036, Train: 100.00%, Valid: 53.40% Test: 54.60%
Split: 01, Run: 01, Epoch: 380, Loss: 9.1903, Train: 100.00%, Valid: 56.60% Test: 55.90%
Split: 01, Run: 01, Epoch: 400, Loss: 9.6533, Train: 100.00%, Valid: 55.20% Test: 55.90%
Split: 01, Run: 01, Epoch: 420, Loss: 9.6294, Train: 100.00%, Valid: 54.40% Test: 55.40%
Split: 01, Run: 01, Epoch: 440, Loss: 9.8739, Train: 100.00%, Valid: 55.80% Test: 56.10%
Split: 01, Run: 01, Epoch: 460, Loss: 10.0354, Train: 100.00%, Valid: 55.40% Test: 55.80%
Split: 01, Run: 01, Epoch: 480, Loss: 10.1966, Train: 100.00%, Valid: 54.60% Test: 55.80%
Split: 01, Run: 01, Epoch: 500, Loss: 10.0364, Train: 100.00%, Valid: 56.40% Test: 56.80%
Split: 01, Run: 01
None time:  17.917191676795483
None Run 01:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 100.00
   Final Test: 74.50
Split: 01, Run: 02, Epoch: 20, Loss: 9.9505, Train: 100.00%, Valid: 57.80% Test: 57.80%
Split: 01, Run: 02, Epoch: 40, Loss: 9.6737, Train: 100.00%, Valid: 56.20% Test: 55.80%
Split: 01, Run: 02, Epoch: 60, Loss: 9.6291, Train: 100.00%, Valid: 54.80% Test: 55.20%
Split: 01, Run: 02, Epoch: 80, Loss: 9.9552, Train: 100.00%, Valid: 56.40% Test: 56.40%
Split: 01, Run: 02, Epoch: 100, Loss: 9.5343, Train: 100.00%, Valid: 55.80% Test: 55.70%
Split: 01, Run: 02, Epoch: 120, Loss: 9.6969, Train: 100.00%, Valid: 55.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 140, Loss: 9.4036, Train: 100.00%, Valid: 53.80% Test: 55.30%
Split: 01, Run: 02, Epoch: 160, Loss: 10.2110, Train: 100.00%, Valid: 53.80% Test: 54.90%
Split: 01, Run: 02, Epoch: 180, Loss: 9.7040, Train: 100.00%, Valid: 54.80% Test: 55.60%
Split: 01, Run: 02, Epoch: 200, Loss: 9.5188, Train: 100.00%, Valid: 54.00% Test: 55.30%
Split: 01, Run: 02, Epoch: 220, Loss: 9.2282, Train: 100.00%, Valid: 53.60% Test: 54.70%
Split: 01, Run: 02, Epoch: 240, Loss: 9.3310, Train: 100.00%, Valid: 54.60% Test: 55.30%
Split: 01, Run: 02, Epoch: 260, Loss: 9.7724, Train: 100.00%, Valid: 56.40% Test: 56.30%
Split: 01, Run: 02, Epoch: 280, Loss: 10.3613, Train: 100.00%, Valid: 56.20% Test: 56.70%
Split: 01, Run: 02, Epoch: 300, Loss: 10.1530, Train: 100.00%, Valid: 56.80% Test: 56.90%
Split: 01, Run: 02, Epoch: 320, Loss: 10.6082, Train: 100.00%, Valid: 59.00% Test: 59.80%
Split: 01, Run: 02, Epoch: 340, Loss: 9.9140, Train: 100.00%, Valid: 59.80% Test: 60.80%
Split: 01, Run: 02, Epoch: 360, Loss: 9.7528, Train: 100.00%, Valid: 60.60% Test: 60.10%
Split: 01, Run: 02, Epoch: 380, Loss: 9.9509, Train: 100.00%, Valid: 60.20% Test: 60.10%
Split: 01, Run: 02, Epoch: 400, Loss: 10.2779, Train: 100.00%, Valid: 61.40% Test: 62.20%
Split: 01, Run: 02, Epoch: 420, Loss: 9.6158, Train: 100.00%, Valid: 60.00% Test: 60.60%
Split: 01, Run: 02, Epoch: 440, Loss: 8.8905, Train: 100.00%, Valid: 60.60% Test: 61.80%
Split: 01, Run: 02, Epoch: 460, Loss: 9.7674, Train: 100.00%, Valid: 59.80% Test: 60.30%
Split: 01, Run: 02, Epoch: 480, Loss: 9.6832, Train: 100.00%, Valid: 60.00% Test: 61.00%
Split: 01, Run: 02, Epoch: 500, Loss: 10.4267, Train: 100.00%, Valid: 59.60% Test: 59.40%
Split: 01, Run: 02
None time:  17.573781982064247
None Run 02:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 100.00
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 20, Loss: 10.5566, Train: 100.00%, Valid: 60.00% Test: 59.50%
Split: 01, Run: 03, Epoch: 40, Loss: 10.3675, Train: 100.00%, Valid: 71.00% Test: 68.20%
Split: 01, Run: 03, Epoch: 60, Loss: 10.0207, Train: 100.00%, Valid: 68.00% Test: 66.60%
Split: 01, Run: 03, Epoch: 80, Loss: 10.2338, Train: 100.00%, Valid: 64.80% Test: 64.80%
Split: 01, Run: 03, Epoch: 100, Loss: 10.4034, Train: 100.00%, Valid: 64.40% Test: 63.90%
Split: 01, Run: 03, Epoch: 120, Loss: 10.6727, Train: 100.00%, Valid: 63.00% Test: 62.50%
Split: 01, Run: 03, Epoch: 140, Loss: 10.1790, Train: 100.00%, Valid: 63.80% Test: 63.10%
Split: 01, Run: 03, Epoch: 160, Loss: 11.0427, Train: 100.00%, Valid: 61.20% Test: 60.20%
Split: 01, Run: 03, Epoch: 180, Loss: 9.1927, Train: 100.00%, Valid: 60.40% Test: 59.00%
Split: 01, Run: 03, Epoch: 200, Loss: 9.5184, Train: 100.00%, Valid: 60.80% Test: 59.70%
Split: 01, Run: 03, Epoch: 220, Loss: 9.1758, Train: 100.00%, Valid: 59.00% Test: 58.50%
Split: 01, Run: 03, Epoch: 240, Loss: 9.5905, Train: 100.00%, Valid: 57.80% Test: 57.10%
Split: 01, Run: 03, Epoch: 260, Loss: 9.1838, Train: 100.00%, Valid: 59.20% Test: 58.00%
Split: 01, Run: 03, Epoch: 280, Loss: 9.9564, Train: 100.00%, Valid: 61.40% Test: 61.40%
Split: 01, Run: 03, Epoch: 300, Loss: 10.6728, Train: 100.00%, Valid: 61.00% Test: 60.60%
Split: 01, Run: 03, Epoch: 320, Loss: 10.2481, Train: 100.00%, Valid: 61.60% Test: 60.30%
Split: 01, Run: 03, Epoch: 340, Loss: 9.8205, Train: 100.00%, Valid: 61.20% Test: 61.50%
Split: 01, Run: 03, Epoch: 360, Loss: 9.5522, Train: 100.00%, Valid: 60.60% Test: 62.10%
Split: 01, Run: 03, Epoch: 380, Loss: 9.2897, Train: 100.00%, Valid: 60.20% Test: 60.10%
Split: 01, Run: 03, Epoch: 400, Loss: 9.5488, Train: 100.00%, Valid: 60.00% Test: 59.30%
Split: 01, Run: 03, Epoch: 420, Loss: 8.8748, Train: 100.00%, Valid: 60.60% Test: 61.50%
Split: 01, Run: 03, Epoch: 440, Loss: 10.4378, Train: 100.00%, Valid: 61.00% Test: 61.10%
Split: 01, Run: 03, Epoch: 460, Loss: 9.3487, Train: 100.00%, Valid: 60.20% Test: 60.10%
Split: 01, Run: 03, Epoch: 480, Loss: 9.3290, Train: 100.00%, Valid: 61.20% Test: 60.50%
Split: 01, Run: 03, Epoch: 500, Loss: 9.6422, Train: 100.00%, Valid: 60.80% Test: 60.20%
Split: 01, Run: 03
None time:  17.54779414832592
None Run 03:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 100.00
   Final Test: 74.50
total time:  54.82456879038364
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 74.50 Â± 0.00
[32m[I 2021-07-20 00:54:56,017][0m Trial 0 finished with value: 72.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 30}. Best is trial 0 with value: 72.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 30}   trial.value:  72    {'train': '100.00 Â± 0.00', 'valid': '72.00 Â± 0.00', 'test': '74.50 Â± 0.00'}
test_acc
['74.50 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 30}
Best trial Value:  72.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '72.00 Â± 0.00', 'test': '74.50 Â± 0.00'}
optuna total time:  54.84038549195975
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0, 3, 6, 9], 'lambda2': [0, 3, 6, 9], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  16
[32m[I 2021-07-20 00:55:08,179][0m A new study created in memory with name: no-name-82943f6b-44d5-45ce-804c-15b81ff24f41[0m
lambda1:  3
lambda2:  0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=3, lambda2=0, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 0.0311, Train: 14.29%, Valid: 12.20% Test: 12.90%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0024, Train: 14.29%, Valid: 12.40% Test: 13.00%
Split: 01, Run: 01
None time:  8.038690603338182
None Run 01:
Highest Train: 96.43
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 02, Epoch: 200, Loss: 0.0014, Train: 14.29%, Valid: 16.20% Test: 14.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.0000, Train: 14.29%, Valid: 16.20% Test: 14.90%
Split: 01, Run: 02
None time:  7.966058981604874
None Run 02:
Highest Train: 96.43
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
total time:  17.68402332253754
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 72.00 Â± 0.00
  Final Train: 96.43 Â± 0.00
   Final Test: 74.50 Â± 0.00
[32m[I 2021-07-20 00:55:25,873][0m Trial 0 finished with value: 72.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 3, 'lambda2': 0, 'K': 10}. Best is trial 0 with value: 72.0.[0m
lambda1:  6
lambda2:  3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=6, lambda2=3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 7.3222, Train: 100.00%, Valid: 13.60% Test: 14.30%
Split: 01, Run: 01, Epoch: 400, Loss: 7.2808, Train: 100.00%, Valid: 14.40% Test: 15.00%
Split: 01, Run: 01
None time:  8.052667516283691
None Run 01:
Highest Train: 100.00
Highest Valid: 67.40
  Final Train: 100.00
   Final Test: 70.60
Split: 01, Run: 02, Epoch: 200, Loss: 8.1880, Train: 100.00%, Valid: 19.80% Test: 18.20%
Split: 01, Run: 02, Epoch: 400, Loss: 6.7650, Train: 100.00%, Valid: 20.60% Test: 19.10%
Split: 01, Run: 02
None time:  7.913283725269139
None Run 02:
Highest Train: 100.00
Highest Valid: 67.40
  Final Train: 100.00
   Final Test: 70.60
total time:  16.04132516682148
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 67.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 70.60 Â± 0.00
[32m[I 2021-07-20 00:55:41,921][0m Trial 1 finished with value: 67.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 6, 'lambda2': 3, 'K': 10}. Best is trial 0 with value: 72.0.[0m
lambda1:  6
lambda2:  6
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=6, lambda2=6, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.3, 0.4, 0.5], 'lambda2': [0.1, 0.2, 0.3, 0.4, 0.5], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  25
[32m[I 2021-07-20 00:56:03,407][0m A new study created in memory with name: no-name-987b899f-8914-4031-9cb8-f42787111e0b[0m
lambda1:  0.4
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.4, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 14.5141, Train: 100.00%, Valid: 34.40% Test: 39.40%
Split: 01, Run: 01, Epoch: 400, Loss: 13.4237, Train: 100.00%, Valid: 40.00% Test: 44.50%
Split: 01, Run: 01
None time:  8.221233821474016
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.60
Split: 01, Run: 02, Epoch: 200, Loss: 13.3283, Train: 100.00%, Valid: 43.20% Test: 47.00%
Split: 01, Run: 02, Epoch: 400, Loss: 13.9681, Train: 100.00%, Valid: 42.80% Test: 46.70%
Split: 01, Run: 02
None time:  7.854031357914209
None Run 02:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.60
total time:  17.78579660039395
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.60 Â± 0.00
[32m[I 2021-07-20 00:56:21,202][0m Trial 0 finished with value: 69.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.4, 'K': 10}. Best is trial 0 with value: 69.5999984741211.[0m
lambda1:  0.4
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 8.9943, Train: 100.00%, Valid: 32.60% Test: 37.90%
Split: 01, Run: 01, Epoch: 400, Loss: 8.7953, Train: 100.00%, Valid: 35.20% Test: 40.20%
Split: 01, Run: 01
None time:  7.899277152493596
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.80
Split: 01, Run: 02, Epoch: 200, Loss: 8.3806, Train: 100.00%, Valid: 40.20% Test: 43.80%
Split: 01, Run: 02, Epoch: 400, Loss: 8.7290, Train: 100.00%, Valid: 43.20% Test: 46.90%
Split: 01, Run: 02
None time:  7.948454068042338
None Run 02:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.80
total time:  15.923410242423415
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.80 Â± 0.00
[32m[I 2021-07-20 00:56:37,132][0m Trial 1 finished with value: 69.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 69.5999984741211.[0m
lambda1:  0.4
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.5, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 20.5191, Train: 100.00%, Valid: 37.80% Test: 42.60%
Split: 01, Run: 01, Epoch: 400, Loss: 18.3045, Train: 100.00%, Valid: 40.00% Test: 45.00%
Split: 01, Run: 01
None time:  8.035104542970657
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.50
Split: 01, Run: 02, Epoch: 200, Loss: 19.0742, Train: 100.00%, Valid: 48.00% Test: 50.40%
Split: 01, Run: 02, Epoch: 400, Loss: 19.1821, Train: 100.00%, Valid: 46.20% Test: 49.50%
Split: 01, Run: 02
None time:  7.901515897363424
None Run 02:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.50
total time:  16.00755700469017
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.50 Â± 0.00
[32m[I 2021-07-20 00:56:53,145][0m Trial 2 finished with value: 69.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.5, 'K': 10}. Best is trial 0 with value: 69.5999984741211.[0m
lambda1:  0.5
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.1, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 1.0978, Train: 100.00%, Valid: 20.80% Test: 23.50%
Split: 01, Run: 01, Epoch: 400, Loss: 1.0580, Train: 100.00%, Valid: 24.20% Test: 26.90%
Split: 01, Run: 01
None time:  8.144331788644195
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.30
Split: 01, Run: 02, Epoch: 200, Loss: 1.0351, Train: 100.00%, Valid: 34.80% Test: 36.40%
Split: 01, Run: 02, Epoch: 400, Loss: 0.9893, Train: 100.00%, Valid: 37.80% Test: 39.20%
Split: 01, Run: 02
None time:  7.9885088838636875
None Run 02:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.30
total time:  16.20287188142538
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.30 Â± 0.00
[32m[I 2021-07-20 00:57:09,353][0m Trial 3 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.1, 'K': 10}. Best is trial 3 with value: 69.80000305175781.[0m
lambda1:  0.5
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.2, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 3.4436, Train: 100.00%, Valid: 26.00% Test: 29.10%
Split: 01, Run: 01, Epoch: 400, Loss: 3.4293, Train: 100.00%, Valid: 29.60% Test: 33.20%
Split: 01, Run: 01
None time:  7.8476948123425245
None Run 01:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.80
Split: 01, Run: 02, Epoch: 200, Loss: 3.2444, Train: 100.00%, Valid: 42.40% Test: 45.00%
Split: 01, Run: 02, Epoch: 400, Loss: 3.2244, Train: 100.00%, Valid: 44.80% Test: 48.60%
Split: 01, Run: 02
None time:  7.7384232589975
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.80
total time:  15.650729537941515
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.80 Â± 0.00
[32m[I 2021-07-20 00:57:25,010][0m Trial 4 finished with value: 69.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.2, 'K': 10}. Best is trial 3 with value: 69.80000305175781.[0m
lambda1:  0.5
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 6.6721, Train: 100.00%, Valid: 29.00% Test: 31.50%
Split: 01, Run: 01, Epoch: 400, Loss: 6.6595, Train: 100.00%, Valid: 30.40% Test: 34.60%
Split: 01, Run: 01
None time:  7.710439122281969
None Run 01:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.40
Split: 01, Run: 02, Epoch: 200, Loss: 6.3733, Train: 100.00%, Valid: 37.20% Test: 42.50%
Split: 01, Run: 02, Epoch: 400, Loss: 6.3708, Train: 100.00%, Valid: 46.00% Test: 49.80%
Split: 01, Run: 02
None time:  7.737694999203086
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.40
total time:  15.51943578850478
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.40 Â± 0.00
[32m[I 2021-07-20 00:57:40,534][0m Trial 5 finished with value: 69.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.3, 'K': 10}. Best is trial 3 with value: 69.80000305175781.[0m
lambda1:  0.5
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.4, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 10.6713, Train: 100.00%, Valid: 30.20% Test: 32.50%
Split: 01, Run: 01, Epoch: 400, Loss: 10.1833, Train: 100.00%, Valid: 31.40% Test: 36.10%
Split: 01, Run: 01
None time:  7.785337161272764
None Run 01:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.30
Split: 01, Run: 02, Epoch: 200, Loss: 9.8566, Train: 100.00%, Valid: 46.20% Test: 49.50%
Split: 01, Run: 02, Epoch: 400, Loss: 9.9812, Train: 100.00%, Valid: 48.60% Test: 52.30%
Split: 01, Run: 02
None time:  7.786610919982195
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.30
total time:  15.638473997823894
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.30 Â± 0.00
[32m[I 2021-07-20 00:57:56,178][0m Trial 6 finished with value: 69.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.4, 'K': 10}. Best is trial 3 with value: 69.80000305175781.[0m
lambda1:  0.5
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.5, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 15.0609, Train: 100.00%, Valid: 32.20% Test: 37.80%
Split: 01, Run: 01, Epoch: 400, Loss: 14.1220, Train: 100.00%, Valid: 34.00% Test: 38.60%
Split: 01, Run: 01
None time:  7.7674869410693645
None Run 01:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.40
Split: 01, Run: 02, Epoch: 200, Loss: 14.2169, Train: 100.00%, Valid: 48.00% Test: 51.20%
Split: 01, Run: 02, Epoch: 400, Loss: 14.6599, Train: 100.00%, Valid: 46.80% Test: 51.60%
Split: 01, Run: 02
None time:  8.295867845416069
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 100.00
   Final Test: 71.40
total time:  16.133196841925383
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 71.40 Â± 0.00
[32m[I 2021-07-20 00:58:12,318][0m Trial 7 finished with value: 69.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.5, 'K': 10}. Best is trial 3 with value: 69.80000305175781.[0m
lambda1:  0.1
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 84.0427, Train: 100.00%, Valid: 64.40% Test: 65.60%
Split: 01, Run: 01, Epoch: 400, Loss: 79.1251, Train: 100.00%, Valid: 66.00% Test: 67.10%
Split: 01, Run: 01
None time:  7.7559645883738995
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 72.90
Split: 01, Run: 02, Epoch: 200, Loss: 79.9190, Train: 100.00%, Valid: 63.80% Test: 64.70%
Split: 01, Run: 02, Epoch: 400, Loss: 76.1590, Train: 100.00%, Valid: 63.80% Test: 64.10%
Split: 01, Run: 02
None time:  7.71503321826458
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 72.90
total time:  15.536064147949219
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.90 Â± 0.00
[32m[I 2021-07-20 00:58:27,859][0m Trial 8 finished with value: 70.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.5, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.2
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.1, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 4.2099, Train: 100.00%, Valid: 33.60% Test: 38.00%
Split: 01, Run: 01, Epoch: 400, Loss: 4.2413, Train: 100.00%, Valid: 34.60% Test: 41.00%
Split: 01, Run: 01
None time:  7.783381141722202
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.00
Split: 01, Run: 02, Epoch: 200, Loss: 3.9398, Train: 100.00%, Valid: 45.40% Test: 48.70%
Split: 01, Run: 02, Epoch: 400, Loss: 4.1063, Train: 100.00%, Valid: 50.60% Test: 51.40%
Split: 01, Run: 02
None time:  7.770061588846147
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.00
total time:  15.610634317621589
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.00 Â± 0.00
[32m[I 2021-07-20 00:58:43,475][0m Trial 9 finished with value: 70.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.3
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.1, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 2.4234, Train: 100.00%, Valid: 27.80% Test: 32.80%
Split: 01, Run: 01, Epoch: 400, Loss: 2.3745, Train: 100.00%, Valid: 29.60% Test: 33.30%
Split: 01, Run: 01
None time:  7.774651546962559
None Run 01:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 73.00
Split: 01, Run: 02, Epoch: 200, Loss: 2.2383, Train: 100.00%, Valid: 47.60% Test: 51.00%
Split: 01, Run: 02, Epoch: 400, Loss: 2.2213, Train: 100.00%, Valid: 40.60% Test: 46.10%
Split: 01, Run: 02
None time:  7.750167776830494
None Run 02:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 73.00
total time:  15.590030087158084
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.00 Â± 0.00
[32m[I 2021-07-20 00:58:59,070][0m Trial 10 finished with value: 70.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.1, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.3
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.2, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 6.9784, Train: 100.00%, Valid: 34.00% Test: 38.70%
Split: 01, Run: 01, Epoch: 400, Loss: 7.4658, Train: 100.00%, Valid: 34.20% Test: 41.10%
Split: 01, Run: 01
None time:  7.854440751485527
None Run 01:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 72.50
Split: 01, Run: 02, Epoch: 200, Loss: 6.7218, Train: 100.00%, Valid: 42.60% Test: 47.20%
Split: 01, Run: 02, Epoch: 400, Loss: 7.1188, Train: 100.00%, Valid: 53.00% Test: 56.20%
Split: 01, Run: 02
None time:  7.787657633423805
None Run 02:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 72.50
total time:  15.702865023165941
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.50 Â± 0.00
[32m[I 2021-07-20 00:59:14,779][0m Trial 11 finished with value: 70.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.2, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.3
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 13.4240, Train: 100.00%, Valid: 38.00% Test: 42.80%
Split: 01, Run: 01, Epoch: 400, Loss: 12.6142, Train: 100.00%, Valid: 40.00% Test: 45.20%
Split: 01, Run: 01
None time:  7.724413656629622
None Run 01:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 100.00
   Final Test: 72.40
Split: 01, Run: 02, Epoch: 200, Loss: 12.5586, Train: 100.00%, Valid: 52.80% Test: 54.70%
Split: 01, Run: 02, Epoch: 400, Loss: 13.0558, Train: 100.00%, Valid: 51.40% Test: 53.50%
Split: 01, Run: 02
None time:  7.721944845281541
None Run 02:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 100.00
   Final Test: 72.40
total time:  15.506925132125616
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.20 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.40 Â± 0.00
[32m[I 2021-07-20 00:59:30,291][0m Trial 12 finished with value: 70.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.3, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.3
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.4, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 20.6443, Train: 100.00%, Valid: 46.40% Test: 49.10%
Split: 01, Run: 01, Epoch: 400, Loss: 19.1207, Train: 100.00%, Valid: 49.00% Test: 51.10%
Split: 01, Run: 01
None time:  7.697394805960357
None Run 01:
Highest Train: 100.00
Highest Valid: 70.00
  Final Train: 100.00
   Final Test: 72.10
Split: 01, Run: 02, Epoch: 200, Loss: 19.8178, Train: 100.00%, Valid: 54.60% Test: 54.70%
Split: 01, Run: 02, Epoch: 400, Loss: 19.1173, Train: 100.00%, Valid: 52.00% Test: 55.20%
Split: 01, Run: 02
None time:  7.9045783542096615
None Run 02:
Highest Train: 100.00
Highest Valid: 70.00
  Final Train: 100.00
   Final Test: 72.10
total time:  15.666340810246766
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.10 Â± 0.00
[32m[I 2021-07-20 00:59:45,963][0m Trial 13 finished with value: 70.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.4, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.3
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.5, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 27.5827, Train: 100.00%, Valid: 42.60% Test: 46.80%
Split: 01, Run: 01, Epoch: 400, Loss: 25.7920, Train: 100.00%, Valid: 48.20% Test: 51.30%
Split: 01, Run: 01
None time:  7.6654064897447824
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.00
Split: 01, Run: 02, Epoch: 200, Loss: 28.2287, Train: 100.00%, Valid: 56.80% Test: 57.00%
Split: 01, Run: 02, Epoch: 400, Loss: 27.3963, Train: 100.00%, Valid: 51.00% Test: 54.00%
Split: 01, Run: 02
None time:  7.694983485154808
None Run 02:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.00
total time:  15.42246693186462
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.00 Â± 0.00
[32m[I 2021-07-20 01:00:01,392][0m Trial 14 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.5, 'K': 10}. Best is trial 8 with value: 70.80000305175781.[0m
lambda1:  0.4
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.1, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 1.5602, Train: 100.00%, Valid: 24.40% Test: 27.60%
Split: 01, Run: 01, Epoch: 400, Loss: 1.4661, Train: 100.00%, Valid: 26.60% Test: 30.10%
Split: 01, Run: 01
None time:  7.860412743873894
None Run 01:
Highest Train: 100.00
Highest Valid: 71.20
  Final Train: 100.00
   Final Test: 72.50
Split: 01, Run: 02, Epoch: 200, Loss: 1.4640, Train: 100.00%, Valid: 40.80% Test: 43.30%
Split: 01, Run: 02, Epoch: 400, Loss: 1.4131, Train: 100.00%, Valid: 44.80% Test: 47.90%
Split: 01, Run: 02
None time:  7.746285906992853
None Run 02:
Highest Train: 100.00
Highest Valid: 71.20
  Final Train: 100.00
   Final Test: 72.50
total time:  15.670169939287007
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.20 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.50 Â± 0.00
[32m[I 2021-07-20 01:00:17,068][0m Trial 15 finished with value: 71.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.1, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.4
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.2, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 4.7414, Train: 100.00%, Valid: 30.00% Test: 33.20%
Split: 01, Run: 01, Epoch: 400, Loss: 4.7893, Train: 100.00%, Valid: 32.60% Test: 35.00%
Split: 01, Run: 01
None time:  7.845540496520698
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.10
Split: 01, Run: 02, Epoch: 200, Loss: 4.5764, Train: 100.00%, Valid: 43.20% Test: 46.50%
Split: 01, Run: 02, Epoch: 400, Loss: 4.5326, Train: 100.00%, Valid: 48.80% Test: 51.20%
Split: 01, Run: 02
None time:  7.972285717725754
None Run 02:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 100.00
   Final Test: 72.10
total time:  15.875180149450898
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.10 Â± 0.00
[32m[I 2021-07-20 01:00:32,949][0m Trial 16 finished with value: 69.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.2, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.1
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 42.1097, Train: 100.00%, Valid: 61.80% Test: 62.70%
Split: 01, Run: 01, Epoch: 400, Loss: 39.8226, Train: 100.00%, Valid: 63.80% Test: 64.50%
Split: 01, Run: 01
None time:  7.776497999206185
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.10
Split: 01, Run: 02, Epoch: 200, Loss: 41.4836, Train: 100.00%, Valid: 61.80% Test: 62.10%
Split: 01, Run: 02, Epoch: 400, Loss: 41.6265, Train: 100.00%, Valid: 63.40% Test: 64.20%
Split: 01, Run: 02
None time:  7.730510352179408
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.10
total time:  15.574359029531479
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.10 Â± 0.00
[32m[I 2021-07-20 01:00:48,530][0m Trial 17 finished with value: 70.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.2
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.3, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 20.8370, Train: 100.00%, Valid: 50.80% Test: 52.90%
Split: 01, Run: 01, Epoch: 400, Loss: 19.1466, Train: 100.00%, Valid: 52.20% Test: 53.20%
Split: 01, Run: 01
None time:  7.7034333208575845
None Run 01:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 72.60
Split: 01, Run: 02, Epoch: 200, Loss: 20.6675, Train: 100.00%, Valid: 58.60% Test: 60.10%
Split: 01, Run: 02, Epoch: 400, Loss: 20.7525, Train: 100.00%, Valid: 58.80% Test: 59.40%
Split: 01, Run: 02
None time:  7.905620296485722
None Run 02:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 100.00
   Final Test: 72.60
total time:  15.667233147658408
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.60 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.60 Â± 0.00
[32m[I 2021-07-20 01:01:04,203][0m Trial 18 finished with value: 70.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.3, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.2
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.4, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 31.5839, Train: 100.00%, Valid: 53.40% Test: 54.30%
Split: 01, Run: 01, Epoch: 400, Loss: 28.2312, Train: 100.00%, Valid: 54.20% Test: 56.30%
Split: 01, Run: 01
None time:  8.088560303673148
None Run 01:
Highest Train: 100.00
Highest Valid: 70.40
  Final Train: 100.00
   Final Test: 72.60
Split: 01, Run: 02, Epoch: 200, Loss: 32.4239, Train: 100.00%, Valid: 58.40% Test: 58.80%
Split: 01, Run: 02, Epoch: 400, Loss: 32.3668, Train: 100.00%, Valid: 55.80% Test: 56.10%
Split: 01, Run: 02
None time:  8.285140573047101
None Run 02:
Highest Train: 100.00
Highest Valid: 70.40
  Final Train: 100.00
   Final Test: 72.60
total time:  16.436578614637256
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.60 Â± 0.00
[32m[I 2021-07-20 01:01:20,647][0m Trial 19 finished with value: 70.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.4, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.2
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.5, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 44.3741, Train: 100.00%, Valid: 53.00% Test: 55.40%
Split: 01, Run: 01, Epoch: 400, Loss: 38.1063, Train: 100.00%, Valid: 55.60% Test: 57.60%
Split: 01, Run: 01
None time:  8.558944897726178
None Run 01:
Highest Train: 100.00
Highest Valid: 70.40
  Final Train: 100.00
   Final Test: 72.70
Split: 01, Run: 02, Epoch: 200, Loss: 43.6311, Train: 100.00%, Valid: 54.00% Test: 56.00%
Split: 01, Run: 02, Epoch: 400, Loss: 43.5962, Train: 100.00%, Valid: 56.40% Test: 58.80%
Split: 01, Run: 02
None time:  8.349441209807992
None Run 02:
Highest Train: 100.00
Highest Valid: 70.40
  Final Train: 100.00
   Final Test: 72.70
total time:  16.99825464375317
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.40 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.70 Â± 0.00
[32m[I 2021-07-20 01:01:37,652][0m Trial 20 finished with value: 70.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.5, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.1
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.2, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 24.0488, Train: 100.00%, Valid: 58.00% Test: 58.40%
Split: 01, Run: 01, Epoch: 400, Loss: 23.9012, Train: 100.00%, Valid: 59.80% Test: 60.50%
Split: 01, Run: 01
None time:  8.177163018845022
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.20
Split: 01, Run: 02, Epoch: 200, Loss: 24.4655, Train: 100.00%, Valid: 60.00% Test: 60.30%
Split: 01, Run: 02, Epoch: 400, Loss: 25.0777, Train: 100.00%, Valid: 61.40% Test: 61.00%
Split: 01, Run: 02
None time:  8.372349191457033
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.20
total time:  16.66096198000014
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.20 Â± 0.00
[32m[I 2021-07-20 01:01:54,320][0m Trial 21 finished with value: 70.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.2
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.2, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 11.3007, Train: 100.00%, Valid: 44.20% Test: 46.80%
Split: 01, Run: 01, Epoch: 400, Loss: 11.2730, Train: 100.00%, Valid: 44.60% Test: 47.70%
Split: 01, Run: 01
None time:  8.241939967498183
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 72.90
Split: 01, Run: 02, Epoch: 200, Loss: 11.1086, Train: 100.00%, Valid: 48.40% Test: 50.60%
Split: 01, Run: 02, Epoch: 400, Loss: 12.4346, Train: 100.00%, Valid: 56.40% Test: 57.60%
Split: 01, Run: 02
None time:  8.511653071269393
None Run 02:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 72.90
total time:  16.831809036433697
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 72.90 Â± 0.00
[32m[I 2021-07-20 01:02:11,159][0m Trial 22 finished with value: 71.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.2, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.1
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 9.2825, Train: 100.00%, Valid: 48.80% Test: 50.80%
Split: 01, Run: 01, Epoch: 400, Loss: 9.4481, Train: 100.00%, Valid: 53.80% Test: 53.70%
Split: 01, Run: 01
None time:  8.296746000647545
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.50
Split: 01, Run: 02, Epoch: 200, Loss: 9.5638, Train: 100.00%, Valid: 56.60% Test: 57.10%
Split: 01, Run: 02, Epoch: 400, Loss: 9.8036, Train: 100.00%, Valid: 57.60% Test: 57.00%
Split: 01, Run: 02
None time:  8.164965034462512
None Run 02:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.50
total time:  16.55067950580269
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.00 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.50 Â± 0.00
[32m[I 2021-07-20 01:02:27,717][0m Trial 23 finished with value: 71.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
lambda1:  0.1
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.4, log_steps=200, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=2, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 2 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 200, Loss: 61.7059, Train: 100.00%, Valid: 62.80% Test: 64.40%
Split: 01, Run: 01, Epoch: 400, Loss: 58.8862, Train: 100.00%, Valid: 64.60% Test: 66.10%
Split: 01, Run: 01
None time:  8.052877294830978
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.10
Split: 01, Run: 02, Epoch: 200, Loss: 60.5299, Train: 100.00%, Valid: 64.20% Test: 65.50%
Split: 01, Run: 02, Epoch: 400, Loss: 58.2478, Train: 100.00%, Valid: 67.00% Test: 67.60%
Split: 01, Run: 02
None time:  8.31561776343733
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 73.10
total time:  16.442047111690044
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.80 Â± 0.00
  Final Train: 100.00 Â± 0.00
   Final Test: 73.10 Â± 0.00
[32m[I 2021-07-20 01:02:44,166][0m Trial 24 finished with value: 70.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}. Best is trial 15 with value: 71.20000457763672.[0m
Study statistics: 
  Number of finished trials:  25
  Number of pruned trials:  0
  Number of complete trials:  25
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.5, 'K': 10}   trial.value:  70.8    {'train': '100.00 Â± 0.00', 'valid': '70.80 Â± 0.00', 'test': '72.90 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}   trial.value:  70.8    {'train': '100.00 Â± 0.00', 'valid': '70.80 Â± 0.00', 'test': '73.10 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}   trial.value:  70.8    {'train': '100.00 Â± 0.00', 'valid': '70.80 Â± 0.00', 'test': '73.20 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}   trial.value:  71    {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '73.50 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}   trial.value:  70.8    {'train': '100.00 Â± 0.00', 'valid': '70.80 Â± 0.00', 'test': '73.10 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}   trial.value:  70.8    {'train': '100.00 Â± 0.00', 'valid': '70.80 Â± 0.00', 'test': '73.00 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.3, 'K': 10}   trial.value:  70.6    {'train': '100.00 Â± 0.00', 'valid': '70.60 Â± 0.00', 'test': '72.60 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.4, 'K': 10}   trial.value:  70.4    {'train': '100.00 Â± 0.00', 'valid': '70.40 Â± 0.00', 'test': '72.60 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.5, 'K': 10}   trial.value:  70.4    {'train': '100.00 Â± 0.00', 'valid': '70.40 Â± 0.00', 'test': '72.70 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.2, 'K': 10}   trial.value:  71    {'train': '100.00 Â± 0.00', 'valid': '71.00 Â± 0.00', 'test': '72.90 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.1, 'K': 10}   trial.value:  70.6    {'train': '100.00 Â± 0.00', 'valid': '70.60 Â± 0.00', 'test': '73.00 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.2, 'K': 10}   trial.value:  70.6    {'train': '100.00 Â± 0.00', 'valid': '70.60 Â± 0.00', 'test': '72.50 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.3, 'K': 10}   trial.value:  70.2    {'train': '100.00 Â± 0.00', 'valid': '70.20 Â± 0.00', 'test': '72.40 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.4, 'K': 10}   trial.value:  70    {'train': '100.00 Â± 0.00', 'valid': '70.00 Â± 0.00', 'test': '72.10 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.5, 'K': 10}   trial.value:  69.8    {'train': '100.00 Â± 0.00', 'valid': '69.80 Â± 0.00', 'test': '72.00 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.4, 'K': 10}   trial.value:  69.6    {'train': '100.00 Â± 0.00', 'valid': '69.60 Â± 0.00', 'test': '71.60 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.3, 'K': 10}   trial.value:  69.6    {'train': '100.00 Â± 0.00', 'valid': '69.60 Â± 0.00', 'test': '71.80 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.5, 'K': 10}   trial.value:  69.6    {'train': '100.00 Â± 0.00', 'valid': '69.60 Â± 0.00', 'test': '71.50 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.1, 'K': 10}   trial.value:  71.2    {'train': '100.00 Â± 0.00', 'valid': '71.20 Â± 0.00', 'test': '72.50 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.2, 'K': 10}   trial.value:  69.8    {'train': '100.00 Â± 0.00', 'valid': '69.80 Â± 0.00', 'test': '72.10 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.1, 'K': 10}   trial.value:  69.8    {'train': '100.00 Â± 0.00', 'valid': '69.80 Â± 0.00', 'test': '72.30 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.2, 'K': 10}   trial.value:  69.4    {'train': '100.00 Â± 0.00', 'valid': '69.40 Â± 0.00', 'test': '71.80 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.3, 'K': 10}   trial.value:  69.4    {'train': '100.00 Â± 0.00', 'valid': '69.40 Â± 0.00', 'test': '71.40 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.4, 'K': 10}   trial.value:  69.4    {'train': '100.00 Â± 0.00', 'valid': '69.40 Â± 0.00', 'test': '71.30 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.5, 'K': 10}   trial.value:  69.4    {'train': '100.00 Â± 0.00', 'valid': '69.40 Â± 0.00', 'test': '71.40 Â± 0.00'}
test_acc
['72.90 Â± 0.00', '73.10 Â± 0.00', '73.20 Â± 0.00', '73.50 Â± 0.00', '73.10 Â± 0.00', '73.00 Â± 0.00', '72.60 Â± 0.00', '72.60 Â± 0.00', '72.70 Â± 0.00', '72.90 Â± 0.00', '73.00 Â± 0.00', '72.50 Â± 0.00', '72.40 Â± 0.00', '72.10 Â± 0.00', '72.00 Â± 0.00', '71.60 Â± 0.00', '71.80 Â± 0.00', '71.50 Â± 0.00', '72.50 Â± 0.00', '72.10 Â± 0.00', '72.30 Â± 0.00', '71.80 Â± 0.00', '71.40 Â± 0.00', '71.30 Â± 0.00', '71.40 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.1, 'K': 10}
Best trial Value:  71.20000457763672
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '71.20 Â± 0.00', 'test': '72.50 Â± 0.00'}
optuna total time:  400.77395319286734
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:08:55,873][0m A new study created in memory with name: no-name-77fab4c8-988c-45ac-aaa9-32742592bedf[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  16.007779013365507
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  13.885116886347532
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  13.865896686911583
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  51.31287926249206
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-26 08:09:47,261][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  51.393592992797494
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:11:44,328][0m A new study created in memory with name: no-name-f0d65098-ccd7-452c-b929-0af3064ec940[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.2765495758503675
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  6.674132788553834
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  7.1363425552845
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  22.725100483745337
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:12:07,062][0m Trial 0 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
test_acc
['80.43 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  22.739481806755066
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-26 08:12:17,397][0m A new study created in memory with name: no-name-c1a0b3d5-1eeb-4756-8664-14b9ec29368a[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 95.00%, Valid: 77.80% Test: 76.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 95.00%, Valid: 78.80% Test: 76.60%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 96.67%, Valid: 78.20% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 93.33%, Valid: 77.80% Test: 75.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 95.00%, Valid: 78.00% Test: 74.90%
Split: 01, Run: 01
None time:  8.47030183300376
None Run 01:
Highest Train: 98.33
Highest Valid: 80.80
  Final Train: 96.67
   Final Test: 78.20
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 96.67%, Valid: 78.60% Test: 77.90%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0706, Train: 96.67%, Valid: 77.20% Test: 75.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0888, Train: 93.33%, Valid: 78.60% Test: 76.20%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-26 08:12:34,134][0m A new study created in memory with name: no-name-04c9d6f0-c29a-48e6-b33e-d3a46ef7de03[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 400, Loss: 0.0912, Train: 96.67%, Valid: 79.40% Test: 77.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0535, Train: 93.33%, Valid: 77.60% Test: 74.30%
Split: 01, Run: 02
None time:  8.012981180101633
None Run 02:
Highest Train: 98.33
Highest Valid: 81.00
  Final Train: 95.00
   Final Test: 77.90
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 03, Epoch: 100, Loss: 0.1885, Train: 93.33%, Valid: 77.20% Test: 74.90%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 74.20% Test: 75.00%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1082, Train: 95.00%, Valid: 78.60% Test: 76.10%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 73.40% Test: 73.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 72.60% Test: 74.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0853, Train: 95.00%, Valid: 77.80% Test: 75.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 69.80% Test: 72.10%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0595, Train: 96.67%, Valid: 78.80% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 70.20% Test: 72.50%
Split: 01, Run: 01
None time:  7.61200974136591
None Run 01:
Highest Train: 100.00
Highest Valid: 75.80
  Final Train: 100.00
   Final Test: 76.00
Split: 01, Run: 03, Epoch: 500, Loss: 0.0716, Train: 96.67%, Valid: 76.20% Test: 75.50%
Split: 01, Run: 03
None time:  8.158292312175035
None Run 03:
Highest Train: 98.33
Highest Valid: 81.00
  Final Train: 96.67
   Final Test: 77.90
total time:  26.39111684076488
None All runs:
Highest Train: 98.33 Â± 0.00
Highest Valid: 80.93 Â± 0.12
  Final Train: 96.11 Â± 0.96
   Final Test: 78.00 Â± 0.17
[32m[I 2021-07-26 08:12:43,797][0m Trial 0 finished with value: 80.93333435058594 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 80.93333435058594.[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 74.00% Test: 75.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 98.33%, Valid: 78.40% Test: 76.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 72.00% Test: 73.70%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 98.33%, Valid: 78.00% Test: 77.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 71.00% Test: 73.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 72.60% Test: 73.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 98.33%, Valid: 77.80% Test: 77.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 72.80% Test: 73.60%
Split: 01, Run: 02
None time:  7.106517093256116
None Run 02:
Highest Train: 100.00
Highest Valid: 75.20
  Final Train: 100.00
   Final Test: 75.20
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 98.33%, Valid: 78.20% Test: 75.40%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 71.60% Test: 73.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 98.33%, Valid: 76.00% Test: 75.10%
Split: 01, Run: 01
None time:  8.083411417901516
None Run 01:
Highest Train: 98.33
Highest Valid: 80.20
  Final Train: 98.33
   Final Test: 77.20
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 73.60% Test: 73.90%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 98.33%, Valid: 77.40% Test: 76.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 71.00% Test: 72.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0706, Train: 98.33%, Valid: 74.80% Test: 73.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 73.00% Test: 74.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0888, Train: 98.33%, Valid: 77.60% Test: 76.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 70.60% Test: 72.50%
Split: 01, Run: 03
None time:  7.089303130283952
None Run 03:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 100.00
   Final Test: 76.00
total time:  23.462541826069355
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 75.53 Â± 0.31
  Final Train: 100.00 Â± 0.00
   Final Test: 75.73 Â± 0.46
[32m[I 2021-07-26 08:12:57,606][0m Trial 0 finished with value: 75.53333282470703 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 0 with value: 75.53333282470703.[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 0.0912, Train: 98.33%, Valid: 78.80% Test: 76.90%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0535, Train: 98.33%, Valid: 76.80% Test: 75.00%
Split: 01, Run: 02
None time:  7.858240395784378
None Run 02:
Highest Train: 100.00
Highest Valid: 80.60
  Final Train: 98.33
   Final Test: 78.00
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 03, Epoch: 100, Loss: 0.1885, Train: 96.67%, Valid: 77.60% Test: 75.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.1082, Train: 98.33%, Valid: 76.80% Test: 75.80%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.073217649012804
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 03, Epoch: 300, Loss: 0.0853, Train: 98.33%, Valid: 76.60% Test: 75.20%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0595, Train: 98.33%, Valid: 77.60% Test: 77.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0716, Train: 98.33%, Valid: 76.20% Test: 75.20%
Split: 01, Run: 03
None time:  8.088140008971095
None Run 03:
Highest Train: 100.00
Highest Valid: 80.40
  Final Train: 98.33
   Final Test: 77.90
total time:  24.247325714677572
None All runs:
Highest Train: 99.44 Â± 0.96
Highest Valid: 80.40 Â± 0.20
  Final Train: 98.33 Â± 0.00
   Final Test: 77.70 Â± 0.44
[32m[I 2021-07-26 08:13:08,055][0m Trial 1 finished with value: 80.39999389648438 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 0 with value: 80.93333435058594.[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 98.33%, Valid: 78.00% Test: 75.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 98.33%, Valid: 77.00% Test: 75.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  7.040788630023599
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 98.33%, Valid: 76.60% Test: 75.80%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 100.00%, Valid: 76.80% Test: 75.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 98.33%, Valid: 75.00% Test: 74.60%
Split: 01, Run: 01
None time:  7.906322089955211
None Run 01:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 98.33
   Final Test: 77.00
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 98.33%, Valid: 76.20% Test: 75.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  7.117158707231283
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  21.295217173174024
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:13:18,906][0m Trial 1 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 1 with value: 81.20000457763672.[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 02, Epoch: 200, Loss: 0.0706, Train: 100.00%, Valid: 73.80% Test: 72.70%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 74.60% Test: 78.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0888, Train: 98.33%, Valid: 76.60% Test: 75.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 76.20% Test: 76.70%
Split: 01, Run: 02, Epoch: 400, Loss: 0.0912, Train: 98.33%, Valid: 76.60% Test: 76.30%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 75.40% Test: 77.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0535, Train: 98.33%, Valid: 74.80% Test: 73.90%
Split: 01, Run: 02
None time:  7.801497980952263
None Run 02:
Highest Train: 100.00
Highest Valid: 79.00
  Final Train: 98.33
   Final Test: 76.60
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 71.80% Test: 74.50%
Split: 01, Run: 03, Epoch: 100, Loss: 0.1885, Train: 98.33%, Valid: 76.00% Test: 74.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 73.40% Test: 75.20%
Split: 01, Run: 01
None time:  7.200712556019425
None Run 01:
Highest Train: 100.00
Highest Valid: 78.00
  Final Train: 100.00
   Final Test: 78.50
Split: 01, Run: 03, Epoch: 200, Loss: 0.1082, Train: 100.00%, Valid: 75.00% Test: 75.30%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 77.20% Test: 78.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0853, Train: 100.00%, Valid: 75.20% Test: 74.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 75.00% Test: 77.40%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0595, Train: 98.33%, Valid: 77.20% Test: 76.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 74.20% Test: 75.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 74.40% Test: 76.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0716, Train: 98.33%, Valid: 75.20% Test: 74.10%
Split: 01, Run: 03
None time:  7.90710704959929
None Run 03:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 98.33
   Final Test: 77.10
total time:  23.80979548767209
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 78.87 Â± 0.42
  Final Train: 98.33 Â± 0.00
   Final Test: 76.90 Â± 0.26
[32m[I 2021-07-26 08:13:31,872][0m Trial 2 finished with value: 78.86666870117188 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 0 with value: 80.93333435058594.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  80.933    {'train': '96.11 Â± 0.96', 'valid': '80.93 Â± 0.12', 'test': '78.00 Â± 0.17'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  80.4    {'train': '98.33 Â± 0.00', 'valid': '80.40 Â± 0.20', 'test': '77.70 Â± 0.44'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  78.867    {'train': '98.33 Â± 0.00', 'valid': '78.87 Â± 0.42', 'test': '76.90 Â± 0.26'}
test_acc
['78.00 Â± 0.17', '77.70 Â± 0.44', '76.90 Â± 0.26']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  80.93333435058594
Best trial Acc:  {'train': '96.11 Â± 0.96', 'valid': '80.93 Â± 0.12', 'test': '78.00 Â± 0.17'}
optuna total time:  74.4813002217561
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 74.20% Test: 76.10%
Split: 01, Run: 02
None time:  7.0558926817029715
None Run 02:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 100.00
   Final Test: 78.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 75.00% Test: 76.60%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 75.20% Test: 76.30%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 74.40% Test: 75.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 74.80% Test: 75.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 72.40% Test: 75.60%
Split: 01, Run: 03
None time:  6.624127935618162
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.60
total time:  20.954619450494647
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 78.07 Â± 0.70
  Final Train: 100.00 Â± 0.00
   Final Test: 78.50 Â± 0.10
[32m[I 2021-07-26 08:13:39,867][0m Trial 2 finished with value: 78.06665802001953 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 1 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  75.533    {'train': '100.00 Â± 0.00', 'valid': '75.53 Â± 0.31', 'test': '75.73 Â± 0.46'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  78.067    {'train': '100.00 Â± 0.00', 'valid': '78.07 Â± 0.70', 'test': '78.50 Â± 0.10'}
test_acc
['75.73 Â± 0.46', '80.43 Â± 0.31', '78.50 Â± 0.10']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  65.73907217383385
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:14:46,860][0m A new study created in memory with name: no-name-c12b81ad-62f3-4e50-9c1a-6f443defe8b6[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='PubMed', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[19717, 19717, nnz=88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])
APPNP(
  (lin1): Linear(in_features=500, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=3, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.1179, Train: 95.00%, Valid: 77.80% Test: 76.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0636, Train: 95.00%, Valid: 78.80% Test: 76.60%
Split: 01, Run: 01, Epoch: 300, Loss: 0.1174, Train: 96.67%, Valid: 78.20% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0698, Train: 93.33%, Valid: 77.80% Test: 75.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0603, Train: 95.00%, Valid: 78.00% Test: 74.90%
Split: 01, Run: 01
None time:  8.418558925390244
None Run 01:
Highest Train: 98.33
Highest Valid: 80.80
  Final Train: 96.67
   Final Test: 78.20
Split: 01, Run: 02, Epoch: 100, Loss: 0.1257, Train: 96.67%, Valid: 78.60% Test: 77.90%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:15:13,977][0m A new study created in memory with name: no-name-fffb70db-2df9-4c2c-a344-256e789e87ab[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.111814387142658
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  6.551014954224229
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  6.76562880165875
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  22.095237152650952
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:15:36,080][0m Trial 0 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
test_acc
['80.43 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  22.107938600704074
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=None, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1, 0.2, 0.3], 'dropout': [0.5], 'K': [10]}
num_trial:  3
[32m[I 2021-07-26 08:15:38,066][0m A new study created in memory with name: no-name-72d3086d-14e4-4972-a0e0-bc003b507932[0m
K:  10
alpha:  0.3
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.3, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.3)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  5.431509887799621
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.879865584895015
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.871993117034435
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  16.798707207664847
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:15:54,873][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}. Best is trial 0 with value: 62.0.[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.781572714447975
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.538346640765667
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.662264229729772
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  14.035703092813492
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:16:08,915][0m Trial 1 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 62.0.[0m
K:  10
alpha:  0.2
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.2, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.2)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  4.5826065838336945
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.506486743688583
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.446857439354062
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  13.595525983721018
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:16:22,516][0m Trial 2 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  3
  Number of pruned trials:  0
  Number of complete trials:  3
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.2, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
test_acc
['57.57 Â± 1.77', '57.57 Â± 1.77', '57.57 Â± 1.77']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.3, 'K': 10}
Best trial Value:  62.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
optuna total time:  44.454623114317656
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:16:46,683][0m A new study created in memory with name: no-name-cea59391-945f-48c5-a95a-9939becdbad4[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 97.14%, Valid: 79.20% Test: 82.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 99.29%, Valid: 78.60% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 100.00%, Valid: 79.00% Test: 82.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 79.00% Test: 83.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 79.20% Test: 82.30%
Split: 01, Run: 01
None time:  14.193781077861786
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 84.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 97.86%, Valid: 79.00% Test: 82.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 79.80% Test: 83.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 80.00% Test: 81.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 99.29%, Valid: 79.80% Test: 82.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 78.60% Test: 82.60%
Split: 01, Run: 02
None time:  13.873707616701722
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 99.29
   Final Test: 83.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 80.40% Test: 82.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 80.00% Test: 83.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 100.00%, Valid: 80.20% Test: 82.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 80.00% Test: 82.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 99.29%, Valid: 80.20% Test: 83.20%
Split: 01, Run: 03
None time:  13.39017983712256
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 99.29
   Final Test: 84.10
total time:  43.085935309529305
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 83.77 Â± 0.58
[32m[I 2021-07-26 08:17:29,778][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.333    {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
test_acc
['83.77 Â± 0.58']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.33333587646484
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '83.77 Â± 0.58'}
optuna total time:  43.09953937865794
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:21:08,083][0m A new study created in memory with name: no-name-c49d9328-51bf-4b59-a178-5cd52b042d7a[0m
lambda1:  0.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.9825, Train: 86.43%, Valid: 85.00% Test: 85.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.8332, Train: 90.71%, Valid: 91.00% Test: 91.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.7991, Train: 92.86%, Valid: 91.20% Test: 91.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.7828, Train: 92.14%, Valid: 91.40% Test: 92.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.7996, Train: 94.29%, Valid: 91.60% Test: 92.00%
Split: 01, Run: 01
None time:  2.900922290980816
None Run 01:
Highest Train: 95.71
Highest Valid: 92.80
  Final Train: 92.14
   Final Test: 91.90
Split: 01, Run: 02, Epoch: 100, Loss: 0.9425, Train: 85.00%, Valid: 84.20% Test: 85.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.8339, Train: 91.43%, Valid: 90.20% Test: 91.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.8020, Train: 90.71%, Valid: 91.40% Test: 91.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.7891, Train: 93.57%, Valid: 92.00% Test: 92.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.7915, Train: 90.71%, Valid: 92.00% Test: 92.20%
Split: 01, Run: 02
None time:  2.755334449931979
None Run 02:
Highest Train: 94.29
Highest Valid: 93.00
  Final Train: 92.86
   Final Test: 92.30
Split: 01, Run: 03, Epoch: 100, Loss: 0.9534, Train: 85.71%, Valid: 86.40% Test: 87.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.8210, Train: 91.43%, Valid: 90.00% Test: 90.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.7832, Train: 92.86%, Valid: 91.40% Test: 92.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.7863, Train: 94.29%, Valid: 92.60% Test: 92.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.7747, Train: 92.86%, Valid: 91.60% Test: 91.90%
Split: 01, Run: 03
None time:  2.755884103477001
None Run 03:
Highest Train: 95.71
Highest Valid: 93.00
  Final Train: 92.14
   Final Test: 91.70
total time:  10.05931225977838
None All runs:
Highest Train: 95.24 Â± 0.82
Highest Valid: 92.93 Â± 0.12
  Final Train: 92.38 Â± 0.41
   Final Test: 91.97 Â± 0.31
[32m[I 2021-07-26 08:21:18,152][0m Trial 0 finished with value: 92.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 92.9333267211914.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}   trial.value:  92.933    {'train': '92.38 Â± 0.41', 'valid': '92.93 Â± 0.12', 'test': '91.97 Â± 0.31'}
test_acc
['91.97 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}
Best trial Value:  92.9333267211914
Best trial Acc:  {'train': '92.38 Â± 0.41', 'valid': '92.93 Â± 0.12', 'test': '91.97 Â± 0.31'}
optuna total time:  10.074366802349687
Traceback (most recent call last):
  File "main_optuna.py", line 9, in <module>
    from train_eval import train, test
ImportError: cannot import name 'train' from 'train_eval' (/mnt/ufs18/home-088/xiaorui/Project/GCN/ElasticGNN-2021-06-10-9pm/code_altopt/train_eval.py)
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:23:42,811][0m A new study created in memory with name: no-name-dc5cb3c1-da73-4702-ae7a-af16b9dff9eb[0m
lambda1:  0.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 60.00% Test: 59.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 58.00% Test: 56.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.60%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.20% Test: 55.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.90%
Split: 01, Run: 01
None time:  3.0132766142487526
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 57.00% Test: 56.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 59.40% Test: 56.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.40% Test: 55.30%
Split: 01, Run: 02
None time:  2.8793165162205696
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.00% Test: 56.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.40% Test: 55.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 55.80% Test: 56.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 57.00% Test: 55.10%
Split: 01, Run: 03
None time:  2.84291978366673
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  10.410604793578386
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:23:53,230][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
test_acc
['57.57 Â± 1.77']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}
Best trial Value:  62.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
optuna total time:  10.42385889030993
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:25:11,845][0m A new study created in memory with name: no-name-49e77c20-4692-4ac7-9c61-3c2b85ace87f[0m
lambda1:  0.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 52.60% Test: 50.10%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 100.00%, Valid: 49.40% Test: 48.20%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 50.80% Test: 49.20%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 100.00%, Valid: 49.40% Test: 46.90%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 50.80% Test: 47.30%
Split: 01, Run: 01
None time:  3.0961385928094387
None Run 01:
Highest Train: 100.00
Highest Valid: 57.60
  Final Train: 100.00
   Final Test: 55.20
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 52.00% Test: 50.20%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 49.40% Test: 48.60%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 49.40% Test: 49.20%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 45.60% Test: 48.30%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 51.40% Test: 48.70%
Split: 01, Run: 02
None time:  2.8374559953808784
None Run 02:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 55.10
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 50.40% Test: 50.70%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 52.20% Test: 50.00%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 49.20% Test: 48.30%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 49.20% Test: 47.90%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 48.40% Test: 47.90%
Split: 01, Run: 03
None time:  2.828627595677972
None Run 03:
Highest Train: 100.00
Highest Valid: 59.20
  Final Train: 100.00
   Final Test: 54.50
total time:  10.417020810768008
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.27 Â± 0.83
  Final Train: 100.00 Â± 0.00
   Final Test: 54.93 Â± 0.38
[32m[I 2021-07-26 08:25:22,270][0m Trial 0 finished with value: 58.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 58.266666412353516.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}   trial.value:  58.267    {'train': '100.00 Â± 0.00', 'valid': '58.27 Â± 0.83', 'test': '54.93 Â± 0.38'}
test_acc
['54.93 Â± 0.38']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}
Best trial Value:  58.266666412353516
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '58.27 Â± 0.83', 'test': '54.93 Â± 0.38'}
optuna total time:  10.42994779907167
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:28:47,688][0m A new study created in memory with name: no-name-7d6b8578-7a2c-421e-a410-7fbac5077f58[0m
lambda1:  0.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 60.00% Test: 59.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 58.00% Test: 56.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.60%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.20% Test: 55.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.90%
Split: 01, Run: 01
None time:  8.279832746833563
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 57.00% Test: 56.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 59.40% Test: 56.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.40% Test: 55.30%
Split: 01, Run: 02
None time:  8.119688596576452
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.00% Test: 56.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.00% Test: 56.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.40% Test: 55.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 55.80% Test: 56.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 57.00% Test: 55.10%
Split: 01, Run: 03
None time:  8.26757944561541
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  26.326916890218854
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:29:14,024][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
test_acc
['57.57 Â± 1.77']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}
Best trial Value:  62.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
optuna total time:  26.341091765090823
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:29:37,110][0m A new study created in memory with name: no-name-69961509-2737-4414-9969-a558d8b5c8a7[0m
lambda1:  0.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 01
None time:  7.962736474350095
None Run 01:
Highest Train: 96.43
Highest Valid: 75.80
  Final Train: 94.29
   Final Test: 75.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 02
None time:  7.7667180970311165
None Run 02:
Highest Train: 96.43
Highest Valid: 75.80
  Final Train: 94.29
   Final Test: 75.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 27.14%, Valid: 11.40% Test: 14.90%
Split: 01, Run: 03
None time:  7.835469301789999
None Run 03:
Highest Train: 96.43
Highest Valid: 75.80
  Final Train: 94.29
   Final Test: 75.10
total time:  25.17860888503492
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 75.80 Â± 0.00
  Final Train: 94.29 Â± 0.00
   Final Test: 75.10 Â± 0.00
[32m[I 2021-07-26 08:30:02,297][0m Trial 0 finished with value: 75.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 75.80000305175781.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}   trial.value:  75.8    {'train': '94.29 Â± 0.00', 'valid': '75.80 Â± 0.00', 'test': '75.10 Â± 0.00'}
test_acc
['75.10 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.0, 'lambda2': 0.0, 'K': 10}
Best trial Value:  75.80000305175781
Best trial Acc:  {'train': '94.29 Â± 0.00', 'valid': '75.80 Â± 0.00', 'test': '75.10 Â± 0.00'}
optuna total time:  25.192012403160334
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  6
[32m[I 2021-07-26 08:30:47,542][0m A new study created in memory with name: no-name-c9a40209-f07d-446d-83b5-f4057c7858f4[0m
lambda1:  0.5
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 72.80% Test: 74.60%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 72.40% Test: 74.30%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 72.80% Test: 75.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 68.20% Test: 72.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 69.80% Test: 72.30%
Split: 01, Run: 01
None time:  7.894414372742176
None Run 01:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 100.00
   Final Test: 75.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 73.40% Test: 74.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 72.80% Test: 73.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 71.40% Test: 73.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 72.20% Test: 72.70%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 71.00% Test: 72.40%
Split: 01, Run: 02
None time:  7.8064068127423525
None Run 02:
Highest Train: 100.00
Highest Valid: 74.20
  Final Train: 100.00
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 69.20% Test: 73.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 71.60% Test: 73.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 71.60% Test: 73.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 71.80% Test: 73.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 72.20% Test: 72.30%
Split: 01, Run: 03
None time:  7.811370767652988
None Run 03:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 100.00
   Final Test: 75.00
total time:  25.14555212855339
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.60 Â± 0.35
  Final Train: 100.00 Â± 0.00
   Final Test: 75.07 Â± 0.60
[32m[I 2021-07-26 08:31:12,695][0m Trial 0 finished with value: 74.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 74.5999984741211.[0m
lambda1:  1.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 69.20% Test: 71.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 68.00% Test: 70.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 69.00% Test: 70.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 65.00% Test: 67.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 67.20% Test: 67.60%
Split: 01, Run: 01
None time:  7.667407907545567
None Run 01:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 70.00% Test: 69.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 69.00% Test: 70.20%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 68.60% Test: 69.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 67.80% Test: 69.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 66.20% Test: 67.60%
Split: 01, Run: 02
None time:  7.608351204544306
None Run 02:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 65.80% Test: 68.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 67.80% Test: 69.30%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 66.80% Test: 69.10%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 68.60% Test: 68.40%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 67.60% Test: 69.40%
Split: 01, Run: 03
None time:  7.605860324576497
None Run 03:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
total time:  22.948502404615283
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.00 Â± 0.00
  Final Train: 96.43 Â± 0.00
   Final Test: 74.50 Â± 0.00
[32m[I 2021-07-26 08:31:35,649][0m Trial 1 finished with value: 72.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 74.5999984741211.[0m
lambda1:  0.2
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 75.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 77.80% Test: 79.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 73.80% Test: 76.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 75.00% Test: 76.90%
Split: 01, Run: 01
None time:  7.685481401160359
None Run 01:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 100.00
   Final Test: 77.60
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 77.40% Test: 78.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 77.80% Test: 78.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 76.40% Test: 77.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 75.40% Test: 76.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 75.60% Test: 76.50%
Split: 01, Run: 02
None time:  7.5828891787678
None Run 02:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 74.80% Test: 77.00%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 75.60% Test: 77.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 77.60% Test: 77.80%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 75.80% Test: 77.70%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 76.90%
Split: 01, Run: 03
None time:  7.604980241507292
None Run 03:
Highest Train: 100.00
Highest Valid: 78.60
  Final Train: 100.00
   Final Test: 79.30
total time:  22.925064580515027
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 78.73 Â± 0.42
  Final Train: 100.00 Â± 0.00
   Final Test: 78.50 Â± 0.85
[32m[I 2021-07-26 08:31:58,580][0m Trial 2 finished with value: 78.73333740234375 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}. Best is trial 2 with value: 78.73333740234375.[0m
lambda1:  0.4
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 73.40% Test: 76.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 74.80% Test: 74.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 75.20% Test: 76.70%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 70.00% Test: 73.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 71.40% Test: 73.30%
Split: 01, Run: 01
None time:  7.656738629564643
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 76.60
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 74.40% Test: 75.60%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 73.60% Test: 74.60%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 73.00% Test: 74.70%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 72.40% Test: 73.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 72.60% Test: 74.10%
Split: 01, Run: 02
None time:  7.622155452147126
None Run 02:
Highest Train: 100.00
Highest Valid: 75.80
  Final Train: 100.00
   Final Test: 76.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 70.40% Test: 73.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 73.60% Test: 75.10%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 74.60% Test: 75.10%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 72.40% Test: 74.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 73.60% Test: 73.20%
Split: 01, Run: 03
None time:  7.61775042116642
None Run 03:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 100.00
   Final Test: 76.30
total time:  22.9529319293797
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.27 Â± 0.50
  Final Train: 100.00 Â± 0.00
   Final Test: 76.33 Â± 0.25
[32m[I 2021-07-26 08:32:21,538][0m Trial 3 finished with value: 76.26667022705078 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.0, 'K': 10}. Best is trial 2 with value: 78.73333740234375.[0m
lambda1:  0.1
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 99.29%, Valid: 78.20% Test: 81.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 99.29%, Valid: 80.20% Test: 81.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 98.57%, Valid: 80.80% Test: 81.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 98.57%, Valid: 76.00% Test: 78.00%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 78.20% Test: 79.50%
Split: 01, Run: 01
None time:  7.593161737546325
None Run 01:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 100.00
   Final Test: 81.30
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.20% Test: 80.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 80.80% Test: 81.10%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 79.00% Test: 80.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 98.57%, Valid: 78.20% Test: 79.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 02
None time:  7.587299361824989
None Run 02:
Highest Train: 100.00
Highest Valid: 81.80
  Final Train: 99.29
   Final Test: 80.60
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 77.80% Test: 79.00%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 98.57%, Valid: 78.60% Test: 79.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 99.29%, Valid: 79.00% Test: 80.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 98.57%, Valid: 76.80% Test: 78.90%
Split: 01, Run: 03
None time:  7.611093316227198
None Run 03:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 99.29
   Final Test: 80.50
total time:  22.844183726236224
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.00 Â± 0.53
  Final Train: 99.52 Â± 0.41
   Final Test: 80.80 Â± 0.44
[32m[I 2021-07-26 08:32:44,388][0m Trial 4 finished with value: 82.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}. Best is trial 4 with value: 82.0.[0m
lambda1:  0.3
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 74.60% Test: 77.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 76.00% Test: 76.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 76.80% Test: 78.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 71.80% Test: 74.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 73.20% Test: 74.90%
Split: 01, Run: 01
None time:  7.678180389106274
None Run 01:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 77.90
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 75.60% Test: 77.40%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 76.20% Test: 76.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 74.80% Test: 76.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 74.00% Test: 75.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 73.80% Test: 75.50%
Split: 01, Run: 02
None time:  7.628912957385182
None Run 02:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 100.00
   Final Test: 76.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 72.00% Test: 75.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 74.00% Test: 76.20%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 77.20% Test: 76.70%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 73.80% Test: 75.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 73.80% Test: 75.50%
Split: 01, Run: 03
None time:  7.611621238291264
None Run 03:
Highest Train: 100.00
Highest Valid: 77.60
  Final Train: 100.00
   Final Test: 77.40
total time:  22.9669891577214
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.53 Â± 0.31
  Final Train: 100.00 Â± 0.00
   Final Test: 77.13 Â± 0.93
[32m[I 2021-07-26 08:33:07,360][0m Trial 5 finished with value: 77.53333282470703 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.0, 'K': 10}. Best is trial 4 with value: 82.0.[0m
Study statistics: 
  Number of finished trials:  6
  Number of pruned trials:  0
  Number of complete trials:  6
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}   trial.value:  82    {'train': '99.52 Â± 0.41', 'valid': '82.00 Â± 0.53', 'test': '80.80 Â± 0.44'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}   trial.value:  78.733    {'train': '100.00 Â± 0.00', 'valid': '78.73 Â± 0.42', 'test': '78.50 Â± 0.85'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.0, 'K': 10}   trial.value:  77.533    {'train': '100.00 Â± 0.00', 'valid': '77.53 Â± 0.31', 'test': '77.13 Â± 0.93'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.0, 'K': 10}   trial.value:  76.267    {'train': '100.00 Â± 0.00', 'valid': '76.27 Â± 0.50', 'test': '76.33 Â± 0.25'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.0, 'K': 10}   trial.value:  74.6    {'train': '100.00 Â± 0.00', 'valid': '74.60 Â± 0.35', 'test': '75.07 Â± 0.60'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 0.0, 'K': 10}   trial.value:  72    {'train': '96.43 Â± 0.00', 'valid': '72.00 Â± 0.00', 'test': '74.50 Â± 0.00'}
test_acc
['80.80 Â± 0.44', '78.50 Â± 0.85', '77.13 Â± 0.93', '76.33 Â± 0.25', '75.07 Â± 0.60', '74.50 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}
Best trial Value:  82.0
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '82.00 Â± 0.53', 'test': '80.80 Â± 0.44'}
optuna total time:  139.82497918605804
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  6
[32m[I 2021-07-26 08:34:06,355][0m A new study created in memory with name: no-name-c00a0ca0-bdc6-4626-b045-c74b1c856f2e[0m
lambda1:  0.2
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 99.29%, Valid: 71.00% Test: 71.70%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 99.29%, Valid: 72.20% Test: 71.00%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 72.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 99.29%, Valid: 70.80% Test: 69.50%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 72.00% Test: 72.20%
Split: 01, Run: 01
None time:  8.37009309977293
None Run 01:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 74.40
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 70.60% Test: 71.80%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 69.00% Test: 69.30%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 74.20% Test: 73.00%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 72.00% Test: 71.00%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 73.20% Test: 73.40%
Split: 01, Run: 02
None time:  7.9661016296595335
None Run 02:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 99.29
   Final Test: 76.40
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 69.60% Test: 70.80%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 73.60% Test: 75.00%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 72.60% Test: 73.60%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 73.80% Test: 73.10%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 72.40% Test: 73.50%
Split: 01, Run: 03
None time:  7.986407263204455
None Run 03:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 78.20
total time:  25.94987055659294
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.20 Â± 0.72
  Final Train: 99.76 Â± 0.41
   Final Test: 76.33 Â± 1.90
[32m[I 2021-07-26 08:34:32,314][0m Trial 0 finished with value: 77.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 77.20000457763672.[0m
lambda1:  1.0
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=1.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 60.60% Test: 61.10%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 100.00%, Valid: 60.00% Test: 60.20%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 61.40% Test: 62.40%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 100.00%, Valid: 59.20% Test: 58.70%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 60.20% Test: 61.10%
Split: 01, Run: 01
None time:  7.942759700119495
None Run 01:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 60.80% Test: 61.10%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 58.20% Test: 57.40%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 62.40% Test: 61.70%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 57.40% Test: 60.10%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 62.40% Test: 61.90%
Split: 01, Run: 02
None time:  7.897978823632002
None Run 02:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 60.40% Test: 60.60%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 62.00% Test: 62.50%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 60.00% Test: 60.60%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 61.60% Test: 60.10%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 60.80% Test: 60.70%
Split: 01, Run: 03
None time:  7.885956021025777
None Run 03:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
total time:  23.79102511703968
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.00 Â± 0.00
  Final Train: 96.43 Â± 0.00
   Final Test: 74.50 Â± 0.00
[32m[I 2021-07-26 08:34:56,110][0m Trial 1 finished with value: 72.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 77.20000457763672.[0m
lambda1:  0.3
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 68.40% Test: 70.00%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 99.29%, Valid: 68.80% Test: 68.90%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 71.60% Test: 72.20%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 99.29%, Valid: 68.00% Test: 67.20%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 69.60% Test: 69.50%
Split: 01, Run: 01
None time:  7.94631283916533
None Run 01:
Highest Train: 100.00
Highest Valid: 75.20
  Final Train: 100.00
   Final Test: 76.20
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 68.40% Test: 69.30%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 67.00% Test: 67.20%
Using backend: pytorch
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 71.20% Test: 70.60%
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.05], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:35:09,683][0m A new study created in memory with name: no-name-de2d55bd-9b87-4ec2-b9d6-65365903191d[0m
lambda1:  0.05
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 68.20% Test: 68.90%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 71.60% Test: 70.40%
Split: 01, Run: 02
None time:  8.44390626437962
None Run 02:
Highest Train: 100.00
Highest Valid: 75.40
  Final Train: 99.29
   Final Test: 75.50
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 96.43%, Valid: 78.80% Test: 81.70%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 67.80% Test: 68.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 97.86%, Valid: 81.00% Test: 81.70%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 72.20% Test: 72.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.14%, Valid: 79.60% Test: 81.10%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 97.14%, Valid: 77.00% Test: 77.90%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 70.20% Test: 70.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 97.86%, Valid: 78.20% Test: 79.90%
Split: 01, Run: 01
None time:  8.051298236474395
None Run 01:
Highest Train: 98.57
Highest Valid: 81.80
  Final Train: 98.57
   Final Test: 82.00
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 71.00% Test: 70.60%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 95.00%, Valid: 79.80% Test: 80.60%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 69.60% Test: 70.20%
Split: 01, Run: 03
None time:  8.810252666473389
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 100.00
   Final Test: 76.00
total time:  25.25236035324633
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 75.53 Â± 0.42
  Final Train: 99.76 Â± 0.41
   Final Test: 75.90 Â± 0.36
[32m[I 2021-07-26 08:35:21,369][0m Trial 2 finished with value: 75.53333282470703 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 77.20000457763672.[0m
lambda1:  0.5
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.5, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 97.14%, Valid: 80.80% Test: 80.90%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 64.40% Test: 66.60%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 95.00%, Valid: 79.40% Test: 80.60%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 100.00%, Valid: 65.60% Test: 65.40%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 97.14%, Valid: 80.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 66.80% Test: 67.90%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 97.86%, Valid: 79.40% Test: 81.00%
Split: 01, Run: 02
None time:  8.022634077817202
None Run 02:
Highest Train: 98.57
Highest Valid: 81.80
  Final Train: 97.86
   Final Test: 82.60
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 100.00%, Valid: 62.60% Test: 63.40%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 95.00%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 65.40% Test: 66.60%
Split: 01, Run: 01
None time:  7.90534651465714
None Run 01:
Highest Train: 100.00
Highest Valid: 72.40
  Final Train: 100.00
   Final Test: 72.50
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 97.14%, Valid: 79.60% Test: 80.10%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 64.60% Test: 66.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 97.86%, Valid: 81.40% Test: 82.50%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 62.80% Test: 63.10%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 98.57%, Valid: 80.00% Test: 81.50%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 67.00% Test: 67.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 97.14%, Valid: 79.40% Test: 79.40%
Split: 01, Run: 03
None time:  7.9792044926434755
None Run 03:
Highest Train: 98.57
Highest Valid: 82.40
  Final Train: 97.86
   Final Test: 82.10
total time:  25.618063416332006
None All runs:
Highest Train: 98.57 Â± 0.00
Highest Valid: 82.00 Â± 0.35
  Final Train: 98.10 Â± 0.41
   Final Test: 82.23 Â± 0.32
[32m[I 2021-07-26 08:35:35,315][0m Trial 0 finished with value: 82.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 82.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}   trial.value:  82    {'train': '98.10 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '82.23 Â± 0.32'}
test_acc
['82.23 Â± 0.32']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}
Best trial Value:  82.0
Best trial Acc:  {'train': '98.10 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '82.23 Â± 0.32'}
optuna total time:  25.636476507410407
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 63.40% Test: 65.80%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 67.60% Test: 68.00%
Split: 01, Run: 02
None time:  7.888049349188805
None Run 02:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 96.43
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 63.60% Test: 66.40%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 68.20% Test: 67.50%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 65.00% Test: 67.30%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 66.80% Test: 66.80%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 65.00% Test: 67.20%
Split: 01, Run: 03
None time:  7.89527971483767
None Run 03:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 72.60
total time:  23.748430397361517
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.47 Â± 0.50
  Final Train: 98.81 Â± 2.06
   Final Test: 73.20 Â± 1.13
[32m[I 2021-07-26 08:35:45,123][0m Trial 3 finished with value: 72.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 77.20000457763672.[0m
lambda1:  0.1
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 98.57%, Valid: 74.20% Test: 73.90%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 98.57%, Valid: 74.60% Test: 73.50%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 99.29%, Valid: 75.80% Test: 75.70%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 98.57%, Valid: 73.40% Test: 72.50%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 98.57%, Valid: 75.80% Test: 74.90%
Split: 01, Run: 01
None time:  7.856436982750893
None Run 01:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 99.29
   Final Test: 75.80
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 73.40% Test: 74.50%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 71.40% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 77.20% Test: 75.20%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 74.60% Test: 73.30%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 77.20% Test: 77.10%
Split: 01, Run: 02
None time:  7.956715155392885
None Run 02:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 97.86
   Final Test: 79.40
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 97.86%, Valid: 72.80% Test: 73.40%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 77.40% Test: 78.30%
Using backend: pytorch
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 99.29%, Valid: 76.00% Test: 76.80%
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 08:36:06,505][0m A new study created in memory with name: no-name-ec0772e4-1c80-4df5-8f46-15361f6fc2e8[0m
lambda1:  0.01
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 77.40% Test: 76.20%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 03
None time:  8.04968211427331
None Run 03:
Highest Train: 100.00
Highest Valid: 79.60
  Final Train: 99.29
   Final Test: 80.40
total time:  23.91392765752971
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.33 Â± 0.23
  Final Train: 98.81 Â± 0.82
   Final Test: 78.53 Â± 2.42
[32m[I 2021-07-26 08:36:09,042][0m Trial 4 finished with value: 79.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}. Best is trial 4 with value: 79.33333587646484.[0m
lambda1:  0.4
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.4, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 87.14%, Valid: 75.00% Test: 77.70%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 100.00%, Valid: 66.40% Test: 68.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 84.29%, Valid: 76.80% Test: 77.80%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 99.29%, Valid: 66.80% Test: 67.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 85.71%, Valid: 76.00% Test: 77.80%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 69.20% Test: 69.90%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 88.57%, Valid: 77.00% Test: 76.40%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 100.00%, Valid: 64.60% Test: 65.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 90.00%, Valid: 77.20% Test: 77.30%
Split: 01, Run: 01
None time:  8.157543510198593
None Run 01:
Highest Train: 96.43
Highest Valid: 79.80
  Final Train: 90.00
   Final Test: 80.30
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 81.43%, Valid: 70.40% Test: 73.80%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 67.00% Test: 68.30%
Split: 01, Run: 01
None time:  8.824031848460436
None Run 01:
Highest Train: 100.00
Highest Valid: 73.80
  Final Train: 100.00
   Final Test: 75.20
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 87.86%, Valid: 76.40% Test: 76.20%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 66.80% Test: 67.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 84.29%, Valid: 76.60% Test: 77.40%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 64.60% Test: 65.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 85.00%, Valid: 75.00% Test: 77.40%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 69.00% Test: 69.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 90.00%, Valid: 76.80% Test: 77.40%
Split: 01, Run: 02
None time:  7.994787182658911
None Run 02:
Highest Train: 96.43
Highest Valid: 78.00
  Final Train: 90.00
   Final Test: 77.60
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 65.80% Test: 67.00%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 84.29%, Valid: 76.80% Test: 77.50%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 70.00% Test: 69.60%
Split: 01, Run: 02
None time:  8.796952461823821
None Run 02:
Highest Train: 100.00
Highest Valid: 73.80
  Final Train: 99.29
   Final Test: 74.10
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 82.86%, Valid: 75.40% Test: 77.10%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 65.00% Test: 66.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 85.71%, Valid: 78.00% Test: 78.30%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 70.60% Test: 69.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 87.86%, Valid: 77.60% Test: 78.50%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 67.40% Test: 69.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 86.43%, Valid: 76.40% Test: 76.90%
Split: 01, Run: 03
None time:  8.002997597679496
None Run 03:
Highest Train: 96.43
Highest Valid: 78.60
  Final Train: 87.86
   Final Test: 79.20
total time:  25.818718791007996
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 78.80 Â± 0.92
  Final Train: 89.29 Â± 1.24
   Final Test: 79.03 Â± 1.36
[32m[I 2021-07-26 08:36:32,332][0m Trial 0 finished with value: 78.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 78.79999542236328.[0m
lambda1:  0.05
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 69.20% Test: 69.10%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 96.43%, Valid: 78.80% Test: 81.70%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 66.40% Test: 68.00%
Split: 01, Run: 03
None time:  8.804364774376154
None Run 03:
Highest Train: 100.00
Highest Valid: 74.40
  Final Train: 100.00
   Final Test: 74.10
total time:  26.486014334484935
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.00 Â± 0.35
  Final Train: 99.76 Â± 0.41
   Final Test: 74.47 Â± 0.64
[32m[I 2021-07-26 08:36:35,534][0m Trial 5 finished with value: 74.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.0, 'K': 10}. Best is trial 4 with value: 79.33333587646484.[0m
Study statistics: 
  Number of finished trials:  6
  Number of pruned trials:  0
  Number of complete trials:  6
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}   trial.value:  79.333    {'train': '98.81 Â± 0.82', 'valid': '79.33 Â± 0.23', 'test': '78.53 Â± 2.42'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}   trial.value:  77.2    {'train': '99.76 Â± 0.41', 'valid': '77.20 Â± 0.72', 'test': '76.33 Â± 1.90'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.3, 'lambda2': 0.0, 'K': 10}   trial.value:  75.533    {'train': '99.76 Â± 0.41', 'valid': '75.53 Â± 0.42', 'test': '75.90 Â± 0.36'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.4, 'lambda2': 0.0, 'K': 10}   trial.value:  74    {'train': '99.76 Â± 0.41', 'valid': '74.00 Â± 0.35', 'test': '74.47 Â± 0.64'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.5, 'lambda2': 0.0, 'K': 10}   trial.value:  72.467    {'train': '98.81 Â± 2.06', 'valid': '72.47 Â± 0.50', 'test': '73.20 Â± 1.13'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 1.0, 'lambda2': 0.0, 'K': 10}   trial.value:  72    {'train': '96.43 Â± 0.00', 'valid': '72.00 Â± 0.00', 'test': '74.50 Â± 0.00'}
test_acc
['78.53 Â± 2.42', '76.33 Â± 1.90', '75.90 Â± 0.36', '74.47 Â± 0.64', '73.20 Â± 1.13', '74.50 Â± 0.00']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}
Best trial Value:  79.33333587646484
Best trial Acc:  {'train': '98.81 Â± 0.82', 'valid': '79.33 Â± 0.23', 'test': '78.53 Â± 2.42'}
optuna total time:  149.18616837635636
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 97.86%, Valid: 81.00% Test: 81.70%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.14%, Valid: 79.60% Test: 81.10%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 97.14%, Valid: 77.00% Test: 77.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 97.86%, Valid: 78.20% Test: 79.90%
Split: 01, Run: 01
None time:  7.900246659293771
None Run 01:
Highest Train: 98.57
Highest Valid: 81.80
  Final Train: 98.57
   Final Test: 82.00
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 95.00%, Valid: 79.80% Test: 80.60%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 97.14%, Valid: 80.80% Test: 80.90%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 95.00%, Valid: 79.40% Test: 80.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 97.14%, Valid: 80.40% Test: 80.20%
Using backend: pytorch
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 97.86%, Valid: 79.40% Test: 81.00%
Split: 01, Run: 02
None time:  7.941864218562841
None Run 02:
Highest Train: 98.57
Highest Valid: 81.80
  Final Train: 97.86
   Final Test: 82.60
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 08:36:48,904][0m A new study created in memory with name: no-name-41adad2b-4d70-4dc4-a45a-3dc31b06ce6d[0m
lambda1:  0.01
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 95.00%, Valid: 79.40% Test: 80.20%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 97.14%, Valid: 79.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 70.71%, Valid: 56.80% Test: 57.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 97.86%, Valid: 81.40% Test: 82.50%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 78.57%, Valid: 62.20% Test: 62.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 98.57%, Valid: 80.00% Test: 81.50%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 90.00%, Valid: 74.60% Test: 76.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 97.14%, Valid: 79.40% Test: 79.40%
Split: 01, Run: 03
None time:  7.9001352693885565
None Run 03:
Highest Train: 98.57
Highest Valid: 82.40
  Final Train: 97.86
   Final Test: 82.10
total time:  23.811875754967332
None All runs:
Highest Train: 98.57 Â± 0.00
Highest Valid: 82.00 Â± 0.35
  Final Train: 98.10 Â± 0.41
   Final Test: 82.23 Â± 0.32
[32m[I 2021-07-26 08:36:56,149][0m Trial 1 finished with value: 82.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 82.0.[0m
lambda1:  0.2
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 90.00%, Valid: 73.60% Test: 75.00%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 75.60% Test: 78.40%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 89.29%, Valid: 73.20% Test: 73.40%
Split: 01, Run: 01
None time:  8.341956857591867
None Run 01:
Highest Train: 96.43
Highest Valid: 78.40
  Final Train: 92.86
   Final Test: 78.10
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 87.14%, Valid: 69.80% Test: 67.30%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 77.80% Test: 79.20%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 88.57%, Valid: 71.20% Test: 70.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 73.80% Test: 76.70%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 92.86%, Valid: 74.00% Test: 73.80%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 75.00% Test: 76.90%
Split: 01, Run: 01
None time:  7.973204897716641
None Run 01:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 100.00
   Final Test: 77.60
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 90.00%, Valid: 70.00% Test: 70.00%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 77.40% Test: 78.10%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 94.29%, Valid: 77.40% Test: 76.10%
Split: 01, Run: 02
None time:  8.091929633170366
None Run 02:
Highest Train: 96.43
Highest Valid: 79.00
  Final Train: 91.43
   Final Test: 79.90
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 77.80% Test: 78.40%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 74.29%, Valid: 67.80% Test: 67.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 76.40% Test: 77.10%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 73.57%, Valid: 70.00% Test: 70.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 75.40% Test: 76.50%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 75.71%, Valid: 68.40% Test: 69.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 75.60% Test: 76.50%
Split: 01, Run: 02
None time:  7.949748275801539
None Run 02:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 80.00%, Valid: 69.20% Test: 71.50%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 74.80% Test: 77.00%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 85.00%, Valid: 71.40% Test: 72.90%
Split: 01, Run: 03
None time:  8.10152236931026
None Run 03:
Highest Train: 96.43
Highest Valid: 77.20
  Final Train: 90.00
   Final Test: 76.60
total time:  26.25055013783276
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 78.20 Â± 0.92
  Final Train: 91.43 Â± 1.43
   Final Test: 78.20 Â± 1.65
[32m[I 2021-07-26 08:37:15,163][0m Trial 0 finished with value: 78.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.0, 'K': 10}. Best is trial 0 with value: 78.20000457763672.[0m
lambda1:  0.05
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 75.60% Test: 77.70%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 97.14%, Valid: 74.00% Test: 74.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 77.60% Test: 77.80%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 95.71%, Valid: 73.00% Test: 73.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 75.80% Test: 77.70%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 97.86%, Valid: 77.40% Test: 76.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 76.90%
Split: 01, Run: 03
None time:  7.9682267885655165
None Run 03:
Highest Train: 100.00
Highest Valid: 78.60
  Final Train: 100.00
   Final Test: 79.30
total time:  23.953239472582936
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 78.73 Â± 0.42
  Final Train: 100.00 Â± 0.00
   Final Test: 78.50 Â± 0.85
[32m[I 2021-07-26 08:37:20,108][0m Trial 2 finished with value: 78.73333740234375 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 82.0.[0m
lambda1:  0.02
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 98.57%, Valid: 75.80% Test: 72.80%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 93.57%, Valid: 78.80% Test: 79.40%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 97.86%, Valid: 77.40% Test: 75.20%
Split: 01, Run: 01
None time:  8.021797036752105
None Run 01:
Highest Train: 99.29
Highest Valid: 80.80
  Final Train: 99.29
   Final Test: 77.10
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 92.86%, Valid: 78.00% Test: 79.50%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 98.57%, Valid: 75.60% Test: 74.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 91.43%, Valid: 77.40% Test: 78.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 90.71%, Valid: 77.40% Test: 77.20%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 98.57%, Valid: 72.80% Test: 73.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 94.29%, Valid: 78.20% Test: 79.40%
Split: 01, Run: 01
None time:  7.908908616751432
None Run 01:
Highest Train: 96.43
Highest Valid: 81.40
  Final Train: 94.29
   Final Test: 82.30
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 98.57%, Valid: 79.40% Test: 77.20%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 89.29%, Valid: 75.20% Test: 78.60%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 98.57%, Valid: 75.20% Test: 74.70%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 92.14%, Valid: 79.00% Test: 77.60%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 79.40% Test: 78.50%
Split: 01, Run: 02
None time:  8.00476941280067
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 98.57
   Final Test: 78.10
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 92.14%, Valid: 77.80% Test: 78.70%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 95.71%, Valid: 73.60% Test: 74.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 91.43%, Valid: 77.20% Test: 78.30%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 96.43%, Valid: 78.80% Test: 78.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 94.29%, Valid: 79.60% Test: 79.30%
Split: 01, Run: 02
None time:  7.917782716453075
None Run 02:
Highest Train: 96.43
Highest Valid: 80.00
  Final Train: 93.57
   Final Test: 80.40
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 97.14%, Valid: 76.00% Test: 76.70%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 90.00%, Valid: 76.00% Test: 78.80%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 97.86%, Valid: 80.00% Test: 78.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 92.14%, Valid: 77.40% Test: 79.60%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 98.57%, Valid: 79.40% Test: 78.70%
Split: 01, Run: 03
None time:  7.917295368388295
None Run 03:
Highest Train: 99.29
Highest Valid: 81.40
  Final Train: 97.86
   Final Test: 78.90
total time:  24.011651759967208
None All runs:
Highest Train: 99.52 Â± 0.41
Highest Valid: 81.13 Â± 0.31
  Final Train: 98.57 Â± 0.71
   Final Test: 78.03 Â± 0.90
[32m[I 2021-07-26 08:37:39,180][0m Trial 1 finished with value: 81.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 81.13333129882812.[0m
lambda1:  0.2
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 92.86%, Valid: 79.40% Test: 80.90%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 99.29%, Valid: 71.00% Test: 71.70%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 92.86%, Valid: 80.00% Test: 80.20%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 99.29%, Valid: 72.20% Test: 71.00%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 90.71%, Valid: 77.60% Test: 78.00%
Split: 01, Run: 03
None time:  7.9280443377792835
None Run 03:
Highest Train: 96.43
Highest Valid: 81.20
  Final Train: 93.57
   Final Test: 79.30
total time:  23.817719344049692
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 80.87 Â± 0.76
  Final Train: 93.81 Â± 0.41
   Final Test: 80.67 Â± 1.52
[32m[I 2021-07-26 08:37:43,931][0m Trial 3 finished with value: 80.86666107177734 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 82.0.[0m
lambda1:  0.1
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 100.00%, Valid: 72.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 99.29%, Valid: 78.20% Test: 81.00%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 99.29%, Valid: 70.80% Test: 69.50%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 99.29%, Valid: 80.20% Test: 81.80%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 100.00%, Valid: 72.00% Test: 72.20%
Split: 01, Run: 01
None time:  7.955982031300664
None Run 01:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 74.40
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 98.57%, Valid: 80.80% Test: 81.00%
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 70.60% Test: 71.80%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 98.57%, Valid: 76.00% Test: 78.00%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 69.00% Test: 69.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 78.20% Test: 79.50%
Split: 01, Run: 01
None time:  7.893745945766568
None Run 01:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 100.00
   Final Test: 81.30
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 74.20% Test: 73.00%
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.20% Test: 80.50%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 72.00% Test: 71.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 80.80% Test: 81.10%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 73.20% Test: 73.40%
Split: 01, Run: 02
None time:  7.902303559705615
None Run 02:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 99.29
   Final Test: 76.40
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 79.00% Test: 80.30%
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 100.00%, Valid: 69.60% Test: 70.80%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 98.57%, Valid: 78.20% Test: 79.20%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 73.60% Test: 75.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.60%
Split: 01, Run: 02
None time:  7.847378192469478
None Run 02:
Highest Train: 100.00
Highest Valid: 81.80
  Final Train: 99.29
   Final Test: 80.60
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 100.00%, Valid: 72.60% Test: 73.60%
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 77.80% Test: 79.00%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 73.80% Test: 73.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 98.57%, Valid: 78.60% Test: 79.70%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 72.40% Test: 73.50%
Split: 01, Run: 03
None time:  7.910472244024277
None Run 03:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 78.20
total time:  23.830110697075725
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.20 Â± 0.72
  Final Train: 99.76 Â± 0.41
   Final Test: 76.33 Â± 1.90
[32m[I 2021-07-26 08:38:03,015][0m Trial 2 finished with value: 77.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 81.13333129882812.[0m
lambda1:  0.02
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 89.29%, Valid: 70.40% Test: 70.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 99.29%, Valid: 79.00% Test: 80.50%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 87.86%, Valid: 70.40% Test: 71.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 98.57%, Valid: 76.80% Test: 78.90%
Split: 01, Run: 03
None time:  7.987915989011526
None Run 03:
Highest Train: 100.00
Highest Valid: 82.60
  Final Train: 99.29
   Final Test: 80.50
total time:  23.78693239763379
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.00 Â± 0.53
  Final Train: 99.52 Â± 0.41
   Final Test: 80.80 Â± 0.44
[32m[I 2021-07-26 08:38:07,724][0m Trial 4 finished with value: 82.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 82.0.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.0, 'K': 10}   trial.value:  78.8    {'train': '89.29 Â± 1.24', 'valid': '78.80 Â± 0.92', 'test': '79.03 Â± 1.36'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.0, 'K': 10}   trial.value:  80.867    {'train': '93.81 Â± 0.41', 'valid': '80.87 Â± 0.76', 'test': '80.67 Â± 1.52'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}   trial.value:  82    {'train': '98.10 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '82.23 Â± 0.32'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}   trial.value:  82    {'train': '99.52 Â± 0.41', 'valid': '82.00 Â± 0.53', 'test': '80.80 Â± 0.44'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}   trial.value:  78.733    {'train': '100.00 Â± 0.00', 'valid': '78.73 Â± 0.42', 'test': '78.50 Â± 0.85'}
test_acc
['79.03 Â± 1.36', '80.67 Â± 1.52', '82.23 Â± 0.32', '80.80 Â± 0.44', '78.50 Â± 0.85']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}
Best trial Value:  82.0
Best trial Acc:  {'train': '98.10 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '82.23 Â± 0.32'}
optuna total time:  121.22476105205715
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 94.29%, Valid: 77.00% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 95.00%, Valid: 75.40% Test: 74.90%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 93.57%, Valid: 77.00% Test: 75.10%
Split: 01, Run: 01
None time:  8.010964082553983
None Run 01:
Highest Train: 97.14
Highest Valid: 79.40
  Final Train: 95.71
   Final Test: 78.20
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 92.14%, Valid: 74.00% Test: 72.50%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 92.14%, Valid: 74.00% Test: 72.90%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 95.71%, Valid: 78.00% Test: 75.50%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 93.57%, Valid: 74.40% Test: 73.70%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 97.14%, Valid: 80.40% Test: 78.20%
Split: 01, Run: 02
None time:  7.905618738383055
None Run 02:
Highest Train: 98.57
Highest Valid: 80.40
  Final Train: 97.14
   Final Test: 78.20
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 90.00%, Valid: 71.60% Test: 72.50%
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 87.86%, Valid: 73.20% Test: 74.50%
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 91.43%, Valid: 74.00% Test: 74.00%
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 95.00%, Valid: 76.40% Test: 77.20%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 95.71%, Valid: 79.60% Test: 78.20%
Split: 01, Run: 03
None time:  7.910564599558711
None Run 03:
Highest Train: 96.43
Highest Valid: 79.60
  Final Train: 95.71
   Final Test: 78.20
total time:  23.884780902415514
None All runs:
Highest Train: 97.38 Â± 1.09
Highest Valid: 79.80 Â± 0.53
  Final Train: 96.19 Â± 0.82
   Final Test: 78.20 Â± 0.00
[32m[I 2021-07-26 08:38:26,905][0m Trial 3 finished with value: 79.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 81.13333129882812.[0m
lambda1:  0.1
lambda2:  0.0
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 31.0937, Train: 98.57%, Valid: 74.20% Test: 73.90%
Split: 01, Run: 01, Epoch: 200, Loss: 28.2629, Train: 98.57%, Valid: 74.60% Test: 73.50%
Split: 01, Run: 01, Epoch: 300, Loss: 25.8835, Train: 99.29%, Valid: 75.80% Test: 75.70%
Split: 01, Run: 01, Epoch: 400, Loss: 23.8215, Train: 98.57%, Valid: 73.40% Test: 72.50%
Split: 01, Run: 01, Epoch: 500, Loss: 21.8606, Train: 98.57%, Valid: 75.80% Test: 74.90%
Split: 01, Run: 01
None time:  7.916459809988737
None Run 01:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 99.29
   Final Test: 75.80
Split: 01, Run: 02, Epoch: 100, Loss: 31.8852, Train: 100.00%, Valid: 73.40% Test: 74.50%
Split: 01, Run: 02, Epoch: 200, Loss: 26.5518, Train: 100.00%, Valid: 71.40% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 20.1456, Train: 100.00%, Valid: 77.20% Test: 75.20%
Split: 01, Run: 02, Epoch: 400, Loss: 20.6496, Train: 100.00%, Valid: 74.60% Test: 73.30%
Split: 01, Run: 02, Epoch: 500, Loss: 23.7542, Train: 100.00%, Valid: 77.20% Test: 77.10%
Split: 01, Run: 02
None time:  7.985736973583698
None Run 02:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 97.86
   Final Test: 79.40
Split: 01, Run: 03, Epoch: 100, Loss: 30.0453, Train: 97.86%, Valid: 72.80% Test: 73.40%
Using backend: pytorch
Split: 01, Run: 03, Epoch: 200, Loss: 26.8750, Train: 100.00%, Valid: 77.40% Test: 78.30%
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.1], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 08:38:46,898][0m A new study created in memory with name: no-name-e1450c5f-6db1-466f-af21-78b6254bec95[0m
lambda1:  0.02
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 03, Epoch: 300, Loss: 28.8734, Train: 99.29%, Valid: 76.00% Test: 76.80%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 03, Epoch: 400, Loss: 23.0312, Train: 100.00%, Valid: 77.40% Test: 76.20%
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 79.40% Test: 80.90%
Split: 01, Run: 03, Epoch: 500, Loss: 22.4900, Train: 100.00%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 03
None time:  8.221018601208925
None Run 03:
Highest Train: 100.00
Highest Valid: 79.60
  Final Train: 99.29
   Final Test: 80.40
total time:  24.175229489803314
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.33 Â± 0.23
  Final Train: 98.81 Â± 0.82
   Final Test: 78.53 Â± 2.42
[32m[I 2021-07-26 08:38:51,086][0m Trial 4 finished with value: 79.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}. Best is trial 1 with value: 81.13333129882812.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.0, 'K': 10}   trial.value:  78.2    {'train': '91.43 Â± 1.43', 'valid': '78.20 Â± 0.92', 'test': '78.20 Â± 1.65'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.0, 'K': 10}   trial.value:  79.8    {'train': '96.19 Â± 0.82', 'valid': '79.80 Â± 0.53', 'test': '78.20 Â± 0.00'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}   trial.value:  81.133    {'train': '98.57 Â± 0.71', 'valid': '81.13 Â± 0.31', 'test': '78.03 Â± 0.90'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.0, 'K': 10}   trial.value:  79.333    {'train': '98.81 Â± 0.82', 'valid': '79.33 Â± 0.23', 'test': '78.53 Â± 2.42'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.0, 'K': 10}   trial.value:  77.2    {'train': '99.76 Â± 0.41', 'valid': '77.20 Â± 0.72', 'test': '76.33 Â± 1.90'}
test_acc
['78.20 Â± 1.65', '78.20 Â± 0.00', '78.03 Â± 0.90', '78.53 Â± 2.42', '76.33 Â± 1.90']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.0, 'K': 10}
Best trial Value:  81.13333129882812
Best trial Acc:  {'train': '98.57 Â± 0.71', 'valid': '81.13 Â± 0.31', 'test': '78.03 Â± 0.90'}
optuna total time:  122.18749588541687
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 97.86%, Valid: 79.80% Test: 80.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.14%, Valid: 78.20% Test: 79.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 97.86%, Valid: 77.80% Test: 77.70%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 98.57%, Valid: 79.00% Test: 80.10%
Split: 01, Run: 01
None time:  8.075437439605594
None Run 01:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 98.57
   Final Test: 81.40
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 98.57%, Valid: 78.60% Test: 80.10%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 96.43%, Valid: 80.00% Test: 78.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 97.86%, Valid: 78.40% Test: 79.10%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 97.86%, Valid: 78.60% Test: 78.70%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 98.57%, Valid: 80.20% Test: 80.50%
Split: 01, Run: 02
None time:  7.87683973275125
None Run 02:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 97.86
   Final Test: 81.10
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 98.57%, Valid: 78.60% Test: 79.90%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 97.86%, Valid: 78.80% Test: 80.70%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 98.57%, Valid: 79.80% Test: 81.50%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 97.86%, Valid: 81.00% Test: 80.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 98.57%, Valid: 78.80% Test: 79.30%
Split: 01, Run: 03
None time:  7.996332200244069
None Run 03:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 98.57
   Final Test: 79.90
total time:  25.529146196320653
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.33 Â± 0.31
  Final Train: 98.33 Â± 0.41
   Final Test: 80.80 Â± 0.79
[32m[I 2021-07-26 08:39:12,436][0m Trial 0 finished with value: 81.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 81.33333587646484.[0m
lambda1:  0.05
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 99.29%, Valid: 78.80% Test: 81.90%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 99.29%, Valid: 81.20% Test: 81.70%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.86%, Valid: 79.40% Test: 81.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 98.57%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 98.57%, Valid: 78.20% Test: 80.20%
Split: 01, Run: 01
None time:  7.829603718593717
None Run 01:
Highest Train: 100.00
Highest Valid: 82.20
  Final Train: 99.29
   Final Test: 82.30
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.80% Test: 80.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 81.80% Test: 81.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 99.29%, Valid: 80.00% Test: 81.00%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 98.57%, Valid: 80.20% Test: 80.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 79.80% Test: 81.10%
Split: 01, Run: 02
None time:  7.824962377548218
None Run 02:
Highest Train: 100.00
Highest Valid: 81.80
  Final Train: 100.00
   Final Test: 81.50
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 79.20% Test: 80.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 99.29%, Valid: 79.80% Test: 80.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 81.80% Test: 82.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 98.57%, Valid: 80.20% Test: 81.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 79.60% Test: 79.70%
Split: 01, Run: 03
None time:  7.8222614992409945
None Run 03:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 99.29
   Final Test: 82.20
total time:  23.53783176280558
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 82.13 Â± 0.31
  Final Train: 99.52 Â± 0.41
   Final Test: 82.00 Â± 0.44
[32m[I 2021-07-26 08:39:35,979][0m Trial 1 finished with value: 82.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}. Best is trial 1 with value: 82.13333129882812.[0m
lambda1:  0.2
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 75.60% Test: 78.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 77.60% Test: 79.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 73.60% Test: 76.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 74.80% Test: 76.80%
Split: 01, Run: 01
None time:  7.80940518528223
None Run 01:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 77.80
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 77.40% Test: 78.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 77.80% Test: 78.30%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 76.60% Test: 77.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 75.40% Test: 76.60%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 75.40% Test: 76.30%
Split: 01, Run: 02
None time:  7.678736865520477
None Run 02:
Highest Train: 100.00
Highest Valid: 78.20
  Final Train: 100.00
   Final Test: 78.60
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 74.80% Test: 77.00%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 75.40% Test: 77.80%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 77.80% Test: 77.60%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 75.60% Test: 77.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 75.80% Test: 77.10%
Split: 01, Run: 03
None time:  7.803550515323877
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 79.60
total time:  23.36536803841591
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 78.60 Â± 0.35
  Final Train: 100.00 Â± 0.00
   Final Test: 78.67 Â± 0.90
[32m[I 2021-07-26 08:39:59,351][0m Trial 2 finished with value: 78.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}. Best is trial 1 with value: 82.13333129882812.[0m
lambda1:  0.01
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 78.80% Test: 79.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 78.00% Test: 79.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.86%, Valid: 78.40% Test: 78.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 97.86%, Valid: 78.20% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 98.57%, Valid: 78.80% Test: 79.10%
Split: 01, Run: 01
None time:  8.336498253047466
None Run 01:
Highest Train: 100.00
Highest Valid: 80.40
  Final Train: 98.57
   Final Test: 80.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 98.57%, Valid: 78.00% Test: 79.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 97.14%, Valid: 78.40% Test: 78.10%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 97.86%, Valid: 78.20% Test: 79.00%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 97.86%, Valid: 78.40% Test: 78.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 78.60% Test: 79.30%
Split: 01, Run: 02
None time:  7.762782735750079
None Run 02:
Highest Train: 100.00
Highest Valid: 79.80
  Final Train: 98.57
   Final Test: 80.30
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 98.57%, Valid: 77.60% Test: 78.90%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 98.57%, Valid: 78.00% Test: 79.50%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 98.57%, Valid: 79.60% Test: 79.90%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 98.57%, Valid: 80.00% Test: 79.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 97.86%, Valid: 78.80% Test: 78.40%
Split: 01, Run: 03
None time:  7.787356721237302
None Run 03:
Highest Train: 100.00
Highest Valid: 80.40
  Final Train: 97.86
   Final Test: 79.30
total time:  23.957510240375996
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 80.20 Â± 0.35
  Final Train: 98.33 Â± 0.41
   Final Test: 80.10 Â± 0.72
[32m[I 2021-07-26 08:40:23,314][0m Trial 3 finished with value: 80.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}. Best is trial 1 with value: 82.13333129882812.[0m
lambda1:  0.1
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 78.40% Test: 81.00%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 99.29%, Valid: 80.00% Test: 81.70%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 80.60% Test: 81.00%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 98.57%, Valid: 76.20% Test: 77.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 78.20% Test: 79.50%
Split: 01, Run: 01
None time:  7.6656093671917915
None Run 01:
Highest Train: 100.00
Highest Valid: 81.60
  Final Train: 100.00
   Final Test: 80.90
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 79.20% Test: 80.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 80.80% Test: 81.10%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 79.00% Test: 80.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 78.20% Test: 79.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 77.20% Test: 78.80%
Split: 01, Run: 02
None time:  7.63863817229867
None Run 02:
Highest Train: 100.00
Highest Valid: 81.80
  Final Train: 100.00
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 77.80% Test: 79.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 99.29%, Valid: 78.60% Test: 80.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 79.20% Test: 80.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 77.00% Test: 78.90%
Split: 01, Run: 03
None time:  7.708195947110653
None Run 03:
Highest Train: 100.00
Highest Valid: 82.40
  Final Train: 100.00
   Final Test: 80.20
total time:  23.076274713501334
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.93 Â± 0.42
  Final Train: 100.00 Â± 0.00
   Final Test: 80.60 Â± 0.36
[32m[I 2021-07-26 08:40:46,397][0m Trial 4 finished with value: 81.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}. Best is trial 1 with value: 82.13333129882812.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}   trial.value:  80.2    {'train': '98.33 Â± 0.41', 'valid': '80.20 Â± 0.35', 'test': '80.10 Â± 0.72'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}   trial.value:  81.333    {'train': '98.33 Â± 0.41', 'valid': '81.33 Â± 0.31', 'test': '80.80 Â± 0.79'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}   trial.value:  82.133    {'train': '99.52 Â± 0.41', 'valid': '82.13 Â± 0.31', 'test': '82.00 Â± 0.44'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}   trial.value:  81.933    {'train': '100.00 Â± 0.00', 'valid': '81.93 Â± 0.42', 'test': '80.60 Â± 0.36'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}   trial.value:  78.6    {'train': '100.00 Â± 0.00', 'valid': '78.60 Â± 0.35', 'test': '78.67 Â± 0.90'}
test_acc
['80.10 Â± 0.72', '80.80 Â± 0.79', '82.00 Â± 0.44', '80.60 Â± 0.36', '78.67 Â± 0.90']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}
Best trial Value:  82.13333129882812
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '82.13 Â± 0.31', 'test': '82.00 Â± 0.44'}
optuna total time:  119.50568735226989
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:41:04,550][0m A new study created in memory with name: no-name-c2d21c72-a627-4325-b7ce-2bbd51787162[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.2931194715201855
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  6.730868911370635
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  6.72502726316452
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  22.48126365058124
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:41:27,040][0m Trial 0 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
test_acc
['80.43 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  22.49412875622511
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.05], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:42:23,377][0m A new study created in memory with name: no-name-1d4c514f-9320-4a76-b6d2-ca205ceef457[0m
K:  10
alpha:  0.05
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.05)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 96.43%, Valid: 78.40% Test: 81.10%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 97.86%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 97.86%, Valid: 80.40% Test: 80.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 96.43%, Valid: 74.40% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 96.43%, Valid: 77.40% Test: 78.20%
Split: 01, Run: 01
None time:  7.073081536218524
None Run 01:
Highest Train: 98.57
Highest Valid: 82.20
  Final Train: 98.57
   Final Test: 81.50
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 95.71%, Valid: 79.60% Test: 81.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 97.86%, Valid: 79.20% Test: 80.50%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 96.43%, Valid: 78.00% Test: 79.40%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 97.14%, Valid: 78.00% Test: 80.10%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 96.43%, Valid: 77.80% Test: 78.90%
Split: 01, Run: 02
None time:  6.677183857187629
None Run 02:
Highest Train: 98.57
Highest Valid: 81.60
  Final Train: 97.86
   Final Test: 81.50
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 95.71%, Valid: 78.00% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 97.14%, Valid: 79.40% Test: 80.40%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 97.14%, Valid: 76.80% Test: 78.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 97.14%, Valid: 78.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 95.71%, Valid: 74.60% Test: 77.30%
Split: 01, Run: 03
None time:  6.7581952307373285
None Run 03:
Highest Train: 99.29
Highest Valid: 82.20
  Final Train: 98.57
   Final Test: 80.20
total time:  22.13609316945076
None All runs:
Highest Train: 98.81 Â± 0.41
Highest Valid: 82.00 Â± 0.35
  Final Train: 98.33 Â± 0.41
   Final Test: 81.07 Â± 0.75
[32m[I 2021-07-26 08:42:45,522][0m Trial 0 finished with value: 82.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}. Best is trial 0 with value: 82.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}   trial.value:  82    {'train': '98.33 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '81.07 Â± 0.75'}
test_acc
['81.07 Â± 0.75']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}
Best trial Value:  82.0
Best trial Acc:  {'train': '98.33 Â± 0.41', 'valid': '82.00 Â± 0.35', 'test': '81.07 Â± 0.75'}
optuna total time:  22.15032735094428
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.02, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.02], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:43:03,512][0m A new study created in memory with name: no-name-a8815ca5-8243-4a00-b37e-e4c529c9cc65[0m
K:  10
alpha:  0.02
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.02, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.02)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 95.00%, Valid: 79.20% Test: 81.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 95.71%, Valid: 81.00% Test: 80.60%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 93.57%, Valid: 79.60% Test: 79.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 92.86%, Valid: 74.80% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 93.57%, Valid: 76.20% Test: 77.70%
Split: 01, Run: 01
None time:  7.069182969629765
None Run 01:
Highest Train: 96.43
Highest Valid: 82.60
  Final Train: 95.71
   Final Test: 81.90
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 90.71%, Valid: 79.60% Test: 80.50%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 94.29%, Valid: 79.80% Test: 80.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 91.43%, Valid: 77.80% Test: 78.40%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 94.29%, Valid: 79.80% Test: 79.40%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 93.57%, Valid: 78.60% Test: 78.50%
Split: 01, Run: 02
None time:  6.4601228181272745
None Run 02:
Highest Train: 96.43
Highest Valid: 81.20
  Final Train: 95.00
   Final Test: 80.60
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 94.29%, Valid: 78.40% Test: 79.60%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 94.29%, Valid: 78.60% Test: 79.40%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 92.86%, Valid: 76.40% Test: 78.10%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 92.14%, Valid: 80.00% Test: 79.40%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 91.43%, Valid: 75.60% Test: 76.40%
Split: 01, Run: 03
None time:  6.381215576082468
None Run 03:
Highest Train: 96.43
Highest Valid: 82.00
  Final Train: 95.00
   Final Test: 81.50
total time:  21.628749998286366
None All runs:
Highest Train: 96.43 Â± 0.00
Highest Valid: 81.93 Â± 0.70
  Final Train: 95.24 Â± 0.41
   Final Test: 81.33 Â± 0.67
[32m[I 2021-07-26 08:43:25,149][0m Trial 0 finished with value: 81.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}. Best is trial 0 with value: 81.9333267211914.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}   trial.value:  81.933    {'train': '95.24 Â± 0.41', 'valid': '81.93 Â± 0.70', 'test': '81.33 Â± 0.67'}
test_acc
['81.33 Â± 0.67']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}
Best trial Value:  81.9333267211914
Best trial Acc:  {'train': '95.24 Â± 0.41', 'valid': '81.93 Â± 0.70', 'test': '81.33 Â± 0.67'}
optuna total time:  21.64170952513814
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.01, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.01], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:43:57,272][0m A new study created in memory with name: no-name-9f5f0192-3eb9-487e-83cd-895a08454bd0[0m
K:  10
alpha:  0.01
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.01, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.01)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 94.29%, Valid: 79.00% Test: 80.60%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 92.86%, Valid: 81.00% Test: 80.20%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 92.14%, Valid: 79.20% Test: 79.40%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 90.71%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 91.43%, Valid: 75.20% Test: 77.40%
Split: 01, Run: 01
None time:  7.901854019612074
None Run 01:
Highest Train: 95.71
Highest Valid: 81.80
  Final Train: 93.57
   Final Test: 80.40
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 90.00%, Valid: 78.60% Test: 80.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 92.86%, Valid: 80.40% Test: 79.70%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 90.71%, Valid: 77.40% Test: 77.50%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 93.57%, Valid: 79.60% Test: 79.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 92.86%, Valid: 78.80% Test: 77.70%
Split: 01, Run: 02
None time:  6.898894116282463
None Run 02:
Highest Train: 95.00
Highest Valid: 81.40
  Final Train: 93.57
   Final Test: 79.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 92.86%, Valid: 78.00% Test: 79.40%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 92.86%, Valid: 79.00% Test: 79.60%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 90.71%, Valid: 76.60% Test: 78.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 90.71%, Valid: 79.60% Test: 79.20%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 89.29%, Valid: 75.80% Test: 76.30%
Split: 01, Run: 03
None time:  6.788146980106831
None Run 03:
Highest Train: 95.00
Highest Valid: 82.40
  Final Train: 95.00
   Final Test: 81.10
total time:  23.200989266857505
None All runs:
Highest Train: 95.24 Â± 0.41
Highest Valid: 81.87 Â± 0.50
  Final Train: 94.05 Â± 0.82
   Final Test: 80.40 Â± 0.70
[32m[I 2021-07-26 08:44:20,482][0m Trial 0 finished with value: 81.86666870117188 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}. Best is trial 0 with value: 81.86666870117188.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}   trial.value:  81.867    {'train': '94.05 Â± 0.82', 'valid': '81.87 Â± 0.50', 'test': '80.40 Â± 0.70'}
test_acc
['80.40 Â± 0.70']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}
Best trial Value:  81.86666870117188
Best trial Acc:  {'train': '94.05 Â± 0.82', 'valid': '81.87 Â± 0.50', 'test': '80.40 Â± 0.70'}
optuna total time:  23.214281955733895
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:46:46,050][0m A new study created in memory with name: no-name-3f9ba058-54cd-4938-8e67-a9dc45bc1976[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.721140630543232
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  6.726465668529272
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  6.705184271559119
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  26.37387898378074
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:47:12,433][0m Trial 0 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
test_acc
['80.43 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  26.387843372300267
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:48:11,482][0m A new study created in memory with name: no-name-5f1d7e02-4a0c-418e-9dd0-768d09082054[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 100.00%, Valid: 47.60% Test: 48.70%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 100.00%, Valid: 45.40% Test: 46.00%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 100.00%, Valid: 46.60% Test: 45.90%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 100.00%, Valid: 47.00% Test: 45.60%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 100.00%, Valid: 46.20% Test: 45.50%
Split: 01, Run: 01
None time:  5.725791061297059
None Run 01:
Highest Train: 100.00
Highest Valid: 57.40
  Final Train: 100.00
   Final Test: 55.70
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 100.00%, Valid: 48.20% Test: 49.10%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:48:31,277][0m A new study created in memory with name: no-name-afd16641-16e4-4b5f-b733-e4159950ca44[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 100.00%, Valid: 59.00% Test: 59.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 100.00%, Valid: 57.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 100.00%, Valid: 59.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 100.00%, Valid: 53.60% Test: 55.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 01
None time:  5.157782085239887
None Run 01:
Highest Train: 100.00
Highest Valid: 61.80
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 100.00%, Valid: 61.00% Test: 58.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 100.00%, Valid: 55.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 58.60% Test: 54.90%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 56.40% Test: 55.20%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 100.00%, Valid: 57.60% Test: 55.70%
Split: 01, Run: 02
None time:  4.764208562672138
None Run 02:
Highest Train: 100.00
Highest Valid: 61.40
  Final Train: 100.00
   Final Test: 56.40
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 100.00%, Valid: 57.60% Test: 56.80%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 57.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 100.00%, Valid: 56.20% Test: 55.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 56.80% Test: 55.90%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 100.00%, Valid: 56.20% Test: 54.00%
Split: 01, Run: 03
None time:  4.907217778265476
None Run 03:
Highest Train: 100.00
Highest Valid: 62.80
  Final Train: 100.00
   Final Test: 59.60
total time:  16.44188709743321
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.00 Â± 0.72
  Final Train: 100.00 Â± 0.00
   Final Test: 57.57 Â± 1.77
[32m[I 2021-07-26 08:48:47,729][0m Trial 0 finished with value: 62.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 62.0.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  62    {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
test_acc
['57.57 Â± 1.77']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  62.0
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '62.00 Â± 0.72', 'test': '57.57 Â± 1.77'}
optuna total time:  16.456717083230615
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:49:01,227][0m A new study created in memory with name: no-name-8f79dbab-fb1b-4864-9a2d-3e27b288501f[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 100.00%, Valid: 47.60% Test: 48.70%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 100.00%, Valid: 45.40% Test: 46.00%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 100.00%, Valid: 46.60% Test: 45.90%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 100.00%, Valid: 47.00% Test: 45.60%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 100.00%, Valid: 46.20% Test: 45.50%
Split: 01, Run: 01
None time:  5.829196408390999
None Run 01:
Highest Train: 100.00
Highest Valid: 57.40
  Final Train: 100.00
   Final Test: 55.70
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 100.00%, Valid: 48.20% Test: 49.10%
Split: 01, Run: 02, Epoch: 200, Loss: 4367.1445, Train: 100.00%, Valid: 47.00% Test: 47.70%
Split: 01, Run: 02, Epoch: 300, Loss: 4365.0664, Train: 100.00%, Valid: 45.00% Test: 46.30%
Split: 01, Run: 02, Epoch: 400, Loss: 4365.3105, Train: 100.00%, Valid: 46.40% Test: 47.30%
Split: 01, Run: 02, Epoch: 500, Loss: 4366.8096, Train: 100.00%, Valid: 48.80% Test: 47.20%
Split: 01, Run: 02
None time:  4.934412628412247
None Run 02:
Highest Train: 100.00
Highest Valid: 55.60
  Final Train: 100.00
   Final Test: 54.10
Split: 01, Run: 03, Epoch: 100, Loss: 4367.7212, Train: 100.00%, Valid: 51.80% Test: 51.50%
Split: 01, Run: 03, Epoch: 200, Loss: 4366.7866, Train: 100.00%, Valid: 51.00% Test: 50.30%
Split: 01, Run: 03, Epoch: 300, Loss: 4367.3530, Train: 100.00%, Valid: 49.40% Test: 48.40%
Split: 01, Run: 03, Epoch: 400, Loss: 4366.4707, Train: 100.00%, Valid: 46.80% Test: 48.80%
Split: 01, Run: 03, Epoch: 500, Loss: 4365.7056, Train: 100.00%, Valid: 49.60% Test: 49.50%
Split: 01, Run: 03
None time:  4.866328310221434
None Run 03:
Highest Train: 100.00
Highest Valid: 62.60
  Final Train: 100.00
   Final Test: 57.70
total time:  17.32696482539177
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.53 Â± 3.64
  Final Train: 100.00 Â± 0.00
   Final Test: 55.83 Â± 1.80
[32m[I 2021-07-26 08:49:18,562][0m Trial 0 finished with value: 58.5333366394043 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 58.5333366394043.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  58.533    {'train': '100.00 Â± 0.00', 'valid': '58.53 Â± 3.64', 'test': '55.83 Â± 1.80'}
test_acc
['55.83 Â± 1.80']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  58.5333366394043
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '58.53 Â± 3.64', 'test': '55.83 Â± 1.80'}
optuna total time:  17.340032951906323
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:49:45,146][0m A new study created in memory with name: no-name-b5be7b73-ff4d-4c88-9b13-caae18a46151[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3155, Train: 98.57%, Valid: 77.60% Test: 80.40%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2254, Train: 98.57%, Valid: 77.60% Test: 80.10%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2075, Train: 99.29%, Valid: 79.40% Test: 80.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1727, Train: 99.29%, Valid: 74.00% Test: 77.10%
Split: 01, Run: 01, Epoch: 500, Loss: 0.1964, Train: 99.29%, Valid: 77.00% Test: 77.20%
Split: 01, Run: 01
None time:  7.324125120416284
None Run 01:
Highest Train: 100.00
Highest Valid: 81.00
  Final Train: 100.00
   Final Test: 80.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.3047, Train: 99.29%, Valid: 79.40% Test: 81.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2266, Train: 99.29%, Valid: 77.20% Test: 80.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.1860, Train: 100.00%, Valid: 77.60% Test: 78.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1395, Train: 100.00%, Valid: 77.40% Test: 79.00%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2186, Train: 99.29%, Valid: 77.00% Test: 78.50%
Split: 01, Run: 02
None time:  6.782788338139653
None Run 02:
Highest Train: 100.00
Highest Valid: 81.20
  Final Train: 99.29
   Final Test: 80.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3248, Train: 99.29%, Valid: 76.80% Test: 79.70%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2560, Train: 100.00%, Valid: 77.20% Test: 79.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2363, Train: 99.29%, Valid: 75.80% Test: 77.20%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2238, Train: 100.00%, Valid: 78.60% Test: 78.60%
Split: 01, Run: 03, Epoch: 500, Loss: 0.1942, Train: 99.29%, Valid: 75.40% Test: 77.20%
Split: 01, Run: 03
None time:  6.76499062590301
None Run 03:
Highest Train: 100.00
Highest Valid: 81.40
  Final Train: 100.00
   Final Test: 80.50
total time:  22.548554396256804
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 81.20 Â± 0.20
  Final Train: 99.76 Â± 0.41
   Final Test: 80.43 Â± 0.31
[32m[I 2021-07-26 08:50:07,703][0m Trial 0 finished with value: 81.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 81.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  81.2    {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
test_acc
['80.43 Â± 0.31']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  81.20000457763672
Best trial Acc:  {'train': '99.76 Â± 0.41', 'valid': '81.20 Â± 0.20', 'test': '80.43 Â± 0.31'}
optuna total time:  22.56177137233317
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:50:25,757][0m A new study created in memory with name: no-name-a94441fd-d308-496a-bb00-7c129a763fbd[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 97.86%, Valid: 69.40% Test: 70.50%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 98.57%, Valid: 69.80% Test: 69.70%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 99.29%, Valid: 70.00% Test: 70.60%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 98.57%, Valid: 70.80% Test: 70.30%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 98.57%, Valid: 69.20% Test: 67.80%
Split: 01, Run: 01
None time:  7.595424810424447
None Run 01:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 99.29
   Final Test: 75.50
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 99.29%, Valid: 75.00% Test: 74.60%
Split: 01, Run: 02, Epoch: 200, Loss: 4367.1445, Train: 99.29%, Valid: 72.40% Test: 72.60%
Split: 01, Run: 02, Epoch: 300, Loss: 4365.0664, Train: 100.00%, Valid: 70.20% Test: 71.50%
Split: 01, Run: 02, Epoch: 400, Loss: 4365.3105, Train: 100.00%, Valid: 72.80% Test: 74.10%
Split: 01, Run: 02, Epoch: 500, Loss: 4366.8096, Train: 100.00%, Valid: 74.00% Test: 72.80%
Split: 01, Run: 02
None time:  7.102593537420034
None Run 02:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 98.57
   Final Test: 79.10
Split: 01, Run: 03, Epoch: 100, Loss: 4367.7212, Train: 98.57%, Valid: 76.60% Test: 76.50%
Split: 01, Run: 03, Epoch: 200, Loss: 4366.7866, Train: 99.29%, Valid: 77.20% Test: 77.20%
Split: 01, Run: 03, Epoch: 300, Loss: 4367.3530, Train: 99.29%, Valid: 76.00% Test: 75.40%
Split: 01, Run: 03, Epoch: 400, Loss: 4366.4707, Train: 98.57%, Valid: 75.00% Test: 73.90%
Split: 01, Run: 03, Epoch: 500, Loss: 4365.7056, Train: 100.00%, Valid: 76.40% Test: 75.10%
Split: 01, Run: 03
None time:  7.134160881862044
None Run 03:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 99.29
   Final Test: 78.50
total time:  23.473481548950076
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.80 Â± 1.04
  Final Train: 99.05 Â± 0.41
   Final Test: 77.70 Â± 1.93
[32m[I 2021-07-26 08:50:49,240][0m Trial 0 finished with value: 77.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 77.79999542236328.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  77.8    {'train': '99.05 Â± 0.41', 'valid': '77.80 Â± 1.04', 'test': '77.70 Â± 1.93'}
test_acc
['77.70 Â± 1.93']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  77.79999542236328
Best trial Acc:  {'train': '99.05 Â± 0.41', 'valid': '77.80 Â± 1.04', 'test': '77.70 Â± 1.93'}
optuna total time:  23.488931132480502
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.05], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:51:37,473][0m A new study created in memory with name: no-name-f6e07728-dd6d-438d-b604-f3140970e00a[0m
K:  10
alpha:  0.05
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.05)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 95.00%, Valid: 70.60% Test: 70.50%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 97.86%, Valid: 71.40% Test: 71.50%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 98.57%, Valid: 72.00% Test: 71.90%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 97.86%, Valid: 72.80% Test: 72.90%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 97.86%, Valid: 71.00% Test: 69.20%
Split: 01, Run: 01
None time:  7.7237193658947945
None Run 01:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 98.57
   Final Test: 75.80
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 98.57%, Valid: 76.20% Test: 76.20%
Split: 01, Run: 02, Epoch: 200, Loss: 4367.1445, Train: 97.86%, Valid: 73.20% Test: 73.50%
Split: 01, Run: 02, Epoch: 300, Loss: 4365.0664, Train: 99.29%, Valid: 72.40% Test: 73.00%
Split: 01, Run: 02, Epoch: 400, Loss: 4365.3105, Train: 98.57%, Valid: 74.40% Test: 76.40%
Split: 01, Run: 02, Epoch: 500, Loss: 4366.8096, Train: 98.57%, Valid: 75.80% Test: 73.90%
Split: 01, Run: 02
None time:  7.248680833727121
None Run 02:
Highest Train: 100.00
Highest Valid: 79.20
  Final Train: 99.29
   Final Test: 78.20
Split: 01, Run: 03, Epoch: 100, Loss: 4367.7212, Train: 95.71%, Valid: 76.20% Test: 77.10%
Split: 01, Run: 03, Epoch: 200, Loss: 4366.7866, Train: 99.29%, Valid: 78.00% Test: 78.30%
Split: 01, Run: 03, Epoch: 300, Loss: 4367.3530, Train: 97.14%, Valid: 77.20% Test: 76.90%
Split: 01, Run: 03, Epoch: 400, Loss: 4366.4707, Train: 97.14%, Valid: 77.40% Test: 75.80%
Split: 01, Run: 03, Epoch: 500, Loss: 4365.7056, Train: 99.29%, Valid: 78.20% Test: 76.90%
Split: 01, Run: 03
None time:  7.2854698076844215
None Run 03:
Highest Train: 100.00
Highest Valid: 80.20
  Final Train: 100.00
   Final Test: 79.50
total time:  23.86961086653173
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 79.40 Â± 0.72
  Final Train: 99.29 Â± 0.71
   Final Test: 77.83 Â± 1.88
[32m[I 2021-07-26 08:52:01,351][0m Trial 0 finished with value: 79.4000015258789 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}. Best is trial 0 with value: 79.4000015258789.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}   trial.value:  79.4    {'train': '99.29 Â± 0.71', 'valid': '79.40 Â± 0.72', 'test': '77.83 Â± 1.88'}
test_acc
['77.83 Â± 1.88']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}
Best trial Value:  79.4000015258789
Best trial Acc:  {'train': '99.29 Â± 0.71', 'valid': '79.40 Â± 0.72', 'test': '77.83 Â± 1.88'}
optuna total time:  23.883055770769715
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.02, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.02], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:52:06,022][0m A new study created in memory with name: no-name-5d5c55db-a1e4-4638-933e-a88d1d12c793[0m
K:  10
alpha:  0.02
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.02, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.02)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 92.14%, Valid: 71.60% Test: 70.80%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 94.29%, Valid: 71.40% Test: 72.20%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 92.86%, Valid: 71.60% Test: 72.10%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 95.71%, Valid: 74.40% Test: 72.70%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 93.57%, Valid: 71.00% Test: 69.50%
Split: 01, Run: 01
None time:  7.659721828997135
None Run 01:
Highest Train: 97.86
Highest Valid: 78.60
  Final Train: 97.14
   Final Test: 79.90
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 96.43%, Valid: 77.00% Test: 75.80%
Split: 01, Run: 02, Epoch: 200, Loss: 4367.1445, Train: 94.29%, Valid: 73.60% Test: 74.30%
Split: 01, Run: 02, Epoch: 300, Loss: 4365.0664, Train: 97.14%, Valid: 72.20% Test: 73.80%
Split: 01, Run: 02, Epoch: 400, Loss: 4365.3105, Train: 96.43%, Valid: 74.80% Test: 76.60%
Split: 01, Run: 02, Epoch: 500, Loss: 4366.8096, Train: 95.71%, Valid: 74.60% Test: 74.80%
Split: 01, Run: 02
None time:  7.121655706316233
None Run 02:
Highest Train: 99.29
Highest Valid: 79.60
  Final Train: 96.43
   Final Test: 80.10
Split: 01, Run: 03, Epoch: 100, Loss: 4367.7212, Train: 92.14%, Valid: 76.40% Test: 76.70%
Split: 01, Run: 03, Epoch: 200, Loss: 4366.7866, Train: 95.71%, Valid: 78.40% Test: 78.60%
Split: 01, Run: 03, Epoch: 300, Loss: 4367.3530, Train: 95.71%, Valid: 77.20% Test: 77.60%
Split: 01, Run: 03, Epoch: 400, Loss: 4366.4707, Train: 95.00%, Valid: 77.00% Test: 76.30%
Split: 01, Run: 03, Epoch: 500, Loss: 4365.7056, Train: 97.14%, Valid: 78.60% Test: 78.70%
Split: 01, Run: 03
None time:  7.160406589508057
None Run 03:
Highest Train: 99.29
Highest Valid: 81.60
  Final Train: 98.57
   Final Test: 78.10
total time:  23.547600861638784
None All runs:
Highest Train: 98.81 Â± 0.82
Highest Valid: 79.93 Â± 1.53
  Final Train: 97.38 Â± 1.09
   Final Test: 79.37 Â± 1.10
[32m[I 2021-07-26 08:52:29,579][0m Trial 0 finished with value: 79.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}. Best is trial 0 with value: 79.9333267211914.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}   trial.value:  79.933    {'train': '97.38 Â± 1.09', 'valid': '79.93 Â± 1.53', 'test': '79.37 Â± 1.10'}
test_acc
['79.37 Â± 1.10']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.02, 'K': 10}
Best trial Value:  79.9333267211914
Best trial Acc:  {'train': '97.38 Â± 1.09', 'valid': '79.93 Â± 1.53', 'test': '79.37 Â± 1.10'}
optuna total time:  23.561609575524926
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.01, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.01], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:52:45,984][0m A new study created in memory with name: no-name-6afb9b51-669d-43b1-82df-b84373853965[0m
K:  10
alpha:  0.01
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.01, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='MSE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.01)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4368.0942, Train: 90.00%, Valid: 71.20% Test: 70.90%
Split: 01, Run: 01, Epoch: 200, Loss: 4366.1089, Train: 93.57%, Valid: 71.20% Test: 72.00%
Split: 01, Run: 01, Epoch: 300, Loss: 4366.3164, Train: 91.43%, Valid: 71.00% Test: 72.40%
Split: 01, Run: 01, Epoch: 400, Loss: 4365.4736, Train: 95.71%, Valid: 74.00% Test: 73.10%
Split: 01, Run: 01, Epoch: 500, Loss: 4365.6411, Train: 91.43%, Valid: 70.00% Test: 70.00%
Split: 01, Run: 01
None time:  7.622970178723335
None Run 01:
Highest Train: 96.43
Highest Valid: 78.60
  Final Train: 95.71
   Final Test: 77.70
Split: 01, Run: 02, Epoch: 100, Loss: 4367.9189, Train: 95.71%, Valid: 76.80% Test: 75.90%
Split: 01, Run: 02, Epoch: 200, Loss: 4367.1445, Train: 92.14%, Valid: 72.80% Test: 73.50%
Split: 01, Run: 02, Epoch: 300, Loss: 4365.0664, Train: 94.29%, Valid: 71.60% Test: 74.60%
Split: 01, Run: 02, Epoch: 400, Loss: 4365.3105, Train: 95.00%, Valid: 75.60% Test: 76.70%
Split: 01, Run: 02, Epoch: 500, Loss: 4366.8096, Train: 95.00%, Valid: 74.60% Test: 75.20%
Split: 01, Run: 02
None time:  7.059481544420123
None Run 02:
Highest Train: 97.14
Highest Valid: 80.00
  Final Train: 95.00
   Final Test: 78.10
Split: 01, Run: 03, Epoch: 100, Loss: 4367.7212, Train: 87.14%, Valid: 76.20% Test: 76.40%
Split: 01, Run: 03, Epoch: 200, Loss: 4366.7866, Train: 90.71%, Valid: 77.20% Test: 78.60%
Split: 01, Run: 03, Epoch: 300, Loss: 4367.3530, Train: 94.29%, Valid: 76.80% Test: 77.60%
Split: 01, Run: 03, Epoch: 400, Loss: 4366.4707, Train: 92.14%, Valid: 76.20% Test: 76.60%
Split: 01, Run: 03, Epoch: 500, Loss: 4365.7056, Train: 95.71%, Valid: 78.00% Test: 78.80%
Split: 01, Run: 03
None time:  7.072157630696893
None Run 03:
Highest Train: 97.14
Highest Valid: 80.40
  Final Train: 95.71
   Final Test: 78.40
total time:  23.384604044258595
None All runs:
Highest Train: 96.90 Â± 0.41
Highest Valid: 79.67 Â± 0.95
  Final Train: 95.48 Â± 0.41
   Final Test: 78.07 Â± 0.35
[32m[I 2021-07-26 08:53:09,379][0m Trial 0 finished with value: 79.66666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}. Best is trial 0 with value: 79.66666412353516.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}   trial.value:  79.667    {'train': '95.48 Â± 0.41', 'valid': '79.67 Â± 0.95', 'test': '78.07 Â± 0.35'}
test_acc
['78.07 Â± 0.35']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.01, 'K': 10}
Best trial Value:  79.66666412353516
Best trial Acc:  {'train': '95.48 Â± 0.41', 'valid': '79.67 Â± 0.95', 'test': '78.07 Â± 0.35'}
optuna total time:  23.39976992458105
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.1], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 08:55:48,806][0m A new study created in memory with name: no-name-9cef8bed-70c7-4626-a3c7-1c92090b52b8[0m
lambda1:  0.01
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 58.1542, Train: 93.57%, Valid: 53.40% Test: 54.70%
Split: 01, Run: 01, Epoch: 200, Loss: 50.6786, Train: 94.29%, Valid: 56.40% Test: 54.10%
Split: 01, Run: 01, Epoch: 300, Loss: 51.5626, Train: 96.43%, Valid: 53.00% Test: 51.90%
Split: 01, Run: 01, Epoch: 400, Loss: 49.6535, Train: 98.57%, Valid: 55.80% Test: 54.40%
Split: 01, Run: 01, Epoch: 500, Loss: 47.5685, Train: 97.86%, Valid: 51.60% Test: 51.90%
Split: 01, Run: 01
None time:  7.9096652921289206
None Run 01:
Highest Train: 98.57
Highest Valid: 58.00
  Final Train: 90.00
   Final Test: 57.20
Split: 01, Run: 02, Epoch: 100, Loss: 52.9204, Train: 94.29%, Valid: 54.40% Test: 55.40%
Split: 01, Run: 02, Epoch: 200, Loss: 47.5803, Train: 97.14%, Valid: 53.20% Test: 53.00%
Split: 01, Run: 02, Epoch: 300, Loss: 49.1543, Train: 97.14%, Valid: 52.80% Test: 53.50%
Split: 01, Run: 02, Epoch: 400, Loss: 50.5855, Train: 98.57%, Valid: 52.40% Test: 54.10%
Split: 01, Run: 02, Epoch: 500, Loss: 49.0450, Train: 98.57%, Valid: 53.40% Test: 54.00%
Split: 01, Run: 02
None time:  7.805215733125806
None Run 02:
Highest Train: 99.29
Highest Valid: 62.80
  Final Train: 88.57
   Final Test: 63.10
Split: 01, Run: 03, Epoch: 100, Loss: 55.8495, Train: 91.43%, Valid: 58.20% Test: 59.10%
Split: 01, Run: 03, Epoch: 200, Loss: 50.2964, Train: 95.71%, Valid: 54.60% Test: 55.70%
Split: 01, Run: 03, Epoch: 300, Loss: 50.9655, Train: 95.71%, Valid: 54.00% Test: 55.90%
Split: 01, Run: 03, Epoch: 400, Loss: 47.6746, Train: 97.14%, Valid: 55.20% Test: 55.50%
Split: 01, Run: 03, Epoch: 500, Loss: 45.4850, Train: 97.86%, Valid: 56.40% Test: 55.50%
Split: 01, Run: 03
None time:  7.748025158420205
None Run 03:
Highest Train: 98.57
Highest Valid: 69.40
  Final Train: 95.00
   Final Test: 73.10
total time:  25.164649676531553
None All runs:
Highest Train: 98.81 Â± 0.41
Highest Valid: 63.40 Â± 5.72
  Final Train: 91.19 Â± 3.38
   Final Test: 64.47 Â± 8.04
[32m[I 2021-07-26 08:56:13,991][0m Trial 0 finished with value: 63.40000534057617 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.05
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 20.6896, Train: 95.00%, Valid: 37.00% Test: 37.90%
Split: 01, Run: 01, Epoch: 200, Loss: 19.0520, Train: 96.43%, Valid: 37.00% Test: 38.50%
Split: 01, Run: 01, Epoch: 300, Loss: 20.0602, Train: 99.29%, Valid: 39.60% Test: 39.40%
Split: 01, Run: 01, Epoch: 400, Loss: 19.4886, Train: 100.00%, Valid: 41.00% Test: 41.60%
Split: 01, Run: 01, Epoch: 500, Loss: 17.7536, Train: 100.00%, Valid: 37.40% Test: 38.30%
Split: 01, Run: 01
None time:  7.684192540124059
None Run 01:
Highest Train: 100.00
Highest Valid: 43.80
  Final Train: 100.00
   Final Test: 42.40
Split: 01, Run: 02, Epoch: 100, Loss: 19.8847, Train: 97.86%, Valid: 44.80% Test: 45.90%
Split: 01, Run: 02, Epoch: 200, Loss: 19.2257, Train: 98.57%, Valid: 42.40% Test: 43.90%
Split: 01, Run: 02, Epoch: 300, Loss: 20.0948, Train: 98.57%, Valid: 41.80% Test: 46.00%
Split: 01, Run: 02, Epoch: 400, Loss: 20.3805, Train: 100.00%, Valid: 41.40% Test: 45.00%
Split: 01, Run: 02, Epoch: 500, Loss: 19.9250, Train: 100.00%, Valid: 41.40% Test: 44.60%
Split: 01, Run: 02
None time:  7.638392860069871
None Run 02:
Highest Train: 100.00
Highest Valid: 52.80
  Final Train: 85.00
   Final Test: 51.00
Split: 01, Run: 03, Epoch: 100, Loss: 21.3602, Train: 97.86%, Valid: 52.60% Test: 52.40%
Split: 01, Run: 03, Epoch: 200, Loss: 19.5097, Train: 97.86%, Valid: 43.40% Test: 45.40%
Split: 01, Run: 03, Epoch: 300, Loss: 21.3932, Train: 98.57%, Valid: 41.40% Test: 43.00%
Split: 01, Run: 03, Epoch: 400, Loss: 19.5525, Train: 98.57%, Valid: 42.00% Test: 42.60%
Split: 01, Run: 03, Epoch: 500, Loss: 18.1414, Train: 100.00%, Valid: 38.20% Test: 41.50%
Split: 01, Run: 03
None time:  7.6423253659158945
None Run 03:
Highest Train: 100.00
Highest Valid: 66.80
  Final Train: 97.14
   Final Test: 68.30
total time:  23.026086619123816
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 54.47 Â± 11.59
  Final Train: 94.05 Â± 7.96
   Final Test: 53.90 Â± 13.19
[32m[I 2021-07-26 08:56:37,023][0m Trial 1 finished with value: 54.4666633605957 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.2
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4.1371, Train: 82.86%, Valid: 15.80% Test: 18.30%
Split: 01, Run: 01, Epoch: 200, Loss: 4.2099, Train: 98.57%, Valid: 18.00% Test: 21.90%
Split: 01, Run: 01, Epoch: 300, Loss: 4.2838, Train: 100.00%, Valid: 21.40% Test: 22.80%
Split: 01, Run: 01, Epoch: 400, Loss: 4.2413, Train: 100.00%, Valid: 21.00% Test: 24.00%
Split: 01, Run: 01, Epoch: 500, Loss: 3.7140, Train: 100.00%, Valid: 21.20% Test: 23.50%
Split: 01, Run: 01
None time:  7.688825009390712
None Run 01:
Highest Train: 100.00
Highest Valid: 22.60
  Final Train: 99.29
   Final Test: 24.50
Split: 01, Run: 02, Epoch: 100, Loss: 3.9366, Train: 96.43%, Valid: 29.00% Test: 31.80%
Split: 01, Run: 02, Epoch: 200, Loss: 3.9398, Train: 99.29%, Valid: 25.20% Test: 27.40%
Split: 01, Run: 02, Epoch: 300, Loss: 3.9299, Train: 100.00%, Valid: 23.20% Test: 25.50%
Split: 01, Run: 02, Epoch: 400, Loss: 4.1063, Train: 100.00%, Valid: 26.20% Test: 27.80%
Split: 01, Run: 02, Epoch: 500, Loss: 4.4469, Train: 100.00%, Valid: 26.00% Test: 27.90%
Split: 01, Run: 02
None time:  7.562679216265678
None Run 02:
Highest Train: 100.00
Highest Valid: 33.20
  Final Train: 84.29
   Final Test: 33.20
Split: 01, Run: 03, Epoch: 100, Loss: 4.3493, Train: 98.57%, Valid: 37.60% Test: 38.40%
Split: 01, Run: 03, Epoch: 200, Loss: 4.0575, Train: 100.00%, Valid: 24.60% Test: 28.50%
Split: 01, Run: 03, Epoch: 300, Loss: 4.2652, Train: 100.00%, Valid: 21.60% Test: 25.40%
Split: 01, Run: 03, Epoch: 400, Loss: 3.8164, Train: 100.00%, Valid: 21.40% Test: 23.90%
Split: 01, Run: 03, Epoch: 500, Loss: 4.1208, Train: 100.00%, Valid: 23.00% Test: 25.20%
Split: 01, Run: 03
None time:  7.582967899739742
None Run 03:
Highest Train: 100.00
Highest Valid: 38.60
  Final Train: 97.86
   Final Test: 39.30
total time:  22.88828898407519
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 31.47 Â± 8.14
  Final Train: 93.81 Â± 8.28
   Final Test: 32.33 Â± 7.44
[32m[I 2021-07-26 08:56:59,917][0m Trial 2 finished with value: 31.466665267944336 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.02
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 41.1199, Train: 95.00%, Valid: 51.00% Test: 50.80%
Split: 01, Run: 01, Epoch: 200, Loss: 36.3557, Train: 93.57%, Valid: 51.40% Test: 49.60%
Split: 01, Run: 01, Epoch: 300, Loss: 37.7683, Train: 97.14%, Valid: 48.00% Test: 47.80%
Split: 01, Run: 01, Epoch: 400, Loss: 37.2415, Train: 98.57%, Valid: 52.40% Test: 50.90%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.05], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:57:07,417][0m A new study created in memory with name: no-name-658c278c-11b4-4eef-b636-f6dde0a63139[0m
K:  10
alpha:  0.05
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.05, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 01, Epoch: 500, Loss: 35.2384, Train: 99.29%, Valid: 47.40% Test: 49.00%
Split: 01, Run: 01
None time:  8.18629489466548
None Run 01:
Highest Train: 99.29
Highest Valid: 53.00
  Final Train: 95.00
   Final Test: 52.50
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.05)
)
Split: 01, Run: 02, Epoch: 100, Loss: 38.2870, Train: 95.71%, Valid: 50.40% Test: 52.60%
Split: 01, Run: 02, Epoch: 200, Loss: 36.0599, Train: 98.57%, Valid: 49.60% Test: 51.60%
Split: 01, Run: 01, Epoch: 100, Loss: 0.4391, Train: 97.14%, Valid: 67.00% Test: 72.90%
Split: 01, Run: 02, Epoch: 300, Loss: 35.9651, Train: 98.57%, Valid: 50.40% Test: 50.80%
Split: 01, Run: 01, Epoch: 200, Loss: 0.3319, Train: 97.14%, Valid: 66.00% Test: 70.90%
Split: 01, Run: 02, Epoch: 400, Loss: 37.4143, Train: 98.57%, Valid: 52.80% Test: 51.80%
Split: 01, Run: 02, Epoch: 500, Loss: 36.7299, Train: 99.29%, Valid: 52.60% Test: 52.20%
Split: 01, Run: 02
None time:  8.538824655115604
None Run 02:
Highest Train: 100.00
Highest Valid: 58.80
  Final Train: 89.29
   Final Test: 56.00
Split: 01, Run: 01, Epoch: 300, Loss: 0.3158, Train: 97.14%, Valid: 67.60% Test: 71.20%
Split: 01, Run: 03, Epoch: 100, Loss: 41.4906, Train: 95.00%, Valid: 52.80% Test: 54.20%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2619, Train: 97.14%, Valid: 67.20% Test: 71.70%
Split: 01, Run: 03, Epoch: 200, Loss: 37.4545, Train: 95.71%, Valid: 49.80% Test: 51.40%
Split: 01, Run: 03, Epoch: 300, Loss: 38.7977, Train: 97.14%, Valid: 48.80% Test: 50.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2991, Train: 97.14%, Valid: 66.60% Test: 70.40%
Split: 01, Run: 01
None time:  12.971153343096375
None Run 01:
Highest Train: 98.57
Highest Valid: 69.00
  Final Train: 96.43
   Final Test: 72.30
Split: 01, Run: 03, Epoch: 400, Loss: 36.1228, Train: 98.57%, Valid: 52.00% Test: 52.50%
Split: 01, Run: 02, Epoch: 100, Loss: 0.4419, Train: 96.43%, Valid: 66.80% Test: 72.20%
Split: 01, Run: 03, Epoch: 500, Loss: 33.8258, Train: 98.57%, Valid: 51.40% Test: 52.40%
Split: 01, Run: 03
None time:  8.50172783434391
None Run 03:
Highest Train: 99.29
Highest Valid: 70.00
  Final Train: 97.14
   Final Test: 73.10
total time:  25.27987228706479
None All runs:
Highest Train: 99.52 Â± 0.41
Highest Valid: 60.60 Â± 8.64
  Final Train: 93.81 Â± 4.06
   Final Test: 60.53 Â± 11.02
[32m[I 2021-07-26 08:57:25,203][0m Trial 3 finished with value: 60.59999465942383 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.1
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 9.9177, Train: 95.00%, Valid: 24.00% Test: 27.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.3067, Train: 97.14%, Valid: 67.00% Test: 72.00%
Split: 01, Run: 01, Epoch: 200, Loss: 9.2825, Train: 97.14%, Valid: 24.40% Test: 27.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2896, Train: 98.57%, Valid: 66.20% Test: 70.00%
Split: 01, Run: 01, Epoch: 300, Loss: 9.4504, Train: 99.29%, Valid: 25.40% Test: 27.50%
Split: 01, Run: 01, Epoch: 400, Loss: 9.4481, Train: 99.29%, Valid: 26.40% Test: 28.30%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2498, Train: 97.86%, Valid: 66.80% Test: 71.80%
Split: 01, Run: 01, Epoch: 500, Loss: 9.6054, Train: 100.00%, Valid: 26.60% Test: 28.70%
Split: 01, Run: 01
None time:  8.349931182339787
None Run 01:
Highest Train: 100.00
Highest Valid: 29.60
  Final Train: 100.00
   Final Test: 30.40
Split: 01, Run: 02, Epoch: 500, Loss: 0.2532, Train: 97.14%, Valid: 66.60% Test: 72.00%
Split: 01, Run: 02
None time:  12.440363569185138
None Run 02:
Highest Train: 99.29
Highest Valid: 69.20
  Final Train: 97.86
   Final Test: 71.80
Split: 01, Run: 02, Epoch: 100, Loss: 9.8400, Train: 98.57%, Valid: 35.20% Test: 36.50%
Split: 01, Run: 03, Epoch: 100, Loss: 0.4614, Train: 95.00%, Valid: 65.40% Test: 71.70%
Split: 01, Run: 02, Epoch: 200, Loss: 9.5638, Train: 100.00%, Valid: 32.00% Test: 35.30%
Split: 01, Run: 02, Epoch: 300, Loss: 10.0595, Train: 99.29%, Valid: 28.60% Test: 32.30%
Split: 01, Run: 03, Epoch: 200, Loss: 0.3290, Train: 96.43%, Valid: 66.80% Test: 71.00%
Split: 01, Run: 02, Epoch: 400, Loss: 9.8036, Train: 100.00%, Valid: 31.00% Test: 34.30%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2900, Train: 97.14%, Valid: 67.20% Test: 72.20%
Split: 01, Run: 02, Epoch: 500, Loss: 10.0199, Train: 100.00%, Valid: 34.60% Test: 38.30%
Split: 01, Run: 02
None time:  8.49659482203424
None Run 02:
Highest Train: 100.00
Highest Valid: 44.60
  Final Train: 86.43
   Final Test: 43.10
Split: 01, Run: 03, Epoch: 100, Loss: 10.3738, Train: 98.57%, Valid: 40.40% Test: 43.30%
Split: 01, Run: 03, Epoch: 400, Loss: 0.3108, Train: 97.86%, Valid: 66.20% Test: 70.30%
Split: 01, Run: 03, Epoch: 200, Loss: 9.5147, Train: 98.57%, Valid: 30.20% Test: 34.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2528, Train: 96.43%, Valid: 69.20% Test: 71.60%
Split: 01, Run: 03
None time:  12.142949951812625
None Run 03:
Highest Train: 98.57
Highest Valid: 69.60
  Final Train: 96.43
   Final Test: 71.60
total time:  39.22021420672536
None All runs:
Highest Train: 98.81 Â± 0.41
Highest Valid: 69.27 Â± 0.31
  Final Train: 96.90 Â± 0.82
   Final Test: 71.90 Â± 0.36
[32m[I 2021-07-26 08:57:46,646][0m Trial 0 finished with value: 69.26666259765625 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}. Best is trial 0 with value: 69.26666259765625.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}   trial.value:  69.267    {'train': '96.90 Â± 0.82', 'valid': '69.27 Â± 0.31', 'test': '71.90 Â± 0.36'}
test_acc
['71.90 Â± 0.36']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.05, 'K': 10}
Best trial Value:  69.26666259765625
Best trial Acc:  {'train': '96.90 Â± 0.82', 'valid': '69.27 Â± 0.31', 'test': '71.90 Â± 0.36'}
optuna total time:  39.2343921456486
Split: 01, Run: 03, Epoch: 300, Loss: 10.7391, Train: 100.00%, Valid: 29.00% Test: 33.40%
Split: 01, Run: 03, Epoch: 400, Loss: 10.2775, Train: 100.00%, Valid: 31.40% Test: 31.80%
Split: 01, Run: 03, Epoch: 500, Loss: 10.2011, Train: 100.00%, Valid: 29.80% Test: 32.20%
Split: 01, Run: 03
None time:  8.505271386355162
None Run 03:
Highest Train: 100.00
Highest Valid: 53.60
  Final Train: 89.29
   Final Test: 53.80
total time:  25.409735118970275
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 42.60 Â± 12.12
  Final Train: 91.90 Â± 7.15
   Final Test: 42.43 Â± 11.71
[32m[I 2021-07-26 08:57:50,619][0m Trial 4 finished with value: 42.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}   trial.value:  63.4    {'train': '91.19 Â± 3.38', 'valid': '63.40 Â± 5.72', 'test': '64.47 Â± 8.04'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}   trial.value:  60.6    {'train': '93.81 Â± 4.06', 'valid': '60.60 Â± 8.64', 'test': '60.53 Â± 11.02'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}   trial.value:  54.467    {'train': '94.05 Â± 7.96', 'valid': '54.47 Â± 11.59', 'test': '53.90 Â± 13.19'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}   trial.value:  42.6    {'train': '91.90 Â± 7.15', 'valid': '42.60 Â± 12.12', 'test': '42.43 Â± 11.71'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}   trial.value:  31.467    {'train': '93.81 Â± 8.28', 'valid': '31.47 Â± 8.14', 'test': '32.33 Â± 7.44'}
test_acc
['64.47 Â± 8.04', '60.53 Â± 11.02', '53.90 Â± 13.19', '42.43 Â± 11.71', '32.33 Â± 7.44']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}
Best trial Value:  63.40000534057617
Best trial Acc:  {'train': '91.19 Â± 3.38', 'valid': '63.40 Â± 5.72', 'test': '64.47 Â± 8.04'}
optuna total time:  121.81913363747299
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:58:15,933][0m A new study created in memory with name: no-name-7b146107-3eef-4549-8607-919c93146264[0m
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3841, Train: 99.29%, Valid: 67.60% Test: 72.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2854, Train: 98.57%, Valid: 67.20% Test: 70.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2598, Train: 99.29%, Valid: 67.80% Test: 70.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.2177, Train: 99.29%, Valid: 67.60% Test: 70.90%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2514, Train: 99.29%, Valid: 68.00% Test: 70.80%
Split: 01, Run: 01
None time:  12.021954875439405
None Run 01:
Highest Train: 100.00
Highest Valid: 69.80
  Final Train: 99.29
   Final Test: 72.00
Split: 01, Run: 02, Epoch: 100, Loss: 0.4021, Train: 100.00%, Valid: 66.80% Test: 71.80%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2533, Train: 99.29%, Valid: 67.40% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2370, Train: 100.00%, Valid: 66.20% Test: 70.60%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2016, Train: 100.00%, Valid: 67.80% Test: 71.30%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2087, Train: 99.29%, Valid: 66.00% Test: 71.40%
Split: 01, Run: 02
None time:  12.193197630345821
None Run 02:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 99.29
   Final Test: 71.80
Split: 01, Run: 03, Epoch: 100, Loss: 0.4084, Train: 99.29%, Valid: 67.40% Test: 71.90%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2934, Train: 99.29%, Valid: 68.20% Test: 71.90%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2293, Train: 99.29%, Valid: 66.60% Test: 71.70%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2670, Train: 100.00%, Valid: 68.00% Test: 71.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2188, Train: 98.57%, Valid: 69.40% Test: 71.10%
Split: 01, Run: 03
None time:  12.47719999961555
None Run 03:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 98.57
   Final Test: 72.60
total time:  38.345687905326486
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.07 Â± 0.23
  Final Train: 99.05 Â± 0.41
   Final Test: 72.13 Â± 0.42
[32m[I 2021-07-26 08:58:54,313][0m Trial 0 finished with value: 70.0666732788086 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}. Best is trial 0 with value: 70.0666732788086.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}   trial.value:  70.067    {'train': '99.05 Â± 0.41', 'valid': '70.07 Â± 0.23', 'test': '72.13 Â± 0.42'}
test_acc
['72.13 Â± 0.42']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.1, 'K': 10}
Best trial Value:  70.0666732788086
Best trial Acc:  {'train': '99.05 Â± 0.41', 'valid': '70.07 Â± 0.23', 'test': '72.13 Â± 0.42'}
optuna total time:  38.3844505045563
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.15, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.0], 'lambda2': [0.0], 'alpha': [0.15], 'dropout': [0.5], 'K': [10]}
num_trial:  1
[32m[I 2021-07-26 08:59:57,481][0m A new study created in memory with name: no-name-4f208ce4-e562-4f59-ae66-d374124b42aa[0m
K:  10
alpha:  0.15
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.15, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.0, lambda2=0.0, log_steps=100, loss='CE', lr=0.01, model='APPNP', normalize_features=True, num_layers=2, ogb=False, prop='APPNP', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
APPNP(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): EMP(K=10, mode=APPNP, lambda1=None, lambda2=None, L21=True, alpha=0.15)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3520, Train: 100.00%, Valid: 68.00% Test: 71.60%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2580, Train: 100.00%, Valid: 65.60% Test: 70.40%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2261, Train: 100.00%, Valid: 67.00% Test: 69.80%
Split: 01, Run: 01, Epoch: 400, Loss: 0.1833, Train: 100.00%, Valid: 68.40% Test: 71.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.2211, Train: 100.00%, Valid: 68.20% Test: 71.50%
Split: 01, Run: 01
None time:  13.128926277160645
None Run 01:
Highest Train: 100.00
Highest Valid: 70.00
  Final Train: 100.00
   Final Test: 70.60
Split: 01, Run: 02, Epoch: 100, Loss: 0.3658, Train: 100.00%, Valid: 67.80% Test: 71.30%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2320, Train: 100.00%, Valid: 67.00% Test: 70.80%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2091, Train: 100.00%, Valid: 67.80% Test: 69.20%
Split: 01, Run: 02, Epoch: 400, Loss: 0.1805, Train: 100.00%, Valid: 68.20% Test: 70.70%
Split: 01, Run: 02, Epoch: 500, Loss: 0.1873, Train: 100.00%, Valid: 67.00% Test: 69.70%
Split: 01, Run: 02
None time:  11.72811876796186
None Run 02:
Highest Train: 100.00
Highest Valid: 70.40
  Final Train: 100.00
   Final Test: 70.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.3714, Train: 100.00%, Valid: 68.40% Test: 71.60%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2594, Train: 100.00%, Valid: 67.40% Test: 71.00%
Split: 01, Run: 03, Epoch: 300, Loss: 0.2173, Train: 100.00%, Valid: 66.60% Test: 71.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.2458, Train: 100.00%, Valid: 68.60% Test: 71.30%
Split: 01, Run: 03, Epoch: 500, Loss: 0.2019, Train: 100.00%, Valid: 68.40% Test: 71.50%
Split: 01, Run: 03
None time:  11.782176790758967
None Run 03:
Highest Train: 100.00
Highest Valid: 70.20
  Final Train: 100.00
   Final Test: 71.80
total time:  38.26984286867082
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 70.20 Â± 0.20
  Final Train: 100.00 Â± 0.00
   Final Test: 71.03 Â± 0.67
[32m[I 2021-07-26 09:00:35,785][0m Trial 0 finished with value: 70.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.15, 'K': 10}. Best is trial 0 with value: 70.20000457763672.[0m
Study statistics: 
  Number of finished trials:  1
  Number of pruned trials:  0
  Number of complete trials:  1
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.15, 'K': 10}   trial.value:  70.2    {'train': '100.00 Â± 0.00', 'valid': '70.20 Â± 0.20', 'test': '71.03 Â± 0.67'}
test_acc
['71.03 Â± 0.67']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'alpha': 0.15, 'K': 10}
Best trial Value:  70.20000457763672
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '70.20 Â± 0.20', 'test': '71.03 Â± 0.67'}
optuna total time:  38.308873385190964
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.2], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 09:04:53,610][0m A new study created in memory with name: no-name-9b28c4b6-a086-4674-a6f0-dc7b26ce8662[0m
lambda1:  0.05
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 48.5478, Train: 97.86%, Valid: 52.20% Test: 49.70%
Split: 01, Run: 01, Epoch: 200, Loss: 44.4569, Train: 99.29%, Valid: 49.60% Test: 49.40%
Split: 01, Run: 01, Epoch: 300, Loss: 44.7721, Train: 100.00%, Valid: 46.00% Test: 47.10%
Split: 01, Run: 01, Epoch: 400, Loss: 44.1177, Train: 100.00%, Valid: 49.80% Test: 50.80%
Split: 01, Run: 01, Epoch: 500, Loss: 43.0280, Train: 100.00%, Valid: 51.40% Test: 50.30%
Split: 01, Run: 01
None time:  8.159922819584608
None Run 01:
Highest Train: 100.00
Highest Valid: 55.00
  Final Train: 97.14
   Final Test: 54.70
Split: 01, Run: 02, Epoch: 100, Loss: 46.0467, Train: 99.29%, Valid: 51.40% Test: 52.40%
Split: 01, Run: 02, Epoch: 200, Loss: 42.6518, Train: 100.00%, Valid: 49.80% Test: 50.70%
Split: 01, Run: 02, Epoch: 300, Loss: 44.0199, Train: 100.00%, Valid: 52.40% Test: 51.80%
Split: 01, Run: 02, Epoch: 400, Loss: 44.0556, Train: 100.00%, Valid: 54.40% Test: 52.90%
Split: 01, Run: 02, Epoch: 500, Loss: 44.4329, Train: 100.00%, Valid: 53.40% Test: 52.70%
Split: 01, Run: 02
None time:  7.819097189232707
None Run 02:
Highest Train: 100.00
Highest Valid: 59.80
  Final Train: 100.00
   Final Test: 57.00
Split: 01, Run: 03, Epoch: 100, Loss: 48.3324, Train: 99.29%, Valid: 52.40% Test: 55.50%
Split: 01, Run: 03, Epoch: 200, Loss: 43.9905, Train: 100.00%, Valid: 50.80% Test: 51.90%
Split: 01, Run: 03, Epoch: 300, Loss: 44.0546, Train: 100.00%, Valid: 51.00% Test: 51.50%
Split: 01, Run: 03, Epoch: 400, Loss: 42.0991, Train: 100.00%, Valid: 52.00% Test: 51.70%
Split: 01, Run: 03, Epoch: 500, Loss: 40.5678, Train: 100.00%, Valid: 51.00% Test: 50.50%
Split: 01, Run: 03
None time:  7.973709834739566
None Run 03:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 97.86
   Final Test: 74.30
total time:  25.621935265138745
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.27 Â± 8.76
  Final Train: 98.33 Â± 1.49
   Final Test: 62.00 Â± 10.71
[32m[I 2021-07-26 09:05:19,250][0m Trial 0 finished with value: 62.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.2, 'K': 10}. Best is trial 0 with value: 62.266666412353516.[0m
lambda1:  0.02
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 84.8885, Train: 97.14%, Valid: 61.80% Test: 60.00%
Split: 01, Run: 01, Epoch: 200, Loss: 77.9255, Train: 98.57%, Valid: 60.80% Test: 58.70%
Split: 01, Run: 01, Epoch: 300, Loss: 78.9633, Train: 99.29%, Valid: 60.00% Test: 58.40%
Split: 01, Run: 01, Epoch: 400, Loss: 74.0747, Train: 99.29%, Valid: 62.20% Test: 63.40%
Split: 01, Run: 01, Epoch: 500, Loss: 71.2802, Train: 99.29%, Valid: 60.80% Test: 59.20%
Split: 01, Run: 01
None time:  7.925946058705449
None Run 01:
Highest Train: 99.29
Highest Valid: 65.00
  Final Train: 96.43
   Final Test: 62.50
Split: 01, Run: 02, Epoch: 100, Loss: 80.0455, Train: 97.86%, Valid: 62.20% Test: 62.10%
Split: 01, Run: 02, Epoch: 200, Loss: 72.4895, Train: 100.00%, Valid: 60.00% Test: 60.90%
Split: 01, Run: 02, Epoch: 300, Loss: 73.0259, Train: 100.00%, Valid: 60.20% Test: 60.00%
Split: 01, Run: 02, Epoch: 400, Loss: 73.9765, Train: 100.00%, Valid: 61.20% Test: 61.60%
Split: 01, Run: 02, Epoch: 500, Loss: 72.8124, Train: 100.00%, Valid: 60.80% Test: 60.70%
Split: 01, Run: 02
None time:  7.862035049125552
None Run 02:
Highest Train: 100.00
Highest Valid: 65.80
  Final Train: 98.57
   Final Test: 62.20
Split: 01, Run: 03, Epoch: 100, Loss: 82.0134, Train: 98.57%, Valid: 62.40% Test: 64.10%
Split: 01, Run: 03, Epoch: 200, Loss: 74.0743, Train: 99.29%, Valid: 61.60% Test: 63.30%
Split: 01, Run: 03, Epoch: 300, Loss: 73.3861, Train: 99.29%, Valid: 63.00% Test: 61.90%
Split: 01, Run: 03, Epoch: 400, Loss: 70.7398, Train: 99.29%, Valid: 63.00% Test: 62.80%
Split: 01, Run: 03, Epoch: 500, Loss: 67.2295, Train: 99.29%, Valid: 62.00% Test: 61.60%
Split: 01, Run: 03
None time:  7.871488941833377
None Run 03:
Highest Train: 100.00
Highest Valid: 74.60
  Final Train: 97.86
   Final Test: 75.30
total time:  23.72269675694406
None All runs:
Highest Train: 99.76 Â± 0.41
Highest Valid: 68.47 Â± 5.33
  Final Train: 97.62 Â± 1.09
   Final Test: 66.67 Â± 7.48
[32m[I 2021-07-26 09:05:42,978][0m Trial 1 finished with value: 68.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.2, 'K': 10}. Best is trial 1 with value: 68.46666717529297.[0m
lambda1:  0.2
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 11.9795, Train: 98.57%, Valid: 22.60% Test: 25.20%
Split: 01, Run: 01, Epoch: 200, Loss: 11.3007, Train: 100.00%, Valid: 23.60% Test: 25.90%
Split: 01, Run: 01, Epoch: 300, Loss: 11.4263, Train: 100.00%, Valid: 25.40% Test: 28.00%
Split: 01, Run: 01, Epoch: 400, Loss: 11.2730, Train: 100.00%, Valid: 25.60% Test: 28.50%
Split: 01, Run: 01, Epoch: 500, Loss: 11.0529, Train: 100.00%, Valid: 24.40% Test: 26.20%
Split: 01, Run: 01
None time:  7.992037070915103
None Run 01:
Highest Train: 100.00
Highest Valid: 27.40
  Final Train: 100.00
   Final Test: 28.90
Split: 01, Run: 02, Epoch: 100, Loss: 11.8716, Train: 100.00%, Valid: 28.20% Test: 30.80%
Split: 01, Run: 02, Epoch: 200, Loss: 11.1086, Train: 100.00%, Valid: 27.60% Test: 30.10%
Split: 01, Run: 02, Epoch: 300, Loss: 11.7327, Train: 100.00%, Valid: 30.20% Test: 31.20%
Split: 01, Run: 02, Epoch: 400, Loss: 12.4346, Train: 100.00%, Valid: 31.80% Test: 35.80%
Split: 01, Run: 02, Epoch: 500, Loss: 12.1777, Train: 100.00%, Valid: 32.40% Test: 36.90%
Split: 01, Run: 02
None time:  7.88158006221056
None Run 02:
Highest Train: 100.00
Highest Valid: 44.60
  Final Train: 90.00
   Final Test: 43.50
Split: 01, Run: 03, Epoch: 100, Loss: 12.3128, Train: 100.00%, Valid: 34.60% Test: 37.20%
Split: 01, Run: 03, Epoch: 200, Loss: 12.1992, Train: 100.00%, Valid: 28.20% Test: 31.70%
Split: 01, Run: 03, Epoch: 300, Loss: 12.1758, Train: 100.00%, Valid: 27.20% Test: 30.70%
Split: 01, Run: 03, Epoch: 400, Loss: 11.2937, Train: 100.00%, Valid: 25.80% Test: 29.50%
Split: 01, Run: 03, Epoch: 500, Loss: 11.6072, Train: 100.00%, Valid: 27.60% Test: 30.00%
Split: 01, Run: 03
None time:  7.844461703673005
None Run 03:
Highest Train: 100.00
Highest Valid: 54.80
  Final Train: 96.43
   Final Test: 54.30
total time:  23.786341225728393
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 42.27 Â± 13.85
  Final Train: 95.48 Â± 5.07
   Final Test: 42.23 Â± 12.75
[32m[I 2021-07-26 09:06:06,772][0m Trial 2 finished with value: 42.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.2, 'K': 10}. Best is trial 1 with value: 68.46666717529297.[0m
lambda1:  0.01
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 109.6316, Train: 97.14%, Valid: 64.40% Test: 63.70%
Split: 01, Run: 01, Epoch: 200, Loss: 100.7328, Train: 97.86%, Valid: 64.00% Test: 64.20%
Split: 01, Run: 01, Epoch: 300, Loss: 99.7572, Train: 99.29%, Valid: 63.80% Test: 63.10%
Split: 01, Run: 01, Epoch: 400, Loss: 91.7349, Train: 99.29%, Valid: 64.80% Test: 66.00%
Split: 01, Run: 01, Epoch: 500, Loss: 89.4412, Train: 99.29%, Valid: 62.00% Test: 61.60%
Split: 01, Run: 01
None time:  7.8776088450104
None Run 01:
Highest Train: 100.00
Highest Valid: 67.40
  Final Train: 97.14
   Final Test: 66.20
Split: 01, Run: 02, Epoch: 100, Loss: 103.2766, Train: 98.57%, Valid: 62.60% Test: 63.90%
Split: 01, Run: 02, Epoch: 200, Loss: 91.7686, Train: 99.29%, Valid: 62.20% Test: 63.70%
Split: 01, Run: 02, Epoch: 300, Loss: 92.9227, Train: 100.00%, Valid: 61.60% Test: 62.90%
Split: 01, Run: 02, Epoch: 400, Loss: 92.9202, Train: 100.00%, Valid: 63.20% Test: 62.80%
Split: 01, Run: 02, Epoch: 500, Loss: 89.2651, Train: 100.00%, Valid: 62.60% Test: 62.50%
Split: 01, Run: 02
None time:  7.790967736393213
None Run 02:
Highest Train: 100.00
Highest Valid: 67.60
  Final Train: 98.57
   Final Test: 65.50
Split: 01, Run: 03, Epoch: 100, Loss: 103.3932, Train: 97.14%, Valid: 64.20% Test: 65.00%
Split: 01, Run: 03, Epoch: 200, Loss: 93.2936, Train: 99.29%, Valid: 63.40% Test: 64.30%
Split: 01, Run: 03, Epoch: 300, Loss: 91.3074, Train: 99.29%, Valid: 63.00% Test: 63.70%
Split: 01, Run: 03, Epoch: 400, Loss: 87.9581, Train: 99.29%, Valid: 64.20% Test: 64.50%
Split: 01, Run: 03, Epoch: 500, Loss: 87.5047, Train: 99.29%, Valid: 62.40% Test: 64.00%
Split: 01, Run: 03
None time:  7.769670782610774
None Run 03:
Highest Train: 99.29
Highest Valid: 73.40
  Final Train: 97.86
   Final Test: 75.30
total time:  23.49114714190364
None All runs:
Highest Train: 99.76 Â± 0.41
Highest Valid: 69.47 Â± 3.41
  Final Train: 97.86 Â± 0.71
   Final Test: 69.00 Â± 5.47
[32m[I 2021-07-26 09:06:30,269][0m Trial 3 finished with value: 69.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.2, 'K': 10}. Best is trial 3 with value: 69.46666717529297.[0m
lambda1:  0.1
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 26.3044, Train: 99.29%, Valid: 37.40% Test: 39.00%
Split: 01, Run: 01, Epoch: 200, Loss: 24.0488, Train: 100.00%, Valid: 33.60% Test: 36.10%
Split: 01, Run: 01, Epoch: 300, Loss: 24.9442, Train: 100.00%, Valid: 34.80% Test: 37.90%
Split: 01, Run: 01, Epoch: 400, Loss: 23.9012, Train: 100.00%, Valid: 38.00% Test: 40.70%
Split: 01, Run: 01, Epoch: 500, Loss: 23.7284, Train: 100.00%, Valid: 36.80% Test: 38.60%
Split: 01, Run: 01
None time:  7.738776927813888
None Run 01:
Highest Train: 100.00
Highest Valid: 39.20
  Final Train: 100.00
   Final Test: 41.00
Split: 01, Run: 02, Epoch: 100, Loss: 25.3909, Train: 100.00%, Valid: 41.40% Test: 42.50%
Split: 01, Run: 02, Epoch: 200, Loss: 24.4655, Train: 100.00%, Valid: 37.60% Test: 40.70%
Split: 01, Run: 02, Epoch: 300, Loss: 24.0742, Train: 100.00%, Valid: 38.20% Test: 41.10%
Split: 01, Run: 02, Epoch: 400, Loss: 25.0777, Train: 100.00%, Valid: 39.40% Test: 41.20%
Split: 01, Run: 02, Epoch: 500, Loss: 25.3509, Train: 100.00%, Valid: 38.00% Test: 40.20%
Split: 01, Run: 02
None time:  7.736871233209968
None Run 02:
Highest Train: 100.00
Highest Valid: 52.20
  Final Train: 90.71
   Final Test: 49.50
Split: 01, Run: 03, Epoch: 100, Loss: 26.6331, Train: 100.00%, Valid: 45.80% Test: 46.80%
Split: 01, Run: 03, Epoch: 200, Loss: 25.0061, Train: 100.00%, Valid: 41.00% Test: 43.50%
Split: 01, Run: 03, Epoch: 300, Loss: 26.1668, Train: 100.00%, Valid: 37.80% Test: 41.70%
Split: 01, Run: 03, Epoch: 400, Loss: 24.8739, Train: 100.00%, Valid: 36.80% Test: 40.30%
Split: 01, Run: 03, Epoch: 500, Loss: 23.7292, Train: 100.00%, Valid: 37.40% Test: 40.90%
Split: 01, Run: 03
None time:  7.892438033595681
None Run 03:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 98.57
   Final Test: 69.20
total time:  23.430689252912998
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 53.60 Â± 15.15
  Final Train: 96.43 Â± 5.00
   Final Test: 53.23 Â± 14.47
[32m[I 2021-07-26 09:06:53,705][0m Trial 4 finished with value: 53.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}. Best is trial 3 with value: 69.46666717529297.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.2, 'K': 10}   trial.value:  69.467    {'train': '97.86 Â± 0.71', 'valid': '69.47 Â± 3.41', 'test': '69.00 Â± 5.47'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.2, 'K': 10}   trial.value:  68.467    {'train': '97.62 Â± 1.09', 'valid': '68.47 Â± 5.33', 'test': '66.67 Â± 7.48'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.2, 'K': 10}   trial.value:  62.267    {'train': '98.33 Â± 1.49', 'valid': '62.27 Â± 8.76', 'test': '62.00 Â± 10.71'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}   trial.value:  53.6    {'train': '96.43 Â± 5.00', 'valid': '53.60 Â± 15.15', 'test': '53.23 Â± 14.47'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.2, 'K': 10}   trial.value:  42.267    {'train': '95.48 Â± 5.07', 'valid': '42.27 Â± 13.85', 'test': '42.23 Â± 12.75'}
test_acc
['69.00 Â± 5.47', '66.67 Â± 7.48', '62.00 Â± 10.71', '53.23 Â± 14.47', '42.23 Â± 12.75']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.2, 'K': 10}
Best trial Value:  69.46666717529297
Best trial Acc:  {'train': '97.86 Â± 0.71', 'valid': '69.47 Â± 3.41', 'test': '69.00 Â± 5.47'}
optuna total time:  120.10279251076281
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.3], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 09:07:39,221][0m A new study created in memory with name: no-name-9a4b0e58-5677-4e16-9c32-4988b8e737b0[0m
lambda1:  0.01
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 166.9718, Train: 97.86%, Valid: 66.60% Test: 67.10%
Split: 01, Run: 01, Epoch: 200, Loss: 151.3442, Train: 99.29%, Valid: 68.80% Test: 68.30%
Split: 01, Run: 01, Epoch: 300, Loss: 149.0070, Train: 99.29%, Valid: 67.00% Test: 68.00%
Split: 01, Run: 01, Epoch: 400, Loss: 137.3037, Train: 100.00%, Valid: 69.20% Test: 70.10%
Split: 01, Run: 01, Epoch: 500, Loss: 133.5724, Train: 100.00%, Valid: 66.20% Test: 65.70%
Split: 01, Run: 01
None time:  8.134214743971825
None Run 01:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 98.57
   Final Test: 69.50
Split: 01, Run: 02, Epoch: 100, Loss: 157.5476, Train: 99.29%, Valid: 65.80% Test: 66.60%
Split: 01, Run: 02, Epoch: 200, Loss: 138.4446, Train: 100.00%, Valid: 65.40% Test: 66.20%
Split: 01, Run: 02, Epoch: 300, Loss: 137.9323, Train: 100.00%, Valid: 67.60% Test: 67.70%
Split: 01, Run: 02, Epoch: 400, Loss: 134.9730, Train: 100.00%, Valid: 68.20% Test: 68.60%
Split: 01, Run: 02, Epoch: 500, Loss: 134.9282, Train: 100.00%, Valid: 69.80% Test: 70.50%
Split: 01, Run: 02
None time:  7.919746208935976
None Run 02:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 73.00
Split: 01, Run: 03, Epoch: 100, Loss: 157.6188, Train: 97.86%, Valid: 70.00% Test: 70.40%
Split: 01, Run: 03, Epoch: 200, Loss: 147.1299, Train: 99.29%, Valid: 66.00% Test: 67.40%
Split: 01, Run: 03, Epoch: 300, Loss: 137.8564, Train: 100.00%, Valid: 63.60% Test: 64.00%
Split: 01, Run: 03, Epoch: 400, Loss: 133.4910, Train: 100.00%, Valid: 66.40% Test: 66.60%
Split: 01, Run: 03, Epoch: 500, Loss: 129.6924, Train: 100.00%, Valid: 65.40% Test: 66.50%
Split: 01, Run: 03
None time:  7.991449899971485
None Run 03:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 97.86
   Final Test: 75.40
total time:  25.703278934583068
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.93 Â± 2.52
  Final Train: 98.81 Â± 1.09
   Final Test: 72.63 Â± 2.97
[32m[I 2021-07-26 09:08:04,934][0m Trial 0 finished with value: 72.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 72.9333267211914.[0m
lambda1:  0.05
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 79.0553, Train: 98.57%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 01, Epoch: 200, Loss: 73.4674, Train: 100.00%, Valid: 56.40% Test: 54.20%
Split: 01, Run: 01, Epoch: 300, Loss: 74.9861, Train: 100.00%, Valid: 54.40% Test: 54.70%
Split: 01, Run: 01, Epoch: 400, Loss: 71.4650, Train: 100.00%, Valid: 58.00% Test: 58.50%
Split: 01, Run: 01, Epoch: 500, Loss: 69.5995, Train: 100.00%, Valid: 55.00% Test: 55.00%
Split: 01, Run: 01
None time:  8.206172071397305
None Run 01:
Highest Train: 100.00
Highest Valid: 59.00
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 02, Epoch: 100, Loss: 76.0807, Train: 100.00%, Valid: 54.00% Test: 55.80%
Split: 01, Run: 02, Epoch: 200, Loss: 69.5618, Train: 100.00%, Valid: 52.40% Test: 54.20%
Split: 01, Run: 02, Epoch: 300, Loss: 70.3654, Train: 100.00%, Valid: 53.20% Test: 54.10%
Split: 01, Run: 02, Epoch: 400, Loss: 68.6444, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 02, Epoch: 500, Loss: 70.5757, Train: 100.00%, Valid: 53.80% Test: 55.30%
Split: 01, Run: 02
None time:  7.889962624758482
None Run 02:
Highest Train: 100.00
Highest Valid: 63.60
  Final Train: 100.00
   Final Test: 61.10
Split: 01, Run: 03, Epoch: 100, Loss: 79.1354, Train: 98.57%, Valid: 55.40% Test: 57.70%
Split: 01, Run: 03, Epoch: 200, Loss: 73.2346, Train: 100.00%, Valid: 57.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 300, Loss: 72.2716, Train: 100.00%, Valid: 56.20% Test: 53.50%
Split: 01, Run: 03, Epoch: 400, Loss: 70.6230, Train: 100.00%, Valid: 60.40% Test: 57.50%
Split: 01, Run: 03, Epoch: 500, Loss: 66.7445, Train: 100.00%, Valid: 57.00% Test: 55.50%
Split: 01, Run: 03
None time:  7.863589441403747
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 98.57
   Final Test: 75.60
total time:  24.031979274004698
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 66.20 Â± 8.79
  Final Train: 99.52 Â± 0.82
   Final Test: 64.47 Â± 9.89
[32m[I 2021-07-26 09:08:28,971][0m Trial 1 finished with value: 66.19999694824219 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 72.9333267211914.[0m
lambda1:  0.2
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 21.8876, Train: 100.00%, Valid: 27.20% Test: 28.90%
Split: 01, Run: 01, Epoch: 200, Loss: 20.8370, Train: 100.00%, Valid: 26.80% Test: 30.00%
Split: 01, Run: 01, Epoch: 300, Loss: 21.0666, Train: 100.00%, Valid: 28.20% Test: 31.70%
Split: 01, Run: 01, Epoch: 400, Loss: 19.1466, Train: 100.00%, Valid: 29.40% Test: 32.70%
Split: 01, Run: 01, Epoch: 500, Loss: 19.7309, Train: 100.00%, Valid: 29.80% Test: 31.80%
Split: 01, Run: 01
None time:  7.9138426054269075
None Run 01:
Highest Train: 100.00
Highest Valid: 31.80
  Final Train: 100.00
   Final Test: 33.80
Split: 01, Run: 02, Epoch: 100, Loss: 21.5107, Train: 100.00%, Valid: 32.80% Test: 34.80%
Split: 01, Run: 02, Epoch: 200, Loss: 20.6675, Train: 100.00%, Valid: 35.00% Test: 37.80%
Split: 01, Run: 02, Epoch: 300, Loss: 20.0016, Train: 100.00%, Valid: 34.40% Test: 36.00%
Split: 01, Run: 02, Epoch: 400, Loss: 20.7525, Train: 100.00%, Valid: 33.60% Test: 36.50%
Split: 01, Run: 02, Epoch: 500, Loss: 21.8434, Train: 100.00%, Valid: 32.20% Test: 35.40%
Split: 01, Run: 02
None time:  8.157320918515325
None Run 02:
Highest Train: 100.00
Highest Valid: 52.00
  Final Train: 92.14
   Final Test: 50.40
Split: 01, Run: 03, Epoch: 100, Loss: 22.7874, Train: 100.00%, Valid: 31.80% Test: 33.60%
Split: 01, Run: 03, Epoch: 200, Loss: 21.9644, Train: 100.00%, Valid: 33.40% Test: 36.40%
Split: 01, Run: 03, Epoch: 300, Loss: 21.5830, Train: 100.00%, Valid: 32.20% Test: 35.60%
Split: 01, Run: 03, Epoch: 400, Loss: 20.7197, Train: 100.00%, Valid: 30.40% Test: 32.80%
Split: 01, Run: 03, Epoch: 500, Loss: 20.3357, Train: 100.00%, Valid: 31.80% Test: 34.70%
Split: 01, Run: 03
None time:  7.901346787810326
None Run 03:
Highest Train: 100.00
Highest Valid: 63.40
  Final Train: 97.14
   Final Test: 64.60
total time:  24.036073610186577
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 49.07 Â± 16.00
  Final Train: 96.43 Â± 3.98
   Final Test: 49.60 Â± 15.42
[32m[I 2021-07-26 09:08:53,013][0m Trial 2 finished with value: 49.06666946411133 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 72.9333267211914.[0m
lambda1:  0.02
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 132.8807, Train: 97.86%, Valid: 63.40% Test: 64.10%
Split: 01, Run: 01, Epoch: 200, Loss: 121.7695, Train: 99.29%, Valid: 65.00% Test: 64.80%
Split: 01, Run: 01, Epoch: 300, Loss: 121.6400, Train: 99.29%, Valid: 64.60% Test: 65.00%
Split: 01, Run: 01, Epoch: 400, Loss: 111.3262, Train: 99.29%, Valid: 66.20% Test: 67.10%
Split: 01, Run: 01, Epoch: 500, Loss: 108.4217, Train: 99.29%, Valid: 62.20% Test: 64.30%
Split: 01, Run: 01
None time:  7.8083383068442345
None Run 01:
Highest Train: 100.00
Highest Valid: 68.20
  Final Train: 97.14
   Final Test: 66.30
Split: 01, Run: 02, Epoch: 100, Loss: 125.4348, Train: 99.29%, Valid: 63.80% Test: 64.20%
Split: 01, Run: 02, Epoch: 200, Loss: 112.5869, Train: 100.00%, Valid: 63.20% Test: 64.30%
Split: 01, Run: 02, Epoch: 300, Loss: 110.5237, Train: 100.00%, Valid: 64.20% Test: 63.40%
Split: 01, Run: 02, Epoch: 400, Loss: 111.9142, Train: 100.00%, Valid: 63.80% Test: 62.40%
Split: 01, Run: 02, Epoch: 500, Loss: 110.1975, Train: 100.00%, Valid: 64.80% Test: 63.80%
Split: 01, Run: 02
None time:  7.782632188871503
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 99.29
   Final Test: 67.40
Split: 01, Run: 03, Epoch: 100, Loss: 127.5484, Train: 98.57%, Valid: 65.80% Test: 68.10%
Split: 01, Run: 03, Epoch: 200, Loss: 118.2612, Train: 100.00%, Valid: 64.80% Test: 65.80%
Split: 01, Run: 03, Epoch: 300, Loss: 114.5156, Train: 100.00%, Valid: 64.60% Test: 63.20%
Split: 01, Run: 03, Epoch: 400, Loss: 108.3647, Train: 100.00%, Valid: 65.20% Test: 63.60%
Split: 01, Run: 03, Epoch: 500, Loss: 106.3667, Train: 100.00%, Valid: 66.20% Test: 66.30%
Split: 01, Run: 03
None time:  7.795675091445446
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 97.86
   Final Test: 76.60
total time:  23.44505606405437
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.20 Â± 4.20
  Final Train: 98.10 Â± 1.09
   Final Test: 70.10 Â± 5.66
[32m[I 2021-07-26 09:09:16,463][0m Trial 3 finished with value: 71.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 72.9333267211914.[0m
lambda1:  0.1
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 45.9815, Train: 100.00%, Valid: 43.00% Test: 42.70%
Split: 01, Run: 01, Epoch: 200, Loss: 42.1097, Train: 100.00%, Valid: 39.20% Test: 40.20%
Split: 01, Run: 01, Epoch: 300, Loss: 42.0847, Train: 100.00%, Valid: 39.80% Test: 41.40%
Split: 01, Run: 01, Epoch: 400, Loss: 39.8226, Train: 100.00%, Valid: 43.40% Test: 46.50%
Split: 01, Run: 01, Epoch: 500, Loss: 39.3252, Train: 100.00%, Valid: 40.20% Test: 42.00%
Split: 01, Run: 01
None time:  7.761348966509104
None Run 01:
Highest Train: 100.00
Highest Valid: 46.40
  Final Train: 99.29
   Final Test: 48.80
Split: 01, Run: 02, Epoch: 100, Loss: 43.5717, Train: 100.00%, Valid: 44.20% Test: 46.50%
Split: 01, Run: 02, Epoch: 200, Loss: 41.4836, Train: 100.00%, Valid: 42.40% Test: 44.10%
Split: 01, Run: 02, Epoch: 300, Loss: 40.8240, Train: 100.00%, Valid: 42.40% Test: 45.50%
Split: 01, Run: 02, Epoch: 400, Loss: 41.6265, Train: 100.00%, Valid: 44.40% Test: 45.80%
Split: 01, Run: 02, Epoch: 500, Loss: 42.9186, Train: 100.00%, Valid: 43.40% Test: 46.20%
Split: 01, Run: 02
None time:  7.781514391303062
None Run 02:
Highest Train: 100.00
Highest Valid: 55.60
  Final Train: 100.00
   Final Test: 52.90
Split: 01, Run: 03, Epoch: 100, Loss: 45.5398, Train: 100.00%, Valid: 47.60% Test: 48.70%
Split: 01, Run: 03, Epoch: 200, Loss: 42.5134, Train: 100.00%, Valid: 43.60% Test: 46.50%
Split: 01, Run: 03, Epoch: 300, Loss: 41.4094, Train: 100.00%, Valid: 43.00% Test: 46.40%
Split: 01, Run: 03, Epoch: 400, Loss: 40.4475, Train: 100.00%, Valid: 46.00% Test: 48.10%
Split: 01, Run: 03, Epoch: 500, Loss: 38.7198, Train: 100.00%, Valid: 45.00% Test: 46.30%
Split: 01, Run: 03
None time:  7.79712457023561
None Run 03:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 98.57
   Final Test: 73.60
total time:  23.39270820096135
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.93 Â± 14.49
  Final Train: 99.29 Â± 0.71
   Final Test: 58.43 Â± 13.29
[32m[I 2021-07-26 09:09:39,861][0m Trial 4 finished with value: 58.93333435058594 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}. Best is trial 0 with value: 72.9333267211914.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.3, 'K': 10}   trial.value:  72.933    {'train': '98.81 Â± 1.09', 'valid': '72.93 Â± 2.52', 'test': '72.63 Â± 2.97'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.3, 'K': 10}   trial.value:  71.2    {'train': '98.10 Â± 1.09', 'valid': '71.20 Â± 4.20', 'test': '70.10 Â± 5.66'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.3, 'K': 10}   trial.value:  66.2    {'train': '99.52 Â± 0.82', 'valid': '66.20 Â± 8.79', 'test': '64.47 Â± 9.89'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}   trial.value:  58.933    {'train': '99.29 Â± 0.71', 'valid': '58.93 Â± 14.49', 'test': '58.43 Â± 13.29'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.3, 'K': 10}   trial.value:  49.067    {'train': '96.43 Â± 3.98', 'valid': '49.07 Â± 16.00', 'test': '49.60 Â± 15.42'}
test_acc
['72.63 Â± 2.97', '70.10 Â± 5.66', '64.47 Â± 9.89', '58.43 Â± 13.29', '49.60 Â± 15.42']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.3, 'K': 10}
Best trial Value:  72.9333267211914
Best trial Acc:  {'train': '98.81 Â± 1.09', 'valid': '72.93 Â± 2.52', 'test': '72.63 Â± 2.97'}
optuna total time:  120.64779321663082
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.1, 0.2, 0.3, 0.4, 0.5], 'lambda2': [0.1, 0.2, 0.3, 0.4, 0.5], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  25
[32m[I 2021-07-26 09:09:47,814][0m A new study created in memory with name: no-name-fa8497a0-2948-4e6b-ab57-201e33aa204e[0m
lambda1:  0.3
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.3, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 2.3293, Train: 71.43%, Valid: 13.40% Test: 16.20%
Split: 01, Run: 01, Epoch: 200, Loss: 2.4234, Train: 97.86%, Valid: 16.60% Test: 18.70%
Split: 01, Run: 01, Epoch: 300, Loss: 2.4367, Train: 99.29%, Valid: 16.20% Test: 19.50%
Split: 01, Run: 01, Epoch: 400, Loss: 2.3745, Train: 100.00%, Valid: 17.00% Test: 19.40%
Split: 01, Run: 01, Epoch: 500, Loss: 2.2715, Train: 100.00%, Valid: 16.60% Test: 19.90%
Split: 01, Run: 01
None time:  8.019858200103045
None Run 01:
Highest Train: 100.00
Highest Valid: 19.40
  Final Train: 27.86
   Final Test: 18.10
Split: 01, Run: 02, Epoch: 100, Loss: 2.2065, Train: 94.29%, Valid: 31.20% Test: 31.30%
Split: 01, Run: 02, Epoch: 200, Loss: 2.2383, Train: 100.00%, Valid: 27.20% Test: 28.40%
Split: 01, Run: 02, Epoch: 300, Loss: 2.2431, Train: 100.00%, Valid: 21.20% Test: 23.10%
Split: 01, Run: 02, Epoch: 400, Loss: 2.2213, Train: 100.00%, Valid: 21.40% Test: 23.50%
Split: 01, Run: 02, Epoch: 500, Loss: 2.4692, Train: 100.00%, Valid: 19.00% Test: 21.80%
Split: 01, Run: 02
None time:  7.705423109233379
None Run 02:
Highest Train: 100.00
Highest Valid: 32.40
  Final Train: 95.00
   Final Test: 32.20
Split: 01, Run: 03, Epoch: 100, Loss: 2.4049, Train: 90.71%, Valid: 24.80% Test: 24.50%
Split: 01, Run: 03, Epoch: 200, Loss: 2.3597, Train: 99.29%, Valid: 29.20% Test: 30.10%
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.01, 0.02, 0.05, 0.1, 0.2], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  25
[32m[I 2021-07-26 09:10:33,725][0m A new study created in memory with name: no-name-d6b071f1-0b4f-42b4-ab68-951598a8c5bf[0m
lambda1:  0.1
lambda2:  0.05
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.05, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 3.6321, Train: 68.57%, Valid: 16.80% Test: 20.20%
Split: 01, Run: 01, Epoch: 200, Loss: 3.5100, Train: 91.43%, Valid: 18.60% Test: 21.60%
Split: 01, Run: 01, Epoch: 300, Loss: 3.5800, Train: 93.57%, Valid: 19.60% Test: 22.30%
Split: 01, Run: 01, Epoch: 400, Loss: 3.7741, Train: 97.14%, Valid: 20.40% Test: 23.10%
Split: 01, Run: 01, Epoch: 500, Loss: 3.7594, Train: 97.14%, Valid: 19.80% Test: 22.50%
Split: 01, Run: 01
None time:  7.851079843938351
None Run 01:
Highest Train: 98.57
Highest Valid: 21.20
  Final Train: 30.71
   Final Test: 20.90
Split: 01, Run: 02, Epoch: 100, Loss: 3.3865, Train: 87.86%, Valid: 24.60% Test: 28.60%
Split: 01, Run: 02, Epoch: 200, Loss: 3.6116, Train: 95.71%, Valid: 25.40% Test: 27.50%
Split: 01, Run: 02, Epoch: 300, Loss: 3.7422, Train: 99.29%, Valid: 21.80% Test: 24.70%
Split: 01, Run: 02, Epoch: 400, Loss: 3.7207, Train: 99.29%, Valid: 23.20% Test: 25.10%
Split: 01, Run: 02, Epoch: 500, Loss: 3.9560, Train: 98.57%, Valid: 21.40% Test: 24.70%
Split: 01, Run: 02
None time:  7.592734599485993
None Run 02:
Highest Train: 100.00
Highest Valid: 28.40
  Final Train: 95.71
   Final Test: 29.40
Split: 01, Run: 03, Epoch: 100, Loss: 3.8351, Train: 96.43%, Valid: 39.20% Test: 43.00%
Split: 01, Run: 03, Epoch: 200, Loss: 3.4367, Train: 98.57%, Valid: 28.60% Test: 32.40%
Split: 01, Run: 03, Epoch: 300, Loss: 3.7475, Train: 97.14%, Valid: 21.80% Test: 25.00%
Split: 01, Run: 03, Epoch: 400, Loss: 3.4984, Train: 98.57%, Valid: 22.40% Test: 24.70%
Split: 01, Run: 03, Epoch: 500, Loss: 3.4919, Train: 97.86%, Valid: 23.60% Test: 26.10%
Split: 01, Run: 03
None time:  7.597438035532832
None Run 03:
Highest Train: 99.29
Highest Valid: 44.80
  Final Train: 94.29
   Final Test: 45.20
total time:  24.670653488487005
None All runs:
Highest Train: 99.29 Â± 0.71
Highest Valid: 31.47 Â± 12.10
  Final Train: 73.57 Â± 37.12
   Final Test: 31.83 Â± 12.33
[32m[I 2021-07-26 09:10:58,410][0m Trial 0 finished with value: 31.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.05, 'K': 10}. Best is trial 0 with value: 31.46666717529297.[0m
lambda1:  0.1
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 9.9177, Train: 95.00%, Valid: 24.00% Test: 27.20%
Split: 01, Run: 01, Epoch: 200, Loss: 9.2825, Train: 97.14%, Valid: 24.40% Test: 27.00%
Split: 01, Run: 01, Epoch: 300, Loss: 9.4504, Train: 99.29%, Valid: 25.40% Test: 27.50%
Split: 01, Run: 01, Epoch: 400, Loss: 9.4481, Train: 99.29%, Valid: 26.40% Test: 28.30%
Split: 01, Run: 01, Epoch: 500, Loss: 9.6054, Train: 100.00%, Valid: 26.60% Test: 28.70%
Split: 01, Run: 01
None time:  7.506661850959063
None Run 01:
Highest Train: 100.00
Highest Valid: 29.60
  Final Train: 100.00
   Final Test: 30.40
Split: 01, Run: 02, Epoch: 100, Loss: 9.8400, Train: 98.57%, Valid: 35.20% Test: 36.50%
Split: 01, Run: 02, Epoch: 200, Loss: 9.5638, Train: 100.00%, Valid: 32.00% Test: 35.30%
Split: 01, Run: 02, Epoch: 300, Loss: 10.0595, Train: 99.29%, Valid: 28.60% Test: 32.30%
Split: 01, Run: 02, Epoch: 400, Loss: 9.8036, Train: 100.00%, Valid: 31.00% Test: 34.30%
Split: 01, Run: 02, Epoch: 500, Loss: 10.0199, Train: 100.00%, Valid: 34.60% Test: 38.30%
Split: 01, Run: 02
None time:  7.493738450109959
None Run 02:
Highest Train: 100.00
Highest Valid: 44.60
  Final Train: 86.43
   Final Test: 43.10
Split: 01, Run: 03, Epoch: 100, Loss: 10.3738, Train: 98.57%, Valid: 40.40% Test: 43.30%
Split: 01, Run: 03, Epoch: 200, Loss: 9.5147, Train: 98.57%, Valid: 30.20% Test: 34.10%
Split: 01, Run: 03, Epoch: 300, Loss: 10.7391, Train: 100.00%, Valid: 29.00% Test: 33.40%
Split: 01, Run: 03, Epoch: 400, Loss: 10.2775, Train: 100.00%, Valid: 31.40% Test: 31.80%
Split: 01, Run: 03, Epoch: 500, Loss: 10.2011, Train: 100.00%, Valid: 29.80% Test: 32.20%
Split: 01, Run: 03
None time:  7.502307130023837
None Run 03:
Highest Train: 100.00
Highest Valid: 53.60
  Final Train: 89.29
   Final Test: 53.80
total time:  22.563768044114113
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 42.60 Â± 12.12
  Final Train: 91.90 Â± 7.15
   Final Test: 42.43 Â± 11.71
[32m[I 2021-07-26 09:11:20,980][0m Trial 1 finished with value: 42.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}. Best is trial 1 with value: 42.60000228881836.[0m
lambda1:  0.1
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 26.3044, Train: 99.29%, Valid: 37.40% Test: 39.00%
Split: 01, Run: 01, Epoch: 200, Loss: 24.0488, Train: 100.00%, Valid: 33.60% Test: 36.10%
Split: 01, Run: 01, Epoch: 300, Loss: 24.9442, Train: 100.00%, Valid: 34.80% Test: 37.90%
Split: 01, Run: 01, Epoch: 400, Loss: 23.9012, Train: 100.00%, Valid: 38.00% Test: 40.70%
Split: 01, Run: 01, Epoch: 500, Loss: 23.7284, Train: 100.00%, Valid: 36.80% Test: 38.60%
Split: 01, Run: 01
None time:  7.553719200193882
None Run 01:
Highest Train: 100.00
Highest Valid: 39.20
  Final Train: 100.00
   Final Test: 41.00
Split: 01, Run: 02, Epoch: 100, Loss: 25.3909, Train: 100.00%, Valid: 41.40% Test: 42.50%
Split: 01, Run: 02, Epoch: 200, Loss: 24.4655, Train: 100.00%, Valid: 37.60% Test: 40.70%
Split: 01, Run: 02, Epoch: 300, Loss: 24.0742, Train: 100.00%, Valid: 38.20% Test: 41.10%
Split: 01, Run: 02, Epoch: 400, Loss: 25.0777, Train: 100.00%, Valid: 39.40% Test: 41.20%
Split: 01, Run: 02, Epoch: 500, Loss: 25.3509, Train: 100.00%, Valid: 38.00% Test: 40.20%
Split: 01, Run: 02
None time:  7.512944223359227
None Run 02:
Highest Train: 100.00
Highest Valid: 52.20
  Final Train: 90.71
   Final Test: 49.50
Split: 01, Run: 03, Epoch: 100, Loss: 26.6331, Train: 100.00%, Valid: 45.80% Test: 46.80%
Split: 01, Run: 03, Epoch: 200, Loss: 25.0061, Train: 100.00%, Valid: 41.00% Test: 43.50%
Split: 01, Run: 03, Epoch: 300, Loss: 26.1668, Train: 100.00%, Valid: 37.80% Test: 41.70%
Split: 01, Run: 03, Epoch: 400, Loss: 24.8739, Train: 100.00%, Valid: 36.80% Test: 40.30%
Split: 01, Run: 03, Epoch: 500, Loss: 23.7292, Train: 100.00%, Valid: 37.40% Test: 40.90%
Split: 01, Run: 03
None time:  7.5183267295360565
None Run 03:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 98.57
   Final Test: 69.20
total time:  22.63874746672809
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 53.60 Â± 15.15
  Final Train: 96.43 Â± 5.00
   Final Test: 53.23 Â± 14.47
[32m[I 2021-07-26 09:11:43,624][0m Trial 2 finished with value: 53.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.2
lambda2:  0.01
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.01, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.1712, Train: 16.43%, Valid: 12.40% Test: 13.20%
Split: 01, Run: 01, Epoch: 200, Loss: 0.0804, Train: 19.29%, Valid: 12.20% Test: 13.50%
Split: 01, Run: 01, Epoch: 300, Loss: 0.0748, Train: 25.00%, Valid: 12.20% Test: 13.70%
Split: 01, Run: 01, Epoch: 400, Loss: 0.0836, Train: 27.86%, Valid: 12.20% Test: 13.60%
Split: 01, Run: 01, Epoch: 500, Loss: 0.0836, Train: 32.14%, Valid: 12.40% Test: 13.90%
Split: 01, Run: 01
None time:  7.522375885397196
None Run 01:
Highest Train: 32.14
Highest Valid: 17.20
  Final Train: 25.00
   Final Test: 15.60
Split: 01, Run: 02, Epoch: 100, Loss: 0.0686, Train: 15.71%, Valid: 16.20% Test: 15.00%
Split: 01, Run: 02, Epoch: 200, Loss: 0.0671, Train: 30.71%, Valid: 16.40% Test: 16.40%
Split: 01, Run: 02, Epoch: 300, Loss: 0.0699, Train: 43.57%, Valid: 16.80% Test: 17.00%
Split: 01, Run: 02, Epoch: 400, Loss: 0.0682, Train: 52.86%, Valid: 17.80% Test: 19.80%
Split: 01, Run: 02, Epoch: 500, Loss: 0.0785, Train: 60.00%, Valid: 19.80% Test: 20.00%
Split: 01, Run: 02
None time:  7.546900607645512
None Run 02:
Highest Train: 60.71
Highest Valid: 20.60
  Final Train: 56.43
   Final Test: 21.70
Split: 01, Run: 03, Epoch: 100, Loss: 0.1174, Train: 17.14%, Valid: 16.20% Test: 15.10%
Split: 01, Run: 03, Epoch: 200, Loss: 0.0780, Train: 20.00%, Valid: 16.20% Test: 15.40%
Split: 01, Run: 03, Epoch: 300, Loss: 0.0775, Train: 28.57%, Valid: 16.20% Test: 16.00%
Split: 01, Run: 03, Epoch: 400, Loss: 0.0820, Train: 31.43%, Valid: 16.40% Test: 16.10%
Split: 01, Run: 03, Epoch: 500, Loss: 0.0825, Train: 38.57%, Valid: 16.40% Test: 16.20%
Split: 01, Run: 03
None time:  7.496346881613135
None Run 03:
Highest Train: 39.29
Highest Valid: 19.00
  Final Train: 21.43
   Final Test: 16.30
total time:  22.617363134399056
None All runs:
Highest Train: 44.05 Â± 14.87
Highest Valid: 18.93 Â± 1.70
  Final Train: 34.29 Â± 19.26
   Final Test: 17.87 Â± 3.34
[32m[I 2021-07-26 09:12:06,247][0m Trial 3 finished with value: 18.933334350585938 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.01, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.2
lambda2:  0.02
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.02, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 0.3276, Train: 17.86%, Valid: 12.20% Test: 13.30%
Split: 01, Run: 01, Epoch: 200, Loss: 0.2903, Train: 26.43%, Valid: 12.20% Test: 13.80%
Split: 01, Run: 01, Epoch: 300, Loss: 0.2771, Train: 37.14%, Valid: 12.40% Test: 14.30%
Split: 01, Run: 01, Epoch: 400, Loss: 0.3229, Train: 40.00%, Valid: 12.40% Test: 14.20%
Split: 01, Run: 01, Epoch: 500, Loss: 0.3120, Train: 44.29%, Valid: 12.80% Test: 14.70%
Split: 01, Run: 01
None time:  7.461933899670839
None Run 01:
Highest Train: 47.14
Highest Valid: 18.60
  Final Train: 28.57
   Final Test: 17.10
Split: 01, Run: 02, Epoch: 100, Loss: 0.2479, Train: 38.57%, Valid: 16.60% Test: 17.20%
Split: 01, Run: 02, Epoch: 200, Loss: 0.2646, Train: 57.14%, Valid: 19.00% Test: 20.60%
Split: 01, Run: 02, Epoch: 300, Loss: 0.2837, Train: 73.57%, Valid: 20.60% Test: 22.00%
Split: 01, Run: 02, Epoch: 400, Loss: 0.2707, Train: 89.29%, Valid: 24.20% Test: 25.50%
Split: 01, Run: 02, Epoch: 500, Loss: 0.2774, Train: 94.29%, Valid: 26.20% Test: 26.90%
Split: 01, Run: 02
None time:  7.527762083336711
None Run 02:
Highest Train: 94.29
Highest Valid: 27.20
  Final Train: 93.57
   Final Test: 25.90
Split: 01, Run: 03, Epoch: 100, Loss: 0.3069, Train: 22.14%, Valid: 16.40% Test: 15.50%
Split: 01, Run: 03, Epoch: 200, Loss: 0.2902, Train: 31.43%, Valid: 16.60% Test: 16.30%
Split: 01, Run: 03, Epoch: 300, Loss: 0.3138, Train: 52.14%, Valid: 17.60% Test: 17.70%
Split: 01, Run: 03, Epoch: 400, Loss: 0.3024, Train: 69.29%, Valid: 19.40% Test: 19.50%
Split: 01, Run: 03, Epoch: 500, Loss: 0.3108, Train: 88.57%, Valid: 21.40% Test: 21.70%
Split: 01, Run: 03
None time:  7.529381942003965
None Run 03:
Highest Train: 88.57
Highest Valid: 22.00
  Final Train: 84.29
   Final Test: 21.20
total time:  22.564593171700835
None All runs:
Highest Train: 76.67 Â± 25.73
Highest Valid: 22.60 Â± 4.33
  Final Train: 68.81 Â± 35.16
   Final Test: 21.40 Â± 4.40
[32m[I 2021-07-26 09:12:28,817][0m Trial 4 finished with value: 22.600000381469727 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.02, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.2
lambda2:  0.05
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.05, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 1.3460, Train: 35.71%, Valid: 12.60% Test: 14.20%
Split: 01, Run: 01, Epoch: 200, Loss: 1.4196, Train: 75.00%, Valid: 13.80% Test: 17.10%
Split: 01, Run: 01, Epoch: 300, Loss: 1.3996, Train: 86.43%, Valid: 15.80% Test: 19.30%
Split: 01, Run: 01, Epoch: 400, Loss: 1.5139, Train: 92.14%, Valid: 15.40% Test: 18.60%
Split: 01, Run: 01, Epoch: 500, Loss: 1.3923, Train: 92.86%, Valid: 15.40% Test: 18.20%
Split: 01, Run: 01
None time:  7.550058064982295
None Run 01:
Highest Train: 95.71
Highest Valid: 20.20
  Final Train: 28.57
   Final Test: 18.70
Split: 01, Run: 02, Epoch: 100, Loss: 1.2758, Train: 85.00%, Valid: 27.60% Test: 28.30%
Split: 01, Run: 02, Epoch: 200, Loss: 1.3579, Train: 91.43%, Valid: 28.20% Test: 28.60%
Split: 01, Run: 02, Epoch: 300, Loss: 1.3144, Train: 99.29%, Valid: 28.60% Test: 27.80%
Split: 01, Run: 02, Epoch: 400, Loss: 1.3296, Train: 99.29%, Valid: 19.60% Test: 22.90%
Split: 01, Run: 02, Epoch: 500, Loss: 1.4433, Train: 98.57%, Valid: 20.80% Test: 22.70%
Split: 01, Run: 02
None time:  7.558407051488757
None Run 02:
Highest Train: 99.29
Highest Valid: 32.60
  Final Train: 89.29
   Final Test: 30.60
Split: 01, Run: 03, Epoch: 100, Loss: 1.4144, Train: 65.71%, Valid: 21.20% Test: 20.40%
Split: 01, Run: 03, Epoch: 200, Loss: 1.3950, Train: 94.29%, Valid: 27.40% Test: 28.00%
Split: 01, Run: 03, Epoch: 300, Loss: 1.5164, Train: 98.57%, Valid: 27.00% Test: 29.70%
Split: 01, Run: 03, Epoch: 400, Loss: 1.2998, Train: 99.29%, Valid: 18.80% Test: 22.20%
Split: 01, Run: 03, Epoch: 500, Loss: 1.3162, Train: 99.29%, Valid: 18.80% Test: 20.80%
Split: 01, Run: 03
None time:  7.580653253942728
None Run 03:
Highest Train: 100.00
Highest Valid: 28.80
  Final Train: 93.57
   Final Test: 28.80
total time:  22.740962771698833
None All runs:
Highest Train: 98.33 Â± 2.30
Highest Valid: 27.20 Â± 6.35
  Final Train: 70.48 Â± 36.35
   Final Test: 26.03 Â± 6.41
[32m[I 2021-07-26 09:12:51,563][0m Trial 5 finished with value: 27.200002670288086 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.05, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.2
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 4.1371, Train: 82.86%, Valid: 15.80% Test: 18.30%
Split: 01, Run: 01, Epoch: 200, Loss: 4.2099, Train: 98.57%, Valid: 18.00% Test: 21.90%
Split: 01, Run: 01, Epoch: 300, Loss: 4.2838, Train: 100.00%, Valid: 21.40% Test: 22.80%
Using backend: pytorch
Split: 01, Run: 01, Epoch: 400, Loss: 4.2413, Train: 100.00%, Valid: 21.00% Test: 24.00%
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1, 0.2], 'lambda2': [0.4], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  5
[32m[I 2021-07-26 09:12:58,726][0m A new study created in memory with name: no-name-3ad25d26-854a-4909-b674-aedbf0a035eb[0m
lambda1:  0.02
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 01, Epoch: 500, Loss: 3.7140, Train: 100.00%, Valid: 21.20% Test: 23.50%
Split: 01, Run: 01
None time:  7.958064584061503
None Run 01:
Highest Train: 100.00
Highest Valid: 22.60
  Final Train: 99.29
   Final Test: 24.50
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 100, Loss: 3.9366, Train: 96.43%, Valid: 29.00% Test: 31.80%
Split: 01, Run: 01, Epoch: 100, Loss: 186.5640, Train: 97.86%, Valid: 66.20% Test: 66.90%
Split: 01, Run: 02, Epoch: 200, Loss: 3.9398, Train: 99.29%, Valid: 25.20% Test: 27.40%
Split: 01, Run: 01, Epoch: 200, Loss: 167.9039, Train: 100.00%, Valid: 68.80% Test: 68.20%
Split: 01, Run: 02, Epoch: 300, Loss: 3.9299, Train: 100.00%, Valid: 23.20% Test: 25.50%
Split: 01, Run: 01, Epoch: 300, Loss: 169.4800, Train: 100.00%, Valid: 66.60% Test: 68.20%
Split: 01, Run: 02, Epoch: 400, Loss: 4.1063, Train: 100.00%, Valid: 26.20% Test: 27.80%
Split: 01, Run: 01, Epoch: 400, Loss: 155.9754, Train: 100.00%, Valid: 68.60% Test: 70.90%
Split: 01, Run: 02, Epoch: 500, Loss: 4.4469, Train: 100.00%, Valid: 26.00% Test: 27.90%
Split: 01, Run: 02
None time:  11.521326819434762
None Run 02:
Highest Train: 100.00
Highest Valid: 33.20
  Final Train: 84.29
   Final Test: 33.20
Split: 01, Run: 01, Epoch: 500, Loss: 153.3987, Train: 100.00%, Valid: 65.60% Test: 65.90%
Split: 01, Run: 01
None time:  12.373965041711926
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.70
Split: 01, Run: 03, Epoch: 100, Loss: 4.3493, Train: 98.57%, Valid: 37.60% Test: 38.40%
Split: 01, Run: 02, Epoch: 100, Loss: 175.5091, Train: 99.29%, Valid: 66.00% Test: 66.10%
Split: 01, Run: 03, Epoch: 200, Loss: 4.0575, Train: 100.00%, Valid: 24.60% Test: 28.50%
Split: 01, Run: 02, Epoch: 200, Loss: 156.5637, Train: 100.00%, Valid: 64.80% Test: 64.70%
Split: 01, Run: 03, Epoch: 300, Loss: 4.2652, Train: 100.00%, Valid: 21.60% Test: 25.40%
Split: 01, Run: 02, Epoch: 300, Loss: 152.9579, Train: 100.00%, Valid: 67.60% Test: 66.00%
Split: 01, Run: 03, Epoch: 400, Loss: 3.8164, Train: 100.00%, Valid: 21.40% Test: 23.90%
Split: 01, Run: 02, Epoch: 400, Loss: 149.7007, Train: 100.00%, Valid: 67.00% Test: 65.90%
Split: 01, Run: 03, Epoch: 500, Loss: 4.1208, Train: 100.00%, Valid: 23.00% Test: 25.20%
Split: 01, Run: 03
None time:  11.978474693372846
None Run 03:
Highest Train: 100.00
Highest Valid: 38.60
  Final Train: 97.86
   Final Test: 39.30
total time:  31.51113861426711
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 31.47 Â± 8.14
  Final Train: 93.81 Â± 8.28
   Final Test: 32.33 Â± 7.44
[32m[I 2021-07-26 09:13:23,080][0m Trial 6 finished with value: 31.466665267944336 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.1, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.2
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 500, Loss: 149.9639, Train: 100.00%, Valid: 67.00% Test: 65.40%
Split: 01, Run: 02
None time:  11.989189684391022
None Run 02:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 99.29
   Final Test: 68.00
Split: 01, Run: 01, Epoch: 100, Loss: 11.9795, Train: 98.57%, Valid: 22.60% Test: 25.20%
Split: 01, Run: 03, Epoch: 100, Loss: 177.6311, Train: 99.29%, Valid: 74.00% Test: 75.60%
Split: 01, Run: 01, Epoch: 200, Loss: 11.3007, Train: 100.00%, Valid: 23.60% Test: 25.90%
Split: 01, Run: 03, Epoch: 200, Loss: 163.5249, Train: 100.00%, Valid: 66.60% Test: 69.80%
Split: 01, Run: 01, Epoch: 300, Loss: 11.4263, Train: 100.00%, Valid: 25.40% Test: 28.00%
Split: 01, Run: 03, Epoch: 300, Loss: 156.1477, Train: 100.00%, Valid: 63.80% Test: 65.00%
Split: 01, Run: 01, Epoch: 400, Loss: 11.2730, Train: 100.00%, Valid: 25.60% Test: 28.50%
Split: 01, Run: 03, Epoch: 400, Loss: 151.1016, Train: 100.00%, Valid: 65.40% Test: 65.60%
Split: 01, Run: 01, Epoch: 500, Loss: 11.0529, Train: 100.00%, Valid: 24.40% Test: 26.20%
Split: 01, Run: 01
None time:  11.986189734190702
None Run 01:
Highest Train: 100.00
Highest Valid: 27.40
  Final Train: 100.00
   Final Test: 28.90
Split: 01, Run: 03, Epoch: 500, Loss: 145.0680, Train: 100.00%, Valid: 64.60% Test: 65.50%
Split: 01, Run: 03
None time:  12.00953283160925
None Run 03:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 99.29
   Final Test: 76.60
total time:  38.033715799450874
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.13 Â± 3.56
  Final Train: 99.52 Â± 0.41
   Final Test: 72.10 Â± 4.31
[32m[I 2021-07-26 09:13:36,769][0m Trial 0 finished with value: 72.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.4, 'K': 10}. Best is trial 0 with value: 72.13333129882812.[0m
lambda1:  0.05
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 100, Loss: 11.8716, Train: 100.00%, Valid: 28.20% Test: 30.80%
Split: 01, Run: 01, Epoch: 100, Loss: 116.7342, Train: 99.29%, Valid: 58.00% Test: 58.20%
Split: 01, Run: 02, Epoch: 200, Loss: 11.1086, Train: 100.00%, Valid: 27.60% Test: 30.10%
Split: 01, Run: 01, Epoch: 200, Loss: 104.2203, Train: 100.00%, Valid: 58.40% Test: 55.90%
Split: 01, Run: 02, Epoch: 300, Loss: 11.7327, Train: 100.00%, Valid: 30.20% Test: 31.20%
Split: 01, Run: 01, Epoch: 300, Loss: 106.2922, Train: 100.00%, Valid: 57.20% Test: 55.30%
Split: 01, Run: 02, Epoch: 400, Loss: 12.4346, Train: 100.00%, Valid: 31.80% Test: 35.80%
Split: 01, Run: 01, Epoch: 400, Loss: 99.2351, Train: 100.00%, Valid: 61.40% Test: 60.60%
Split: 01, Run: 02, Epoch: 500, Loss: 12.1777, Train: 100.00%, Valid: 32.40% Test: 36.90%
Split: 01, Run: 02
None time:  11.895693337544799
None Run 02:
Highest Train: 100.00
Highest Valid: 44.60
  Final Train: 90.00
   Final Test: 43.50
Split: 01, Run: 01, Epoch: 500, Loss: 95.3647, Train: 100.00%, Valid: 58.20% Test: 57.90%
Split: 01, Run: 01
None time:  12.000731475651264
None Run 01:
Highest Train: 100.00
Highest Valid: 63.80
  Final Train: 99.29
   Final Test: 61.60
Split: 01, Run: 03, Epoch: 100, Loss: 12.3128, Train: 100.00%, Valid: 34.60% Test: 37.20%
Split: 01, Run: 02, Epoch: 100, Loss: 109.7790, Train: 100.00%, Valid: 58.00% Test: 58.90%
Split: 01, Run: 03, Epoch: 200, Loss: 12.1992, Train: 100.00%, Valid: 28.20% Test: 31.70%
Split: 01, Run: 02, Epoch: 200, Loss: 99.8600, Train: 100.00%, Valid: 55.40% Test: 55.70%
Split: 01, Run: 03, Epoch: 300, Loss: 12.1758, Train: 100.00%, Valid: 27.20% Test: 30.70%
Split: 01, Run: 02, Epoch: 300, Loss: 98.5794, Train: 100.00%, Valid: 56.60% Test: 56.40%
Split: 01, Run: 03, Epoch: 400, Loss: 11.2937, Train: 100.00%, Valid: 25.80% Test: 29.50%
Split: 01, Run: 02, Epoch: 400, Loss: 96.7272, Train: 100.00%, Valid: 60.00% Test: 58.20%
Split: 01, Run: 03, Epoch: 500, Loss: 11.6072, Train: 100.00%, Valid: 27.60% Test: 30.00%
Split: 01, Run: 03
None time:  12.061540503054857
None Run 03:
Highest Train: 100.00
Highest Valid: 54.80
  Final Train: 96.43
   Final Test: 54.30
total time:  36.007478995248675
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 42.27 Â± 13.85
  Final Train: 95.48 Â± 5.07
   Final Test: 42.23 Â± 12.75
[32m[I 2021-07-26 09:13:59,093][0m Trial 7 finished with value: 42.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.2, 'K': 10}. Best is trial 2 with value: 53.60000228881836.[0m
lambda1:  0.01
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 500, Loss: 99.8406, Train: 100.00%, Valid: 59.00% Test: 59.20%
Split: 01, Run: 02
None time:  11.97245355322957
None Run 02:
Highest Train: 100.00
Highest Valid: 66.00
  Final Train: 100.00
   Final Test: 63.50
Split: 01, Run: 01, Epoch: 100, Loss: 109.6316, Train: 97.14%, Valid: 64.40% Test: 63.70%
Split: 01, Run: 03, Epoch: 100, Loss: 112.9585, Train: 99.29%, Valid: 60.20% Test: 60.70%
Split: 01, Run: 01, Epoch: 200, Loss: 100.7328, Train: 97.86%, Valid: 64.00% Test: 64.20%
Split: 01, Run: 03, Epoch: 200, Loss: 104.4303, Train: 100.00%, Valid: 57.00% Test: 57.30%
Split: 01, Run: 01, Epoch: 300, Loss: 99.7572, Train: 99.29%, Valid: 63.80% Test: 63.10%
Split: 01, Run: 03, Epoch: 300, Loss: 100.9845, Train: 100.00%, Valid: 57.40% Test: 56.50%
Split: 01, Run: 01, Epoch: 400, Loss: 91.7349, Train: 99.29%, Valid: 64.80% Test: 66.00%
Split: 01, Run: 03, Epoch: 400, Loss: 97.7311, Train: 100.00%, Valid: 58.80% Test: 57.60%
Split: 01, Run: 01, Epoch: 500, Loss: 89.4412, Train: 99.29%, Valid: 62.00% Test: 61.60%
Split: 01, Run: 01
None time:  11.885588703677058
None Run 01:
Highest Train: 100.00
Highest Valid: 67.40
  Final Train: 97.14
   Final Test: 66.20
Split: 01, Run: 03, Epoch: 500, Loss: 95.5007, Train: 100.00%, Valid: 57.80% Test: 57.80%
Split: 01, Run: 03
None time:  11.959109466522932
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 98.57
   Final Test: 75.40
total time:  36.00132709927857
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 68.60 Â± 6.50
  Final Train: 99.29 Â± 0.71
   Final Test: 66.83 Â± 7.48
[32m[I 2021-07-26 09:14:12,777][0m Trial 1 finished with value: 68.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.4, 'K': 10}. Best is trial 0 with value: 72.13333129882812.[0m
lambda1:  0.2
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.2, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 100, Loss: 103.2766, Train: 98.57%, Valid: 62.60% Test: 63.90%
Split: 01, Run: 01, Epoch: 100, Loss: 33.9394, Train: 100.00%, Valid: 30.00% Test: 32.40%
Split: 01, Run: 01, Epoch: 200, Loss: 31.5839, Train: 100.00%, Valid: 29.40% Test: 33.50%
Split: 01, Run: 01, Epoch: 300, Loss: 31.0591, Train: 100.00%, Valid: 29.60% Test: 32.70%
Split: 01, Run: 01, Epoch: 400, Loss: 28.2312, Train: 100.00%, Valid: 33.00% Test: 37.10%
Using backend: pytorch
Split: 01, Run: 01, Epoch: 500, Loss: 28.4694, Train: 100.00%, Valid: 30.40% Test: 34.20%
Split: 01, Run: 01
None time:  7.791024373844266
None Run 01:
Highest Train: 100.00
Highest Valid: 34.00
  Final Train: 100.00
   Final Test: 35.90
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1], 'lambda2': [0.1, 0.2, 0.3, 0.4], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  16
[32m[I 2021-07-26 09:14:21,111][0m A new study created in memory with name: no-name-b274919a-e18e-4dc9-a139-6ff15511b6ab[0m
lambda1:  0.01
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 100, Loss: 33.2939, Train: 100.00%, Valid: 33.20% Test: 35.70%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 32.4239, Train: 100.00%, Valid: 34.20% Test: 35.00%
Split: 01, Run: 01, Epoch: 100, Loss: 58.1542, Train: 93.57%, Valid: 53.40% Test: 54.70%
Split: 01, Run: 02, Epoch: 300, Loss: 30.1376, Train: 100.00%, Valid: 31.00% Test: 32.90%
Split: 01, Run: 01, Epoch: 200, Loss: 50.6786, Train: 94.29%, Valid: 56.40% Test: 54.10%
Split: 01, Run: 02, Epoch: 400, Loss: 32.3668, Train: 100.00%, Valid: 33.00% Test: 35.10%
Split: 01, Run: 01, Epoch: 300, Loss: 51.5626, Train: 96.43%, Valid: 53.00% Test: 51.90%
Split: 01, Run: 02, Epoch: 500, Loss: 32.0773, Train: 100.00%, Valid: 32.40% Test: 34.90%
Split: 01, Run: 02
None time:  10.978130893781781
None Run 02:
Highest Train: 100.00
Highest Valid: 50.80
  Final Train: 93.57
   Final Test: 48.20
Split: 01, Run: 01, Epoch: 400, Loss: 49.6535, Train: 98.57%, Valid: 55.80% Test: 54.40%
Split: 01, Run: 03, Epoch: 100, Loss: 34.4479, Train: 100.00%, Valid: 33.00% Test: 35.50%
Split: 01, Run: 01, Epoch: 500, Loss: 47.5685, Train: 97.86%, Valid: 51.60% Test: 51.90%
Split: 01, Run: 01
None time:  12.44468360580504
None Run 01:
Highest Train: 98.57
Highest Valid: 58.00
  Final Train: 90.00
   Final Test: 57.20
Split: 01, Run: 03, Epoch: 200, Loss: 33.9613, Train: 100.00%, Valid: 34.40% Test: 37.40%
Split: 01, Run: 02, Epoch: 100, Loss: 52.9204, Train: 94.29%, Valid: 54.40% Test: 55.40%
Split: 01, Run: 03, Epoch: 300, Loss: 31.9184, Train: 100.00%, Valid: 37.00% Test: 39.60%
Split: 01, Run: 02, Epoch: 200, Loss: 47.5803, Train: 97.14%, Valid: 53.20% Test: 53.00%
Split: 01, Run: 03, Epoch: 400, Loss: 29.8658, Train: 100.00%, Valid: 33.20% Test: 37.90%
Split: 01, Run: 02, Epoch: 300, Loss: 49.1543, Train: 97.14%, Valid: 52.80% Test: 53.50%
Split: 01, Run: 03, Epoch: 500, Loss: 30.9144, Train: 100.00%, Valid: 33.60% Test: 36.30%
Split: 01, Run: 03
None time:  11.74998894520104
None Run 03:
Highest Train: 100.00
Highest Valid: 71.60
  Final Train: 98.57
   Final Test: 69.90
total time:  30.58718279749155
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 52.13 Â± 18.84
  Final Train: 97.38 Â± 3.38
   Final Test: 51.33 Â± 17.22
[32m[I 2021-07-26 09:14:43,371][0m Trial 2 finished with value: 52.133331298828125 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.4, 'K': 10}. Best is trial 0 with value: 72.13333129882812.[0m
lambda1:  0.01
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 50.5855, Train: 98.57%, Valid: 52.40% Test: 54.10%
Split: 01, Run: 01, Epoch: 100, Loss: 230.5316, Train: 97.86%, Valid: 68.80% Test: 69.80%
Split: 01, Run: 02, Epoch: 500, Loss: 49.0450, Train: 98.57%, Valid: 53.40% Test: 54.00%
Split: 01, Run: 02
None time:  12.095542196184397
None Run 02:
Highest Train: 99.29
Highest Valid: 62.80
  Final Train: 88.57
   Final Test: 63.10
Split: 01, Run: 01, Epoch: 200, Loss: 207.3807, Train: 99.29%, Valid: 72.00% Test: 72.00%
Split: 01, Run: 03, Epoch: 100, Loss: 55.8495, Train: 91.43%, Valid: 58.20% Test: 59.10%
Split: 01, Run: 01, Epoch: 300, Loss: 206.5573, Train: 99.29%, Valid: 71.20% Test: 70.80%
Split: 01, Run: 03, Epoch: 200, Loss: 50.2964, Train: 95.71%, Valid: 54.60% Test: 55.70%
Split: 01, Run: 01, Epoch: 400, Loss: 191.6575, Train: 100.00%, Valid: 70.40% Test: 71.30%
Split: 01, Run: 03, Epoch: 300, Loss: 50.9655, Train: 95.71%, Valid: 54.00% Test: 55.90%
Split: 01, Run: 01, Epoch: 500, Loss: 187.1939, Train: 100.00%, Valid: 68.80% Test: 69.30%
Split: 01, Run: 01
None time:  11.94859460555017
None Run 01:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 99.29
   Final Test: 71.40
Split: 01, Run: 03, Epoch: 400, Loss: 47.6746, Train: 97.14%, Valid: 55.20% Test: 55.50%
Split: 01, Run: 02, Epoch: 100, Loss: 214.4670, Train: 99.29%, Valid: 67.80% Test: 70.20%
Split: 01, Run: 03, Epoch: 500, Loss: 45.4850, Train: 97.86%, Valid: 56.40% Test: 55.50%
Split: 01, Run: 03
None time:  12.247114332392812
None Run 03:
Highest Train: 98.57
Highest Valid: 69.40
  Final Train: 95.00
   Final Test: 73.10
total time:  38.46306402422488
None All runs:
Highest Train: 98.81 Â± 0.41
Highest Valid: 63.40 Â± 5.72
  Final Train: 91.19 Â± 3.38
   Final Test: 64.47 Â± 8.04
[32m[I 2021-07-26 09:14:59,656][0m Trial 0 finished with value: 63.40000534057617 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.05
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 191.5031, Train: 100.00%, Valid: 68.40% Test: 68.20%
Split: 01, Run: 01, Epoch: 100, Loss: 48.5478, Train: 97.86%, Valid: 52.20% Test: 49.70%
Split: 01, Run: 02, Epoch: 300, Loss: 185.3943, Train: 100.00%, Valid: 72.60% Test: 72.10%
Split: 01, Run: 01, Epoch: 200, Loss: 44.4569, Train: 99.29%, Valid: 49.60% Test: 49.40%
Split: 01, Run: 02, Epoch: 400, Loss: 183.2801, Train: 100.00%, Valid: 69.80% Test: 70.60%
Split: 01, Run: 01, Epoch: 300, Loss: 44.7721, Train: 100.00%, Valid: 46.00% Test: 47.10%
Split: 01, Run: 02, Epoch: 500, Loss: 179.4283, Train: 100.00%, Valid: 72.20% Test: 74.60%
Split: 01, Run: 02
None time:  11.919279893860221
None Run 02:
Highest Train: 100.00
Highest Valid: 75.00
  Final Train: 100.00
   Final Test: 76.40
Split: 01, Run: 01, Epoch: 400, Loss: 44.1177, Train: 100.00%, Valid: 49.80% Test: 50.80%
Split: 01, Run: 03, Epoch: 100, Loss: 218.0485, Train: 98.57%, Valid: 73.20% Test: 75.50%
Split: 01, Run: 01, Epoch: 500, Loss: 43.0280, Train: 100.00%, Valid: 51.40% Test: 50.30%
Split: 01, Run: 01
None time:  11.904561081901193
None Run 01:
Highest Train: 100.00
Highest Valid: 55.00
  Final Train: 97.14
   Final Test: 54.70
Split: 01, Run: 03, Epoch: 200, Loss: 197.8513, Train: 100.00%, Valid: 70.00% Test: 70.40%
Split: 01, Run: 02, Epoch: 100, Loss: 46.0467, Train: 99.29%, Valid: 51.40% Test: 52.40%
Split: 01, Run: 03, Epoch: 300, Loss: 185.6965, Train: 100.00%, Valid: 66.80% Test: 68.30%
Split: 01, Run: 02, Epoch: 200, Loss: 42.6518, Train: 100.00%, Valid: 49.80% Test: 50.70%
Split: 01, Run: 03, Epoch: 400, Loss: 182.2001, Train: 100.00%, Valid: 69.00% Test: 69.70%
Split: 01, Run: 02, Epoch: 300, Loss: 44.0199, Train: 100.00%, Valid: 52.40% Test: 51.80%
Split: 01, Run: 03, Epoch: 500, Loss: 177.0083, Train: 100.00%, Valid: 69.20% Test: 69.90%
Split: 01, Run: 03
None time:  11.983939746394753
None Run 03:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 99.29
   Final Test: 77.50
total time:  35.918125089257956
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.93 Â± 2.30
  Final Train: 99.52 Â± 0.41
   Final Test: 75.10 Â± 3.25
[32m[I 2021-07-26 09:15:19,295][0m Trial 3 finished with value: 74.93334197998047 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}. Best is trial 3 with value: 74.93334197998047.[0m
lambda1:  0.1
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 44.0556, Train: 100.00%, Valid: 54.40% Test: 52.90%
Split: 01, Run: 01, Epoch: 100, Loss: 68.5235, Train: 100.00%, Valid: 42.80% Test: 43.70%
Split: 01, Run: 02, Epoch: 500, Loss: 44.4329, Train: 100.00%, Valid: 53.40% Test: 52.70%
Split: 01, Run: 02
None time:  11.926712431013584
None Run 02:
Highest Train: 100.00
Highest Valid: 59.80
  Final Train: 100.00
   Final Test: 57.00
Split: 01, Run: 01, Epoch: 200, Loss: 61.7059, Train: 100.00%, Valid: 42.20% Test: 43.80%
Split: 01, Run: 03, Epoch: 100, Loss: 48.3324, Train: 99.29%, Valid: 52.40% Test: 55.50%
Split: 01, Run: 01, Epoch: 300, Loss: 61.7763, Train: 100.00%, Valid: 45.40% Test: 46.20%
Split: 01, Run: 03, Epoch: 200, Loss: 43.9905, Train: 100.00%, Valid: 50.80% Test: 51.90%
Split: 01, Run: 01, Epoch: 400, Loss: 58.8862, Train: 100.00%, Valid: 46.40% Test: 50.10%
Split: 01, Run: 03, Epoch: 300, Loss: 44.0546, Train: 100.00%, Valid: 51.00% Test: 51.50%
Split: 01, Run: 01, Epoch: 500, Loss: 58.0917, Train: 100.00%, Valid: 45.20% Test: 46.70%
Split: 01, Run: 01
None time:  12.01497315429151
None Run 01:
Highest Train: 100.00
Highest Valid: 48.80
  Final Train: 100.00
   Final Test: 50.10
Split: 01, Run: 03, Epoch: 400, Loss: 42.0991, Train: 100.00%, Valid: 52.00% Test: 51.70%
Split: 01, Run: 02, Epoch: 100, Loss: 65.0971, Train: 100.00%, Valid: 45.20% Test: 48.10%
Split: 01, Run: 03, Epoch: 500, Loss: 40.5678, Train: 100.00%, Valid: 51.00% Test: 50.50%
Split: 01, Run: 03
None time:  11.93098696321249
None Run 03:
Highest Train: 100.00
Highest Valid: 72.00
  Final Train: 97.86
   Final Test: 74.30
total time:  35.83019880391657
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.27 Â± 8.76
  Final Train: 98.33 Â± 1.49
   Final Test: 62.00 Â± 10.71
[32m[I 2021-07-26 09:15:35,493][0m Trial 1 finished with value: 62.266666412353516 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.2, 'K': 10}. Best is trial 0 with value: 63.40000534057617.[0m
lambda1:  0.05
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 60.5299, Train: 100.00%, Valid: 41.60% Test: 45.80%
Split: 01, Run: 01, Epoch: 100, Loss: 79.0553, Train: 98.57%, Valid: 56.20% Test: 55.40%
Split: 01, Run: 02, Epoch: 300, Loss: 58.1848, Train: 100.00%, Valid: 41.80% Test: 47.00%
Split: 01, Run: 01, Epoch: 200, Loss: 73.4674, Train: 100.00%, Valid: 56.40% Test: 54.20%
Split: 01, Run: 02, Epoch: 400, Loss: 58.2478, Train: 100.00%, Valid: 50.20% Test: 51.00%
Split: 01, Run: 01, Epoch: 300, Loss: 74.9861, Train: 100.00%, Valid: 54.40% Test: 54.70%
Split: 01, Run: 02, Epoch: 500, Loss: 61.5351, Train: 100.00%, Valid: 51.40% Test: 51.30%
Split: 01, Run: 02
None time:  11.964326934888959
None Run 02:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 55.50
Split: 01, Run: 01, Epoch: 400, Loss: 71.4650, Train: 100.00%, Valid: 58.00% Test: 58.50%
Split: 01, Run: 03, Epoch: 100, Loss: 66.4162, Train: 100.00%, Valid: 47.80% Test: 50.70%
Split: 01, Run: 01, Epoch: 500, Loss: 69.5995, Train: 100.00%, Valid: 55.00% Test: 55.00%
Split: 01, Run: 01
None time:  11.961749594658613
None Run 01:
Highest Train: 100.00
Highest Valid: 59.00
  Final Train: 100.00
   Final Test: 56.70
Split: 01, Run: 03, Epoch: 200, Loss: 62.8311, Train: 100.00%, Valid: 44.80% Test: 49.10%
Split: 01, Run: 02, Epoch: 100, Loss: 76.0807, Train: 100.00%, Valid: 54.00% Test: 55.80%
Split: 01, Run: 03, Epoch: 300, Loss: 61.2526, Train: 100.00%, Valid: 43.20% Test: 46.70%
Split: 01, Run: 02, Epoch: 200, Loss: 69.5618, Train: 100.00%, Valid: 52.40% Test: 54.20%
Split: 01, Run: 03, Epoch: 400, Loss: 58.4624, Train: 100.00%, Valid: 45.00% Test: 46.90%
Split: 01, Run: 02, Epoch: 300, Loss: 70.3654, Train: 100.00%, Valid: 53.20% Test: 54.10%
Split: 01, Run: 03, Epoch: 500, Loss: 57.1897, Train: 100.00%, Valid: 46.00% Test: 47.20%
Split: 01, Run: 03
None time:  11.966569980606437
None Run 03:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 98.57
   Final Test: 74.00
total time:  36.01333939284086
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 61.13 Â± 14.16
  Final Train: 99.52 Â± 0.82
   Final Test: 59.87 Â± 12.53
[32m[I 2021-07-26 09:15:55,315][0m Trial 4 finished with value: 61.133331298828125 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}. Best is trial 3 with value: 74.93334197998047.[0m
Study statistics: 
  Number of finished trials:  5
  Number of pruned trials:  0
  Number of complete trials:  5
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}   trial.value:  74.933    {'train': '99.52 Â± 0.41', 'valid': '74.93 Â± 2.30', 'test': '75.10 Â± 3.25'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.4, 'K': 10}   trial.value:  72.133    {'train': '99.52 Â± 0.41', 'valid': '72.13 Â± 3.56', 'test': '72.10 Â± 4.31'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.4, 'K': 10}   trial.value:  68.6    {'train': '99.29 Â± 0.71', 'valid': '68.60 Â± 6.50', 'test': '66.83 Â± 7.48'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}   trial.value:  61.133    {'train': '99.52 Â± 0.82', 'valid': '61.13 Â± 14.16', 'test': '59.87 Â± 12.53'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.2, 'lambda2': 0.4, 'K': 10}   trial.value:  52.133    {'train': '97.38 Â± 3.38', 'valid': '52.13 Â± 18.84', 'test': '51.33 Â± 17.22'}
test_acc
['75.10 Â± 3.25', '72.10 Â± 4.31', '66.83 Â± 7.48', '59.87 Â± 12.53', '51.33 Â± 17.22']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}
Best trial Value:  74.93334197998047
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '74.93 Â± 2.30', 'test': '75.10 Â± 3.25'}
optuna total time:  176.5948328897357
Split: 01, Run: 02, Epoch: 400, Loss: 68.6444, Train: 100.00%, Valid: 55.60% Test: 54.60%
Split: 01, Run: 02, Epoch: 500, Loss: 70.5757, Train: 100.00%, Valid: 53.80% Test: 55.30%
Split: 01, Run: 02
None time:  10.240380579605699
None Run 02:
Highest Train: 100.00
Highest Valid: 63.60
  Final Train: 100.00
   Final Test: 61.10
Split: 01, Run: 03, Epoch: 100, Loss: 79.1354, Train: 98.57%, Valid: 55.40% Test: 57.70%
Split: 01, Run: 03, Epoch: 200, Loss: 73.2346, Train: 100.00%, Valid: 57.40% Test: 56.60%
Split: 01, Run: 03, Epoch: 300, Loss: 72.2716, Train: 100.00%, Valid: 56.20% Test: 53.50%
Split: 01, Run: 03, Epoch: 400, Loss: 70.6230, Train: 100.00%, Valid: 60.40% Test: 57.50%
Split: 01, Run: 03, Epoch: 500, Loss: 66.7445, Train: 100.00%, Valid: 57.00% Test: 55.50%
Split: 01, Run: 03
None time:  7.435777163133025
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 98.57
   Final Test: 75.60
total time:  29.70055739581585
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 66.20 Â± 8.79
  Final Train: 99.52 Â± 0.82
   Final Test: 64.47 Â± 9.89
[32m[I 2021-07-26 09:16:05,200][0m Trial 2 finished with value: 66.19999694824219 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.3, 'K': 10}. Best is trial 2 with value: 66.19999694824219.[0m
lambda1:  0.05
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 116.7342, Train: 99.29%, Valid: 58.00% Test: 58.20%
Split: 01, Run: 01, Epoch: 200, Loss: 104.2203, Train: 100.00%, Valid: 58.40% Test: 55.90%
Split: 01, Run: 01, Epoch: 300, Loss: 106.2922, Train: 100.00%, Valid: 57.20% Test: 55.30%
Split: 01, Run: 01, Epoch: 400, Loss: 99.2351, Train: 100.00%, Valid: 61.40% Test: 60.60%
Split: 01, Run: 01, Epoch: 500, Loss: 95.3647, Train: 100.00%, Valid: 58.20% Test: 57.90%
Split: 01, Run: 01
None time:  7.608282133936882
None Run 01:
Highest Train: 100.00
Highest Valid: 63.80
  Final Train: 99.29
   Final Test: 61.60
Split: 01, Run: 02, Epoch: 100, Loss: 109.7790, Train: 100.00%, Valid: 58.00% Test: 58.90%
Split: 01, Run: 02, Epoch: 200, Loss: 99.8600, Train: 100.00%, Valid: 55.40% Test: 55.70%
Split: 01, Run: 02, Epoch: 300, Loss: 98.5794, Train: 100.00%, Valid: 56.60% Test: 56.40%
Split: 01, Run: 02, Epoch: 400, Loss: 96.7272, Train: 100.00%, Valid: 60.00% Test: 58.20%
Split: 01, Run: 02, Epoch: 500, Loss: 99.8406, Train: 100.00%, Valid: 59.00% Test: 59.20%
Split: 01, Run: 02
None time:  7.528897140175104
None Run 02:
Highest Train: 100.00
Highest Valid: 66.00
  Final Train: 100.00
   Final Test: 63.50
Split: 01, Run: 03, Epoch: 100, Loss: 112.9585, Train: 99.29%, Valid: 60.20% Test: 60.70%
Split: 01, Run: 03, Epoch: 200, Loss: 104.4303, Train: 100.00%, Valid: 57.00% Test: 57.30%
Split: 01, Run: 03, Epoch: 300, Loss: 100.9845, Train: 100.00%, Valid: 57.40% Test: 56.50%
Split: 01, Run: 03, Epoch: 400, Loss: 97.7311, Train: 100.00%, Valid: 58.80% Test: 57.60%
Split: 01, Run: 03, Epoch: 500, Loss: 95.5007, Train: 100.00%, Valid: 57.80% Test: 57.80%
Split: 01, Run: 03
None time:  7.5184807777404785
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 98.57
   Final Test: 75.40
total time:  22.711346924304962
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 68.60 Â± 6.50
  Final Train: 99.29 Â± 0.71
   Final Test: 66.83 Â± 7.48
[32m[I 2021-07-26 09:16:27,916][0m Trial 3 finished with value: 68.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.4, 'K': 10}. Best is trial 3 with value: 68.5999984741211.[0m
lambda1:  0.1
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 9.9177, Train: 95.00%, Valid: 24.00% Test: 27.20%
Split: 01, Run: 01, Epoch: 200, Loss: 9.2825, Train: 97.14%, Valid: 24.40% Test: 27.00%
Split: 01, Run: 01, Epoch: 300, Loss: 9.4504, Train: 99.29%, Valid: 25.40% Test: 27.50%
Split: 01, Run: 01, Epoch: 400, Loss: 9.4481, Train: 99.29%, Valid: 26.40% Test: 28.30%
Split: 01, Run: 01, Epoch: 500, Loss: 9.6054, Train: 100.00%, Valid: 26.60% Test: 28.70%
Split: 01, Run: 01
None time:  7.497035518288612
None Run 01:
Highest Train: 100.00
Highest Valid: 29.60
  Final Train: 100.00
   Final Test: 30.40
Split: 01, Run: 02, Epoch: 100, Loss: 9.8400, Train: 98.57%, Valid: 35.20% Test: 36.50%
Split: 01, Run: 02, Epoch: 200, Loss: 9.5638, Train: 100.00%, Valid: 32.00% Test: 35.30%
Split: 01, Run: 02, Epoch: 300, Loss: 10.0595, Train: 99.29%, Valid: 28.60% Test: 32.30%
Split: 01, Run: 02, Epoch: 400, Loss: 9.8036, Train: 100.00%, Valid: 31.00% Test: 34.30%
Split: 01, Run: 02, Epoch: 500, Loss: 10.0199, Train: 100.00%, Valid: 34.60% Test: 38.30%
Split: 01, Run: 02
None time:  7.764415610581636
None Run 02:
Highest Train: 100.00
Highest Valid: 44.60
  Final Train: 86.43
   Final Test: 43.10
Split: 01, Run: 03, Epoch: 100, Loss: 10.3738, Train: 98.57%, Valid: 40.40% Test: 43.30%
Split: 01, Run: 03, Epoch: 200, Loss: 9.5147, Train: 98.57%, Valid: 30.20% Test: 34.10%
Split: 01, Run: 03, Epoch: 300, Loss: 10.7391, Train: 100.00%, Valid: 29.00% Test: 33.40%
Split: 01, Run: 03, Epoch: 400, Loss: 10.2775, Train: 100.00%, Valid: 31.40% Test: 31.80%
Split: 01, Run: 03, Epoch: 500, Loss: 10.2011, Train: 100.00%, Valid: 29.80% Test: 32.20%
Split: 01, Run: 03
None time:  7.843648275360465
None Run 03:
Highest Train: 100.00
Highest Valid: 53.60
  Final Train: 89.29
   Final Test: 53.80
total time:  23.15758332796395
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 42.60 Â± 12.12
  Final Train: 91.90 Â± 7.15
   Final Test: 42.43 Â± 11.71
[32m[I 2021-07-26 09:16:51,079][0m Trial 4 finished with value: 42.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}. Best is trial 3 with value: 68.5999984741211.[0m
lambda1:  0.1
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 26.3044, Train: 99.29%, Valid: 37.40% Test: 39.00%
Split: 01, Run: 01, Epoch: 200, Loss: 24.0488, Train: 100.00%, Valid: 33.60% Test: 36.10%
Split: 01, Run: 01, Epoch: 300, Loss: 24.9442, Train: 100.00%, Valid: 34.80% Test: 37.90%
Split: 01, Run: 01, Epoch: 400, Loss: 23.9012, Train: 100.00%, Valid: 38.00% Test: 40.70%
Split: 01, Run: 01, Epoch: 500, Loss: 23.7284, Train: 100.00%, Valid: 36.80% Test: 38.60%
Split: 01, Run: 01
None time:  7.890943743288517
None Run 01:
Highest Train: 100.00
Highest Valid: 39.20
  Final Train: 100.00
   Final Test: 41.00
Split: 01, Run: 02, Epoch: 100, Loss: 25.3909, Train: 100.00%, Valid: 41.40% Test: 42.50%
Split: 01, Run: 02, Epoch: 200, Loss: 24.4655, Train: 100.00%, Valid: 37.60% Test: 40.70%
Split: 01, Run: 02, Epoch: 300, Loss: 24.0742, Train: 100.00%, Valid: 38.20% Test: 41.10%
Split: 01, Run: 02, Epoch: 400, Loss: 25.0777, Train: 100.00%, Valid: 39.40% Test: 41.20%
Split: 01, Run: 02, Epoch: 500, Loss: 25.3509, Train: 100.00%, Valid: 38.00% Test: 40.20%
Split: 01, Run: 02
None time:  7.582340264692903
None Run 02:
Highest Train: 100.00
Highest Valid: 52.20
  Final Train: 90.71
   Final Test: 49.50
Split: 01, Run: 03, Epoch: 100, Loss: 26.6331, Train: 100.00%, Valid: 45.80% Test: 46.80%
Split: 01, Run: 03, Epoch: 200, Loss: 25.0061, Train: 100.00%, Valid: 41.00% Test: 43.50%
Split: 01, Run: 03, Epoch: 300, Loss: 26.1668, Train: 100.00%, Valid: 37.80% Test: 41.70%
Split: 01, Run: 03, Epoch: 400, Loss: 24.8739, Train: 100.00%, Valid: 36.80% Test: 40.30%
Split: 01, Run: 03, Epoch: 500, Loss: 23.7292, Train: 100.00%, Valid: 37.40% Test: 40.90%
Split: 01, Run: 03
None time:  7.859140252694488
None Run 03:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 98.57
   Final Test: 69.20
total time:  23.386207703500986
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 53.60 Â± 15.15
  Final Train: 96.43 Â± 5.00
   Final Test: 53.23 Â± 14.47
[32m[I 2021-07-26 09:17:14,473][0m Trial 5 finished with value: 53.60000228881836 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}. Best is trial 3 with value: 68.5999984741211.[0m
lambda1:  0.1
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 45.9815, Train: 100.00%, Valid: 43.00% Test: 42.70%
Split: 01, Run: 01, Epoch: 200, Loss: 42.1097, Train: 100.00%, Valid: 39.20% Test: 40.20%
Split: 01, Run: 01, Epoch: 300, Loss: 42.0847, Train: 100.00%, Valid: 39.80% Test: 41.40%
Split: 01, Run: 01, Epoch: 400, Loss: 39.8226, Train: 100.00%, Valid: 43.40% Test: 46.50%
Split: 01, Run: 01, Epoch: 500, Loss: 39.3252, Train: 100.00%, Valid: 40.20% Test: 42.00%
Split: 01, Run: 01
None time:  7.73716258071363
None Run 01:
Highest Train: 100.00
Highest Valid: 46.40
  Final Train: 99.29
   Final Test: 48.80
Split: 01, Run: 02, Epoch: 100, Loss: 43.5717, Train: 100.00%, Valid: 44.20% Test: 46.50%
Split: 01, Run: 02, Epoch: 200, Loss: 41.4836, Train: 100.00%, Valid: 42.40% Test: 44.10%
Split: 01, Run: 02, Epoch: 300, Loss: 40.8240, Train: 100.00%, Valid: 42.40% Test: 45.50%
Split: 01, Run: 02, Epoch: 400, Loss: 41.6265, Train: 100.00%, Valid: 44.40% Test: 45.80%
Split: 01, Run: 02, Epoch: 500, Loss: 42.9186, Train: 100.00%, Valid: 43.40% Test: 46.20%
Split: 01, Run: 02
None time:  7.70712755061686
None Run 02:
Highest Train: 100.00
Highest Valid: 55.60
  Final Train: 100.00
   Final Test: 52.90
Split: 01, Run: 03, Epoch: 100, Loss: 45.5398, Train: 100.00%, Valid: 47.60% Test: 48.70%
Split: 01, Run: 03, Epoch: 200, Loss: 42.5134, Train: 100.00%, Valid: 43.60% Test: 46.50%
Split: 01, Run: 03, Epoch: 300, Loss: 41.4094, Train: 100.00%, Valid: 43.00% Test: 46.40%
Split: 01, Run: 03, Epoch: 400, Loss: 40.4475, Train: 100.00%, Valid: 46.00% Test: 48.10%
Split: 01, Run: 03, Epoch: 500, Loss: 38.7198, Train: 100.00%, Valid: 45.00% Test: 46.30%
Split: 01, Run: 03
None time:  7.58902052603662
None Run 03:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 98.57
   Final Test: 73.60
total time:  23.109559923410416
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 58.93 Â± 14.49
  Final Train: 99.29 Â± 0.71
   Final Test: 58.43 Â± 13.29
[32m[I 2021-07-26 09:17:37,589][0m Trial 6 finished with value: 58.93333435058594 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}. Best is trial 3 with value: 68.5999984741211.[0m
lambda1:  0.1
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 68.5235, Train: 100.00%, Valid: 42.80% Test: 43.70%
Split: 01, Run: 01, Epoch: 200, Loss: 61.7059, Train: 100.00%, Valid: 42.20% Test: 43.80%
Split: 01, Run: 01, Epoch: 300, Loss: 61.7763, Train: 100.00%, Valid: 45.40% Test: 46.20%
Split: 01, Run: 01, Epoch: 400, Loss: 58.8862, Train: 100.00%, Valid: 46.40% Test: 50.10%
Split: 01, Run: 01, Epoch: 500, Loss: 58.0917, Train: 100.00%, Valid: 45.20% Test: 46.70%
Split: 01, Run: 01
None time:  7.5903694070875645
None Run 01:
Highest Train: 100.00
Highest Valid: 48.80
  Final Train: 100.00
   Final Test: 50.10
Split: 01, Run: 02, Epoch: 100, Loss: 65.0971, Train: 100.00%, Valid: 45.20% Test: 48.10%
Split: 01, Run: 02, Epoch: 200, Loss: 60.5299, Train: 100.00%, Valid: 41.60% Test: 45.80%
Split: 01, Run: 02, Epoch: 300, Loss: 58.1848, Train: 100.00%, Valid: 41.80% Test: 47.00%
Split: 01, Run: 02, Epoch: 400, Loss: 58.2478, Train: 100.00%, Valid: 50.20% Test: 51.00%
Split: 01, Run: 02, Epoch: 500, Loss: 61.5351, Train: 100.00%, Valid: 51.40% Test: 51.30%
Split: 01, Run: 02
None time:  7.527502469718456
None Run 02:
Highest Train: 100.00
Highest Valid: 58.00
  Final Train: 100.00
   Final Test: 55.50
Split: 01, Run: 03, Epoch: 100, Loss: 66.4162, Train: 100.00%, Valid: 47.80% Test: 50.70%
Split: 01, Run: 03, Epoch: 200, Loss: 62.8311, Train: 100.00%, Valid: 44.80% Test: 49.10%
Split: 01, Run: 03, Epoch: 300, Loss: 61.2526, Train: 100.00%, Valid: 43.20% Test: 46.70%
Split: 01, Run: 03, Epoch: 400, Loss: 58.4624, Train: 100.00%, Valid: 45.00% Test: 46.90%
Split: 01, Run: 03, Epoch: 500, Loss: 57.1897, Train: 100.00%, Valid: 46.00% Test: 47.20%
Split: 01, Run: 03
None time:  7.5264405850321054
None Run 03:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 98.57
   Final Test: 74.00
total time:  22.69740378111601
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 61.13 Â± 14.16
  Final Train: 99.52 Â± 0.82
   Final Test: 59.87 Â± 12.53
[32m[I 2021-07-26 09:18:00,291][0m Trial 7 finished with value: 61.133331298828125 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}. Best is trial 3 with value: 68.5999984741211.[0m
lambda1:  0.01
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 230.5316, Train: 97.86%, Valid: 68.80% Test: 69.80%
Split: 01, Run: 01, Epoch: 200, Loss: 207.3807, Train: 99.29%, Valid: 72.00% Test: 72.00%
Split: 01, Run: 01, Epoch: 300, Loss: 206.5573, Train: 99.29%, Valid: 71.20% Test: 70.80%
Split: 01, Run: 01, Epoch: 400, Loss: 191.6575, Train: 100.00%, Valid: 70.40% Test: 71.30%
Split: 01, Run: 01, Epoch: 500, Loss: 187.1939, Train: 100.00%, Valid: 68.80% Test: 69.30%
Split: 01, Run: 01
None time:  7.496552323922515
None Run 01:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 99.29
   Final Test: 71.40
Split: 01, Run: 02, Epoch: 100, Loss: 214.4670, Train: 99.29%, Valid: 67.80% Test: 70.20%
Split: 01, Run: 02, Epoch: 200, Loss: 191.5031, Train: 100.00%, Valid: 68.40% Test: 68.20%
Split: 01, Run: 02, Epoch: 300, Loss: 185.3943, Train: 100.00%, Valid: 72.60% Test: 72.10%
Split: 01, Run: 02, Epoch: 400, Loss: 183.2801, Train: 100.00%, Valid: 69.80% Test: 70.60%
Split: 01, Run: 02, Epoch: 500, Loss: 179.4283, Train: 100.00%, Valid: 72.20% Test: 74.60%
Split: 01, Run: 02
None time:  7.5153366178274155
None Run 02:
Highest Train: 100.00
Highest Valid: 75.00
  Final Train: 100.00
   Final Test: 76.40
Split: 01, Run: 03, Epoch: 100, Loss: 218.0485, Train: 98.57%, Valid: 73.20% Test: 75.50%
Split: 01, Run: 03, Epoch: 200, Loss: 197.8513, Train: 100.00%, Valid: 70.00% Test: 70.40%
Split: 01, Run: 03, Epoch: 300, Loss: 185.6965, Train: 100.00%, Valid: 66.80% Test: 68.30%
Split: 01, Run: 03, Epoch: 400, Loss: 182.2001, Train: 100.00%, Valid: 69.00% Test: 69.70%
Split: 01, Run: 03, Epoch: 500, Loss: 177.0083, Train: 100.00%, Valid: 69.20% Test: 69.90%
Split: 01, Run: 03
None time:  7.522775424644351
None Run 03:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 99.29
   Final Test: 77.50
total time:  22.586500965058804
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.93 Â± 2.30
  Final Train: 99.52 Â± 0.41
   Final Test: 75.10 Â± 3.25
[32m[I 2021-07-26 09:18:22,884][0m Trial 8 finished with value: 74.93334197998047 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.02
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 132.8807, Train: 97.86%, Valid: 63.40% Test: 64.10%
Split: 01, Run: 01, Epoch: 200, Loss: 121.7695, Train: 99.29%, Valid: 65.00% Test: 64.80%
Split: 01, Run: 01, Epoch: 300, Loss: 121.6400, Train: 99.29%, Valid: 64.60% Test: 65.00%
Split: 01, Run: 01, Epoch: 400, Loss: 111.3262, Train: 99.29%, Valid: 66.20% Test: 67.10%
Split: 01, Run: 01, Epoch: 500, Loss: 108.4217, Train: 99.29%, Valid: 62.20% Test: 64.30%
Split: 01, Run: 01
None time:  7.5257790349423885
None Run 01:
Highest Train: 100.00
Highest Valid: 68.20
  Final Train: 97.14
   Final Test: 66.30
Split: 01, Run: 02, Epoch: 100, Loss: 125.4348, Train: 99.29%, Valid: 63.80% Test: 64.20%
Split: 01, Run: 02, Epoch: 200, Loss: 112.5869, Train: 100.00%, Valid: 63.20% Test: 64.30%
Split: 01, Run: 02, Epoch: 300, Loss: 110.5237, Train: 100.00%, Valid: 64.20% Test: 63.40%
Split: 01, Run: 02, Epoch: 400, Loss: 111.9142, Train: 100.00%, Valid: 63.80% Test: 62.40%
Split: 01, Run: 02, Epoch: 500, Loss: 110.1975, Train: 100.00%, Valid: 64.80% Test: 63.80%
Split: 01, Run: 02
None time:  7.519296150654554
None Run 02:
Highest Train: 100.00
Highest Valid: 69.40
  Final Train: 99.29
   Final Test: 67.40
Split: 01, Run: 03, Epoch: 100, Loss: 127.5484, Train: 98.57%, Valid: 65.80% Test: 68.10%
Split: 01, Run: 03, Epoch: 200, Loss: 118.2612, Train: 100.00%, Valid: 64.80% Test: 65.80%
Split: 01, Run: 03, Epoch: 300, Loss: 114.5156, Train: 100.00%, Valid: 64.60% Test: 63.20%
Split: 01, Run: 03, Epoch: 400, Loss: 108.3647, Train: 100.00%, Valid: 65.20% Test: 63.60%
Split: 01, Run: 03, Epoch: 500, Loss: 106.3667, Train: 100.00%, Valid: 66.20% Test: 66.30%
Split: 01, Run: 03
None time:  7.528050314635038
None Run 03:
Highest Train: 100.00
Highest Valid: 76.00
  Final Train: 97.86
   Final Test: 76.60
total time:  22.624389262869954
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.20 Â± 4.20
  Final Train: 98.10 Â± 1.09
   Final Test: 70.10 Â± 5.66
[32m[I 2021-07-26 09:18:45,514][0m Trial 9 finished with value: 71.20000457763672 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.3, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.02
lambda2:  0.4
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.4, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 186.5640, Train: 97.86%, Valid: 66.20% Test: 66.90%
Split: 01, Run: 01, Epoch: 200, Loss: 167.9039, Train: 100.00%, Valid: 68.80% Test: 68.20%
Split: 01, Run: 01, Epoch: 300, Loss: 169.4800, Train: 100.00%, Valid: 66.60% Test: 68.20%
Split: 01, Run: 01, Epoch: 400, Loss: 155.9754, Train: 100.00%, Valid: 68.60% Test: 70.90%
Split: 01, Run: 01, Epoch: 500, Loss: 153.3987, Train: 100.00%, Valid: 65.60% Test: 65.90%
Split: 01, Run: 01
None time:  7.527082111686468
None Run 01:
Highest Train: 100.00
Highest Valid: 69.60
  Final Train: 100.00
   Final Test: 71.70
Split: 01, Run: 02, Epoch: 100, Loss: 175.5091, Train: 99.29%, Valid: 66.00% Test: 66.10%
Split: 01, Run: 02, Epoch: 200, Loss: 156.5637, Train: 100.00%, Valid: 64.80% Test: 64.70%
Split: 01, Run: 02, Epoch: 300, Loss: 152.9579, Train: 100.00%, Valid: 67.60% Test: 66.00%
Split: 01, Run: 02, Epoch: 400, Loss: 149.7007, Train: 100.00%, Valid: 67.00% Test: 65.90%
Split: 01, Run: 02, Epoch: 500, Loss: 149.9639, Train: 100.00%, Valid: 67.00% Test: 65.40%
Split: 01, Run: 02
None time:  7.519360352307558
None Run 02:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 99.29
   Final Test: 68.00
Split: 01, Run: 03, Epoch: 100, Loss: 177.6311, Train: 99.29%, Valid: 74.00% Test: 75.60%
Split: 01, Run: 03, Epoch: 200, Loss: 163.5249, Train: 100.00%, Valid: 66.60% Test: 69.80%
Split: 01, Run: 03, Epoch: 300, Loss: 156.1477, Train: 100.00%, Valid: 63.80% Test: 65.00%
Split: 01, Run: 03, Epoch: 400, Loss: 151.1016, Train: 100.00%, Valid: 65.40% Test: 65.60%
Split: 01, Run: 03, Epoch: 500, Loss: 145.0680, Train: 100.00%, Valid: 64.60% Test: 65.50%
Split: 01, Run: 03
None time:  7.504934152588248
None Run 03:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 99.29
   Final Test: 76.60
total time:  22.611533299088478
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.13 Â± 3.56
  Final Train: 99.52 Â± 0.41
   Final Test: 72.10 Â± 4.31
[32m[I 2021-07-26 09:19:08,131][0m Trial 10 finished with value: 72.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.4, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.05
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 20.6896, Train: 95.00%, Valid: 37.00% Test: 37.90%
Split: 01, Run: 01, Epoch: 200, Loss: 19.0520, Train: 96.43%, Valid: 37.00% Test: 38.50%
Split: 01, Run: 01, Epoch: 300, Loss: 20.0602, Train: 99.29%, Valid: 39.60% Test: 39.40%
Split: 01, Run: 01, Epoch: 400, Loss: 19.4886, Train: 100.00%, Valid: 41.00% Test: 41.60%
Split: 01, Run: 01, Epoch: 500, Loss: 17.7536, Train: 100.00%, Valid: 37.40% Test: 38.30%
Split: 01, Run: 01
None time:  7.559272821992636
None Run 01:
Highest Train: 100.00
Highest Valid: 43.80
  Final Train: 100.00
   Final Test: 42.40
Split: 01, Run: 02, Epoch: 100, Loss: 19.8847, Train: 97.86%, Valid: 44.80% Test: 45.90%
Split: 01, Run: 02, Epoch: 200, Loss: 19.2257, Train: 98.57%, Valid: 42.40% Test: 43.90%
Split: 01, Run: 02, Epoch: 300, Loss: 20.0948, Train: 98.57%, Valid: 41.80% Test: 46.00%
Split: 01, Run: 02, Epoch: 400, Loss: 20.3805, Train: 100.00%, Valid: 41.40% Test: 45.00%
Split: 01, Run: 02, Epoch: 500, Loss: 19.9250, Train: 100.00%, Valid: 41.40% Test: 44.60%
Split: 01, Run: 02
None time:  7.510325517505407
None Run 02:
Highest Train: 100.00
Highest Valid: 52.80
  Final Train: 85.00
   Final Test: 51.00
Split: 01, Run: 03, Epoch: 100, Loss: 21.3602, Train: 97.86%, Valid: 52.60% Test: 52.40%
Split: 01, Run: 03, Epoch: 200, Loss: 19.5097, Train: 97.86%, Valid: 43.40% Test: 45.40%
Split: 01, Run: 03, Epoch: 300, Loss: 21.3932, Train: 98.57%, Valid: 41.40% Test: 43.00%
Split: 01, Run: 03, Epoch: 400, Loss: 19.5525, Train: 98.57%, Valid: 42.00% Test: 42.60%
Split: 01, Run: 03, Epoch: 500, Loss: 18.1414, Train: 100.00%, Valid: 38.20% Test: 41.50%
Split: 01, Run: 03
None time:  7.513332013040781
None Run 03:
Highest Train: 100.00
Highest Valid: 66.80
  Final Train: 97.14
   Final Test: 68.30
total time:  22.632461950182915
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 54.47 Â± 11.59
  Final Train: 94.05 Â± 7.96
   Final Test: 53.90 Â± 13.19
[32m[I 2021-07-26 09:19:30,769][0m Trial 11 finished with value: 54.4666633605957 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.01
lambda2:  0.3
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.3, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 166.9718, Train: 97.86%, Valid: 66.60% Test: 67.10%
Split: 01, Run: 01, Epoch: 200, Loss: 151.3442, Train: 99.29%, Valid: 68.80% Test: 68.30%
Split: 01, Run: 01, Epoch: 300, Loss: 149.0070, Train: 99.29%, Valid: 67.00% Test: 68.00%
Split: 01, Run: 01, Epoch: 400, Loss: 137.3037, Train: 100.00%, Valid: 69.20% Test: 70.10%
Split: 01, Run: 01, Epoch: 500, Loss: 133.5724, Train: 100.00%, Valid: 66.20% Test: 65.70%
Split: 01, Run: 01
None time:  7.51674790494144
None Run 01:
Highest Train: 100.00
Highest Valid: 70.60
  Final Train: 98.57
   Final Test: 69.50
Split: 01, Run: 02, Epoch: 100, Loss: 157.5476, Train: 99.29%, Valid: 65.80% Test: 66.60%
Split: 01, Run: 02, Epoch: 200, Loss: 138.4446, Train: 100.00%, Valid: 65.40% Test: 66.20%
Split: 01, Run: 02, Epoch: 300, Loss: 137.9323, Train: 100.00%, Valid: 67.60% Test: 67.70%
Split: 01, Run: 02, Epoch: 400, Loss: 134.9730, Train: 100.00%, Valid: 68.20% Test: 68.60%
Split: 01, Run: 02, Epoch: 500, Loss: 134.9282, Train: 100.00%, Valid: 69.80% Test: 70.50%
Split: 01, Run: 02
None time:  7.51455663703382
None Run 02:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 73.00
Split: 01, Run: 03, Epoch: 100, Loss: 157.6188, Train: 97.86%, Valid: 70.00% Test: 70.40%
Split: 01, Run: 03, Epoch: 200, Loss: 147.1299, Train: 99.29%, Valid: 66.00% Test: 67.40%
Split: 01, Run: 03, Epoch: 300, Loss: 137.8564, Train: 100.00%, Valid: 63.60% Test: 64.00%
Split: 01, Run: 03, Epoch: 400, Loss: 133.4910, Train: 100.00%, Valid: 66.40% Test: 66.60%
Split: 01, Run: 03, Epoch: 500, Loss: 129.6924, Train: 100.00%, Valid: 65.40% Test: 66.50%
Split: 01, Run: 03
None time:  7.524136062711477
None Run 03:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 97.86
   Final Test: 75.40
total time:  22.60803711041808
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 72.93 Â± 2.52
  Final Train: 98.81 Â± 1.09
   Final Test: 72.63 Â± 2.97
[32m[I 2021-07-26 09:19:53,383][0m Trial 12 finished with value: 72.9333267211914 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.3, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.02
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 84.8885, Train: 97.14%, Valid: 61.80% Test: 60.00%
Split: 01, Run: 01, Epoch: 200, Loss: 77.9255, Train: 98.57%, Valid: 60.80% Test: 58.70%
Split: 01, Run: 01, Epoch: 300, Loss: 78.9633, Train: 99.29%, Valid: 60.00% Test: 58.40%
Split: 01, Run: 01, Epoch: 400, Loss: 74.0747, Train: 99.29%, Valid: 62.20% Test: 63.40%
Split: 01, Run: 01, Epoch: 500, Loss: 71.2802, Train: 99.29%, Valid: 60.80% Test: 59.20%
Split: 01, Run: 01
None time:  7.559398882091045
None Run 01:
Highest Train: 99.29
Highest Valid: 65.00
  Final Train: 96.43
   Final Test: 62.50
Split: 01, Run: 02, Epoch: 100, Loss: 80.0455, Train: 97.86%, Valid: 62.20% Test: 62.10%
Split: 01, Run: 02, Epoch: 200, Loss: 72.4895, Train: 100.00%, Valid: 60.00% Test: 60.90%
Split: 01, Run: 02, Epoch: 300, Loss: 73.0259, Train: 100.00%, Valid: 60.20% Test: 60.00%
Split: 01, Run: 02, Epoch: 400, Loss: 73.9765, Train: 100.00%, Valid: 61.20% Test: 61.60%
Split: 01, Run: 02, Epoch: 500, Loss: 72.8124, Train: 100.00%, Valid: 60.80% Test: 60.70%
Split: 01, Run: 02
None time:  7.49707879498601
None Run 02:
Highest Train: 100.00
Highest Valid: 65.80
  Final Train: 98.57
   Final Test: 62.20
Split: 01, Run: 03, Epoch: 100, Loss: 82.0134, Train: 98.57%, Valid: 62.40% Test: 64.10%
Split: 01, Run: 03, Epoch: 200, Loss: 74.0743, Train: 99.29%, Valid: 61.60% Test: 63.30%
Split: 01, Run: 03, Epoch: 300, Loss: 73.3861, Train: 99.29%, Valid: 63.00% Test: 61.90%
Split: 01, Run: 03, Epoch: 400, Loss: 70.7398, Train: 99.29%, Valid: 63.00% Test: 62.80%
Split: 01, Run: 03, Epoch: 500, Loss: 67.2295, Train: 99.29%, Valid: 62.00% Test: 61.60%
Split: 01, Run: 03
None time:  7.577704729512334
None Run 03:
Highest Train: 100.00
Highest Valid: 74.60
  Final Train: 97.86
   Final Test: 75.30
total time:  22.681492667645216
None All runs:
Highest Train: 99.76 Â± 0.41
Highest Valid: 68.47 Â± 5.33
  Final Train: 97.62 Â± 1.09
   Final Test: 66.67 Â± 7.48
[32m[I 2021-07-26 09:20:16,070][0m Trial 13 finished with value: 68.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.2, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.01
lambda2:  0.2
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.2, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 109.6316, Train: 97.14%, Valid: 64.40% Test: 63.70%
Split: 01, Run: 01, Epoch: 200, Loss: 100.7328, Train: 97.86%, Valid: 64.00% Test: 64.20%
Split: 01, Run: 01, Epoch: 300, Loss: 99.7572, Train: 99.29%, Valid: 63.80% Test: 63.10%
Split: 01, Run: 01, Epoch: 400, Loss: 91.7349, Train: 99.29%, Valid: 64.80% Test: 66.00%
Split: 01, Run: 01, Epoch: 500, Loss: 89.4412, Train: 99.29%, Valid: 62.00% Test: 61.60%
Split: 01, Run: 01
None time:  7.601871352642775
None Run 01:
Highest Train: 100.00
Highest Valid: 67.40
  Final Train: 97.14
   Final Test: 66.20
Split: 01, Run: 02, Epoch: 100, Loss: 103.2766, Train: 98.57%, Valid: 62.60% Test: 63.90%
Split: 01, Run: 02, Epoch: 200, Loss: 91.7686, Train: 99.29%, Valid: 62.20% Test: 63.70%
Split: 01, Run: 02, Epoch: 300, Loss: 92.9227, Train: 100.00%, Valid: 61.60% Test: 62.90%
Split: 01, Run: 02, Epoch: 400, Loss: 92.9202, Train: 100.00%, Valid: 63.20% Test: 62.80%
Split: 01, Run: 02, Epoch: 500, Loss: 89.2651, Train: 100.00%, Valid: 62.60% Test: 62.50%
Split: 01, Run: 02
None time:  7.594437910243869
None Run 02:
Highest Train: 100.00
Highest Valid: 67.60
  Final Train: 98.57
   Final Test: 65.50
Split: 01, Run: 03, Epoch: 100, Loss: 103.3932, Train: 97.14%, Valid: 64.20% Test: 65.00%
Split: 01, Run: 03, Epoch: 200, Loss: 93.2936, Train: 99.29%, Valid: 63.40% Test: 64.30%
Split: 01, Run: 03, Epoch: 300, Loss: 91.3074, Train: 99.29%, Valid: 63.00% Test: 63.70%
Split: 01, Run: 03, Epoch: 400, Loss: 87.9581, Train: 99.29%, Valid: 64.20% Test: 64.50%
Split: 01, Run: 03, Epoch: 500, Loss: 87.5047, Train: 99.29%, Valid: 62.40% Test: 64.00%
Split: 01, Run: 03
None time:  7.663012519478798
None Run 03:
Highest Train: 99.29
Highest Valid: 73.40
  Final Train: 97.86
   Final Test: 75.30
total time:  22.911808909848332
None All runs:
Highest Train: 99.76 Â± 0.41
Highest Valid: 69.47 Â± 3.41
  Final Train: 97.86 Â± 0.71
   Final Test: 69.00 Â± 5.47
[32m[I 2021-07-26 09:20:38,989][0m Trial 14 finished with value: 69.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.2, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
lambda1:  0.02
lambda2:  0.1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 41.1199, Train: 95.00%, Valid: 51.00% Test: 50.80%
Split: 01, Run: 01, Epoch: 200, Loss: 36.3557, Train: 93.57%, Valid: 51.40% Test: 49.60%
Split: 01, Run: 01, Epoch: 300, Loss: 37.7683, Train: 97.14%, Valid: 48.00% Test: 47.80%
Split: 01, Run: 01, Epoch: 400, Loss: 37.2415, Train: 98.57%, Valid: 52.40% Test: 50.90%
Split: 01, Run: 01, Epoch: 500, Loss: 35.2384, Train: 99.29%, Valid: 47.40% Test: 49.00%
Split: 01, Run: 01
None time:  7.702663546428084
None Run 01:
Highest Train: 99.29
Highest Valid: 53.00
  Final Train: 95.00
   Final Test: 52.50
Split: 01, Run: 02, Epoch: 100, Loss: 38.2870, Train: 95.71%, Valid: 50.40% Test: 52.60%
Split: 01, Run: 02, Epoch: 200, Loss: 36.0599, Train: 98.57%, Valid: 49.60% Test: 51.60%
Split: 01, Run: 02, Epoch: 300, Loss: 35.9651, Train: 98.57%, Valid: 50.40% Test: 50.80%
Split: 01, Run: 02, Epoch: 400, Loss: 37.4143, Train: 98.57%, Valid: 52.80% Test: 51.80%
Split: 01, Run: 02, Epoch: 500, Loss: 36.7299, Train: 99.29%, Valid: 52.60% Test: 52.20%
Split: 01, Run: 02
None time:  7.8046070877462626
None Run 02:
Highest Train: 100.00
Highest Valid: 58.80
  Final Train: 89.29
   Final Test: 56.00
Split: 01, Run: 03, Epoch: 100, Loss: 41.4906, Train: 95.00%, Valid: 52.80% Test: 54.20%
Split: 01, Run: 03, Epoch: 200, Loss: 37.4545, Train: 95.71%, Valid: 49.80% Test: 51.40%
Split: 01, Run: 03, Epoch: 300, Loss: 38.7977, Train: 97.14%, Valid: 48.80% Test: 50.90%
Split: 01, Run: 03, Epoch: 400, Loss: 36.1228, Train: 98.57%, Valid: 52.00% Test: 52.50%
Split: 01, Run: 03, Epoch: 500, Loss: 33.8258, Train: 98.57%, Valid: 51.40% Test: 52.40%
Split: 01, Run: 03
None time:  7.619892103597522
None Run 03:
Highest Train: 99.29
Highest Valid: 70.00
  Final Train: 97.14
   Final Test: 73.10
total time:  23.176239540800452
None All runs:
Highest Train: 99.52 Â± 0.41
Highest Valid: 60.60 Â± 8.64
  Final Train: 93.81 Â± 4.06
   Final Test: 60.53 Â± 11.02
[32m[I 2021-07-26 09:21:02,171][0m Trial 15 finished with value: 60.59999465942383 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}. Best is trial 8 with value: 74.93334197998047.[0m
Study statistics: 
  Number of finished trials:  16
  Number of pruned trials:  0
  Number of complete trials:  16
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.1, 'K': 10}   trial.value:  63.4    {'train': '91.19 Â± 3.38', 'valid': '63.40 Â± 5.72', 'test': '64.47 Â± 8.04'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}   trial.value:  74.933    {'train': '99.52 Â± 0.41', 'valid': '74.93 Â± 2.30', 'test': '75.10 Â± 3.25'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.3, 'K': 10}   trial.value:  72.933    {'train': '98.81 Â± 1.09', 'valid': '72.93 Â± 2.52', 'test': '72.63 Â± 2.97'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.2, 'K': 10}   trial.value:  69.467    {'train': '97.86 Â± 0.71', 'valid': '69.47 Â± 3.41', 'test': '69.00 Â± 5.47'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.3, 'K': 10}   trial.value:  71.2    {'train': '98.10 Â± 1.09', 'valid': '71.20 Â± 4.20', 'test': '70.10 Â± 5.66'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.4, 'K': 10}   trial.value:  72.133    {'train': '99.52 Â± 0.41', 'valid': '72.13 Â± 3.56', 'test': '72.10 Â± 4.31'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.2, 'K': 10}   trial.value:  68.467    {'train': '97.62 Â± 1.09', 'valid': '68.47 Â± 5.33', 'test': '66.67 Â± 7.48'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.1, 'K': 10}   trial.value:  60.6    {'train': '93.81 Â± 4.06', 'valid': '60.60 Â± 8.64', 'test': '60.53 Â± 11.02'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.2, 'K': 10}   trial.value:  62.267    {'train': '98.33 Â± 1.49', 'valid': '62.27 Â± 8.76', 'test': '62.00 Â± 10.71'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.3, 'K': 10}   trial.value:  66.2    {'train': '99.52 Â± 0.82', 'valid': '66.20 Â± 8.79', 'test': '64.47 Â± 9.89'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.4, 'K': 10}   trial.value:  68.6    {'train': '99.29 Â± 0.71', 'valid': '68.60 Â± 6.50', 'test': '66.83 Â± 7.48'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.1, 'K': 10}   trial.value:  54.467    {'train': '94.05 Â± 7.96', 'valid': '54.47 Â± 11.59', 'test': '53.90 Â± 13.19'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.1, 'K': 10}   trial.value:  42.6    {'train': '91.90 Â± 7.15', 'valid': '42.60 Â± 12.12', 'test': '42.43 Â± 11.71'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.2, 'K': 10}   trial.value:  53.6    {'train': '96.43 Â± 5.00', 'valid': '53.60 Â± 15.15', 'test': '53.23 Â± 14.47'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.3, 'K': 10}   trial.value:  58.933    {'train': '99.29 Â± 0.71', 'valid': '58.93 Â± 14.49', 'test': '58.43 Â± 13.29'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.4, 'K': 10}   trial.value:  61.133    {'train': '99.52 Â± 0.82', 'valid': '61.13 Â± 14.16', 'test': '59.87 Â± 12.53'}
test_acc
['64.47 Â± 8.04', '75.10 Â± 3.25', '72.63 Â± 2.97', '69.00 Â± 5.47', '70.10 Â± 5.66', '72.10 Â± 4.31', '66.67 Â± 7.48', '60.53 Â± 11.02', '62.00 Â± 10.71', '64.47 Â± 9.89', '66.83 Â± 7.48', '53.90 Â± 13.19', '42.43 Â± 11.71', '53.23 Â± 14.47', '58.43 Â± 13.29', '59.87 Â± 12.53']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.4, 'K': 10}
Best trial Value:  74.93334197998047
Best trial Acc:  {'train': '99.52 Â± 0.41', 'valid': '74.93 Â± 2.30', 'test': '75.10 Â± 3.25'}
optuna total time:  401.0705239716917
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05, 0.1], 'lambda2': [0.5, 0.7, 1], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  12
[32m[I 2021-07-26 09:23:35,539][0m A new study created in memory with name: no-name-25793352-a439-4ba0-bb8d-e8f304fee156[0m
lambda1:  0.1
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 148.5975, Train: 100.00%, Valid: 48.80% Test: 50.40%
Split: 01, Run: 01, Epoch: 200, Loss: 132.8015, Train: 100.00%, Valid: 47.00% Test: 47.90%
Split: 01, Run: 01, Epoch: 300, Loss: 133.9292, Train: 100.00%, Valid: 50.60% Test: 52.30%
Split: 01, Run: 01, Epoch: 400, Loss: 126.4905, Train: 100.00%, Valid: 55.00% Test: 57.50%
Split: 01, Run: 01, Epoch: 500, Loss: 120.5444, Train: 100.00%, Valid: 50.80% Test: 54.10%
Split: 01, Run: 01
None time:  7.82599532790482
None Run 01:
Highest Train: 100.00
Highest Valid: 58.20
  Final Train: 100.00
   Final Test: 59.90
Split: 01, Run: 02, Epoch: 100, Loss: 139.9330, Train: 100.00%, Valid: 51.00% Test: 53.10%
Split: 01, Run: 02, Epoch: 200, Loss: 127.5412, Train: 100.00%, Valid: 47.80% Test: 50.30%
Split: 01, Run: 02, Epoch: 300, Loss: 120.5008, Train: 100.00%, Valid: 46.20% Test: 49.20%
Split: 01, Run: 02, Epoch: 400, Loss: 121.6515, Train: 100.00%, Valid: 49.80% Test: 52.40%
Split: 01, Run: 02, Epoch: 500, Loss: 126.4876, Train: 100.00%, Valid: 49.40% Test: 50.20%
Split: 01, Run: 02
None time:  7.637002889066935
None Run 02:
Highest Train: 100.00
Highest Valid: 63.20
  Final Train: 100.00
   Final Test: 60.50
Split: 01, Run: 03, Epoch: 100, Loss: 145.1225, Train: 100.00%, Valid: 58.60% Test: 60.60%
Split: 01, Run: 03, Epoch: 200, Loss: 134.1435, Train: 100.00%, Valid: 51.00% Test: 55.20%
Split: 01, Run: 03, Epoch: 300, Loss: 130.5690, Train: 100.00%, Valid: 51.00% Test: 51.30%
Split: 01, Run: 03, Epoch: 400, Loss: 125.3006, Train: 100.00%, Valid: 53.60% Test: 53.80%
Split: 01, Run: 03, Epoch: 500, Loss: 121.4954, Train: 100.00%, Valid: 51.00% Test: 51.20%
Split: 01, Run: 03
None time:  7.687625784426928
None Run 03:
Highest Train: 100.00
Highest Valid: 75.00
  Final Train: 98.57
   Final Test: 74.60
total time:  24.833896147087216
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 65.47 Â± 8.63
  Final Train: 99.52 Â± 0.82
   Final Test: 65.00 Â± 8.32
[32m[I 2021-07-26 09:24:00,402][0m Trial 0 finished with value: 65.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.7, 'K': 10}. Best is trial 0 with value: 65.46666717529297.[0m
lambda1:  0.05
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 397.0610, Train: 100.00%, Valid: 64.00% Test: 64.20%
Split: 01, Run: 01, Epoch: 200, Loss: 357.3250, Train: 100.00%, Valid: 64.00% Test: 64.30%
Split: 01, Run: 01, Epoch: 300, Loss: 353.8421, Train: 100.00%, Valid: 64.00% Test: 62.90%
Split: 01, Run: 01, Epoch: 400, Loss: 323.4117, Train: 100.00%, Valid: 65.80% Test: 66.50%
Split: 01, Run: 01, Epoch: 500, Loss: 311.8922, Train: 100.00%, Valid: 63.80% Test: 62.70%
Split: 01, Run: 01
None time:  7.772404752671719
None Run 01:
Highest Train: 100.00
Highest Valid: 68.20
  Final Train: 99.29
   Final Test: 67.70
Split: 01, Run: 02, Epoch: 100, Loss: 374.6374, Train: 100.00%, Valid: 67.80% Test: 67.50%
Split: 01, Run: 02, Epoch: 200, Loss: 332.4854, Train: 100.00%, Valid: 64.80% Test: 63.70%
Split: 01, Run: 02, Epoch: 300, Loss: 315.9682, Train: 100.00%, Valid: 64.80% Test: 63.90%
Split: 01, Run: 02, Epoch: 400, Loss: 322.5914, Train: 100.00%, Valid: 63.40% Test: 62.50%
Split: 01, Run: 02, Epoch: 500, Loss: 319.3086, Train: 100.00%, Valid: 63.60% Test: 63.80%
Split: 01, Run: 02
None time:  7.682002671062946
None Run 02:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 68.00
Split: 01, Run: 03, Epoch: 100, Loss: 386.3424, Train: 100.00%, Valid: 73.60% Test: 74.40%
Split: 01, Run: 03, Epoch: 200, Loss: 355.2132, Train: 100.00%, Valid: 63.40% Test: 68.00%
Split: 01, Run: 03, Epoch: 300, Loss: 332.7571, Train: 100.00%, Valid: 66.80% Test: 67.50%
Split: 01, Run: 03, Epoch: 400, Loss: 320.4848, Train: 100.00%, Valid: 66.40% Test: 66.70%
Split: 01, Run: 03, Epoch: 500, Loss: 307.0777, Train: 100.00%, Valid: 66.20% Test: 66.90%
Split: 01, Run: 03
None time:  7.728214273229241
None Run 03:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 98.57
   Final Test: 77.80
total time:  23.243265414610505
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.93 Â± 4.41
  Final Train: 99.29 Â± 0.71
   Final Test: 71.17 Â± 5.75
[32m[I 2021-07-26 09:24:23,651][0m Trial 1 finished with value: 71.93333435058594 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 1, 'K': 10}. Best is trial 1 with value: 71.93333435058594.[0m
lambda1:  0.1
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 92.5468, Train: 100.00%, Valid: 44.80% Test: 46.30%
Split: 01, Run: 01, Epoch: 200, Loss: 84.0427, Train: 100.00%, Valid: 44.20% Test: 45.90%
Split: 01, Run: 01, Epoch: 300, Loss: 83.3621, Train: 100.00%, Valid: 48.00% Test: 50.00%
Split: 01, Run: 01, Epoch: 400, Loss: 79.1251, Train: 100.00%, Valid: 51.00% Test: 54.30%
Split: 01, Run: 01, Epoch: 500, Loss: 78.5713, Train: 100.00%, Valid: 48.60% Test: 50.00%
Split: 01, Run: 01
None time:  8.594970043748617
None Run 01:
Highest Train: 100.00
Highest Valid: 53.00
  Final Train: 99.29
   Final Test: 53.00
Split: 01, Run: 02, Epoch: 100, Loss: 88.5283, Train: 100.00%, Valid: 47.20% Test: 49.50%
Split: 01, Run: 02, Epoch: 200, Loss: 79.9190, Train: 100.00%, Valid: 43.80% Test: 47.10%
Split: 01, Run: 02, Epoch: 300, Loss: 76.8197, Train: 100.00%, Valid: 41.60% Test: 46.00%
Split: 01, Run: 02, Epoch: 400, Loss: 76.1590, Train: 100.00%, Valid: 45.40% Test: 48.60%
Split: 01, Run: 02, Epoch: 500, Loss: 82.7026, Train: 100.00%, Valid: 44.40% Test: 46.90%
Split: 01, Run: 02
None time:  8.537815356627107
None Run 02:
Highest Train: 100.00
Highest Valid: 60.60
  Final Train: 100.00
   Final Test: 56.20
Split: 01, Run: 03, Epoch: 100, Loss: 90.6857, Train: 100.00%, Valid: 50.20% Test: 51.70%
Split: 01, Run: 03, Epoch: 200, Loss: 84.2424, Train: 100.00%, Valid: 55.20% Test: 58.30%
Split: 01, Run: 03, Epoch: 300, Loss: 82.1096, Train: 100.00%, Valid: 46.20% Test: 47.80%
Split: 01, Run: 03, Epoch: 400, Loss: 77.7434, Train: 100.00%, Valid: 47.00% Test: 49.10%
Split: 01, Run: 03, Epoch: 500, Loss: 78.0056, Train: 100.00%, Valid: 46.80% Test: 48.70%
Split: 01, Run: 03
None time:  8.535283477976918
None Run 03:
Highest Train: 100.00
Highest Valid: 75.20
  Final Train: 99.29
   Final Test: 74.50
total time:  25.731185141950846
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 62.93 Â± 11.28
  Final Train: 99.52 Â± 0.41
   Final Test: 61.23 Â± 11.60
[32m[I 2021-07-26 09:24:49,388][0m Trial 2 finished with value: 62.93333053588867 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.5, 'K': 10}. Best is trial 1 with value: 71.93333435058594.[0m
lambda1:  0.1
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.1, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 251.6022, Train: 100.00%, Valid: 51.20% Test: 53.10%
Split: 01, Run: 01, Epoch: 200, Loss: 218.8589, Train: 100.00%, Valid: 50.80% Test: 51.30%
Split: 01, Run: 01, Epoch: 300, Loss: 215.1293, Train: 100.00%, Valid: 50.00% Test: 52.00%
Split: 01, Run: 01, Epoch: 400, Loss: 206.8874, Train: 100.00%, Valid: 56.20% Test: 59.60%
Split: 01, Run: 01, Epoch: 500, Loss: 201.2780, Train: 100.00%, Valid: 52.80% Test: 54.50%
Split: 01, Run: 01
None time:  8.529720861464739
None Run 01:
Highest Train: 100.00
Highest Valid: 60.20
  Final Train: 100.00
   Final Test: 57.80
Split: 01, Run: 02, Epoch: 100, Loss: 235.2786, Train: 100.00%, Valid: 56.40% Test: 58.60%
Split: 01, Run: 02, Epoch: 200, Loss: 213.4861, Train: 100.00%, Valid: 51.60% Test: 55.20%
Split: 01, Run: 02, Epoch: 300, Loss: 198.2386, Train: 100.00%, Valid: 51.80% Test: 54.30%
Split: 01, Run: 02, Epoch: 400, Loss: 204.6313, Train: 100.00%, Valid: 57.60% Test: 57.90%
Split: 01, Run: 02, Epoch: 500, Loss: 205.3957, Train: 100.00%, Valid: 54.60% Test: 55.70%
Split: 01, Run: 02
None time:  8.607939580455422
None Run 02:
Highest Train: 100.00
Highest Valid: 65.80
  Final Train: 100.00
   Final Test: 62.30
Split: 01, Run: 03, Epoch: 100, Loss: 243.5406, Train: 100.00%, Valid: 69.60% Test: 72.60%
Split: 01, Run: 03, Epoch: 200, Loss: 229.5067, Train: 100.00%, Valid: 53.60% Test: 56.10%
Split: 01, Run: 03, Epoch: 300, Loss: 214.4194, Train: 100.00%, Valid: 54.00% Test: 54.70%
Split: 01, Run: 03, Epoch: 400, Loss: 206.8042, Train: 100.00%, Valid: 54.20% Test: 54.10%
Split: 01, Run: 03, Epoch: 500, Loss: 201.5097, Train: 100.00%, Valid: 52.00% Test: 52.20%
Split: 01, Run: 03
None time:  8.355096727609634
None Run 03:
Highest Train: 100.00
Highest Valid: 75.00
  Final Train: 99.29
   Final Test: 74.40
total time:  25.55600655451417
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 67.00 Â± 7.47
  Final Train: 99.76 Â± 0.41
   Final Test: 64.83 Â± 8.59
[32m[I 2021-07-26 09:25:14,950][0m Trial 3 finished with value: 67.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 1, 'K': 10}. Best is trial 1 with value: 71.93333435058594.[0m
lambda1:  0.01
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 716.8908, Train: 98.57%, Valid: 70.80% Test: 73.30%
Split: 01, Run: 01, Epoch: 200, Loss: 621.0987, Train: 100.00%, Valid: 73.20% Test: 73.90%
Split: 01, Run: 01, Epoch: 300, Loss: 618.8636, Train: 100.00%, Valid: 76.20% Test: 73.70%
Split: 01, Run: 01, Epoch: 400, Loss: 572.2792, Train: 100.00%, Valid: 75.60% Test: 76.10%
Split: 01, Run: 01, Epoch: 500, Loss: 555.8688, Train: 100.00%, Valid: 74.80% Test: 73.70%
Split: 01, Run: 01
None time:  7.585036231204867
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 76.20
Split: 01, Run: 02, Epoch: 100, Loss: 666.0555, Train: 100.00%, Valid: 73.60% Test: 75.10%
Split: 01, Run: 02, Epoch: 200, Loss: 581.6281, Train: 100.00%, Valid: 72.80% Test: 74.50%
Split: 01, Run: 02, Epoch: 300, Loss: 553.3561, Train: 100.00%, Valid: 74.00% Test: 76.30%
Split: 01, Run: 02, Epoch: 400, Loss: 549.4619, Train: 100.00%, Valid: 71.80% Test: 73.40%
Split: 01, Run: 02, Epoch: 500, Loss: 542.9150, Train: 100.00%, Valid: 74.40% Test: 77.70%
Split: 01, Run: 02
None time:  7.691248385235667
None Run 02:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 100.00
   Final Test: 79.40
Split: 01, Run: 03, Epoch: 100, Loss: 662.9578, Train: 98.57%, Valid: 76.80% Test: 78.20%
Split: 01, Run: 03, Epoch: 200, Loss: 600.7442, Train: 100.00%, Valid: 76.80% Test: 80.00%
Split: 01, Run: 03, Epoch: 300, Loss: 558.0021, Train: 100.00%, Valid: 77.00% Test: 79.00%
Split: 01, Run: 03, Epoch: 400, Loss: 556.1805, Train: 100.00%, Valid: 77.00% Test: 79.50%
Split: 01, Run: 03, Epoch: 500, Loss: 536.5043, Train: 100.00%, Valid: 77.40% Test: 80.20%
Split: 01, Run: 03
None time:  7.5575788654387
None Run 03:
Highest Train: 100.00
Highest Valid: 78.20
  Final Train: 100.00
   Final Test: 79.80
total time:  22.89275355450809
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.07 Â± 1.03
  Final Train: 100.00 Â± 0.00
   Final Test: 78.47 Â± 1.97
[32m[I 2021-07-26 09:25:37,848][0m Trial 4 finished with value: 77.06666564941406 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 1, 'K': 10}. Best is trial 4 with value: 77.06666564941406.[0m
lambda1:  0.02
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 592.9175, Train: 99.29%, Valid: 68.40% Test: 70.50%
Split: 01, Run: 01, Epoch: 200, Loss: 521.5066, Train: 100.00%, Valid: 72.40% Test: 72.60%
Split: 01, Run: 01, Epoch: 300, Loss: 524.2834, Train: 100.00%, Valid: 72.60% Test: 74.00%
Split: 01, Run: 01, Epoch: 400, Loss: 479.5249, Train: 100.00%, Valid: 77.20% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 472.4229, Train: 100.00%, Valid: 73.00% Test: 74.40%
Split: 01, Run: 01
None time:  7.511364437639713
None Run 01:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 100.00
   Final Test: 78.40
Split: 01, Run: 02, Epoch: 100, Loss: 558.1734, Train: 100.00%, Valid: 71.20% Test: 74.00%
Split: 01, Run: 02, Epoch: 200, Loss: 487.3016, Train: 100.00%, Valid: 71.20% Test: 70.20%
Split: 01, Run: 02, Epoch: 300, Loss: 469.8120, Train: 100.00%, Valid: 72.40% Test: 72.80%
Split: 01, Run: 02, Epoch: 400, Loss: 453.5393, Train: 100.00%, Valid: 69.20% Test: 71.20%
Split: 01, Run: 02, Epoch: 500, Loss: 463.9928, Train: 100.00%, Valid: 74.60% Test: 78.10%
Split: 01, Run: 02
None time:  7.536129377782345
None Run 02:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 78.10
Split: 01, Run: 03, Epoch: 100, Loss: 564.6490, Train: 100.00%, Valid: 76.20% Test: 77.40%
Split: 01, Run: 03, Epoch: 200, Loss: 516.8737, Train: 100.00%, Valid: 75.20% Test: 78.40%
Split: 01, Run: 03, Epoch: 300, Loss: 474.1263, Train: 100.00%, Valid: 75.80% Test: 76.80%
Split: 01, Run: 03, Epoch: 400, Loss: 469.3538, Train: 100.00%, Valid: 73.60% Test: 76.60%
Split: 01, Run: 03, Epoch: 500, Loss: 457.7332, Train: 100.00%, Valid: 73.20% Test: 75.50%
Split: 01, Run: 03
None time:  7.5540410205721855
None Run 03:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 100.00
   Final Test: 79.30
total time:  22.6519872918725
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.07 Â± 0.58
  Final Train: 100.00 Â± 0.00
   Final Test: 78.60 Â± 0.62
[32m[I 2021-07-26 09:26:00,506][0m Trial 5 finished with value: 77.0666732788086 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.05
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 155.7217, Train: 100.00%, Valid: 59.20% Test: 58.10%
Split: 01, Run: 01, Epoch: 200, Loss: 137.2589, Train: 100.00%, Valid: 60.00% Test: 59.70%
Split: 01, Run: 01, Epoch: 300, Loss: 138.3974, Train: 100.00%, Valid: 59.20% Test: 59.20%
Split: 01, Run: 01, Epoch: 400, Loss: 129.7901, Train: 100.00%, Valid: 61.40% Test: 60.90%
Split: 01, Run: 01, Epoch: 500, Loss: 124.1418, Train: 100.00%, Valid: 62.00% Test: 60.70%
Split: 01, Run: 01
None time:  7.649760263040662
None Run 01:
Highest Train: 100.00
Highest Valid: 64.40
  Final Train: 100.00
   Final Test: 61.80
Split: 01, Run: 02, Epoch: 100, Loss: 146.9274, Train: 100.00%, Valid: 60.60% Test: 61.00%
Split: 01, Run: 02, Epoch: 200, Loss: 131.4936, Train: 100.00%, Valid: 59.20% Test: 58.80%
Split: 01, Run: 02, Epoch: 300, Loss: 129.6758, Train: 100.00%, Valid: 60.60% Test: 59.30%
Split: 01, Run: 02, Epoch: 400, Loss: 128.9052, Train: 100.00%, Valid: 63.80% Test: 63.60%
Split: 01, Run: 02, Epoch: 500, Loss: 131.5558, Train: 100.00%, Valid: 64.00% Test: 63.80%
Split: 01, Run: 02
None time:  7.563603227958083
None Run 02:
Highest Train: 100.00
Highest Valid: 67.20
  Final Train: 100.00
   Final Test: 63.70
Split: 01, Run: 03, Epoch: 100, Loss: 151.3408, Train: 100.00%, Valid: 71.20% Test: 72.30%
Split: 01, Run: 03, Epoch: 200, Loss: 141.0094, Train: 100.00%, Valid: 60.00% Test: 63.80%
Split: 01, Run: 03, Epoch: 300, Loss: 133.3622, Train: 100.00%, Valid: 59.20% Test: 59.30%
Split: 01, Run: 03, Epoch: 400, Loss: 129.5757, Train: 100.00%, Valid: 60.60% Test: 59.00%
Split: 01, Run: 03, Epoch: 500, Loss: 125.1004, Train: 100.00%, Valid: 59.20% Test: 58.50%
Split: 01, Run: 03
None time:  7.5332235638052225
None Run 03:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 98.57
   Final Test: 76.30
total time:  22.803989432752132
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 69.27 Â± 6.17
  Final Train: 99.52 Â± 0.82
   Final Test: 67.27 Â± 7.88
[32m[I 2021-07-26 09:26:23,315][0m Trial 6 finished with value: 69.26667022705078 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.5, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.05
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 241.5226, Train: 100.00%, Valid: 61.00% Test: 61.60%
Split: 01, Run: 01, Epoch: 200, Loss: 217.8604, Train: 100.00%, Valid: 62.60% Test: 63.10%
Split: 01, Run: 01, Epoch: 300, Loss: 215.1773, Train: 100.00%, Valid: 64.20% Test: 62.70%
Split: 01, Run: 01, Epoch: 400, Loss: 200.0375, Train: 100.00%, Valid: 66.00% Test: 65.40%
Split: 01, Run: 01, Epoch: 500, Loss: 192.2067, Train: 100.00%, Valid: 64.40% Test: 63.40%
Split: 01, Run: 01
None time:  7.606834538280964
None Run 01:
Highest Train: 100.00
Highest Valid: 68.60
  Final Train: 100.00
   Final Test: 69.00
Split: 01, Run: 02, Epoch: 100, Loss: 228.4145, Train: 100.00%, Valid: 64.80% Test: 63.60%
Split: 01, Run: 02, Epoch: 200, Loss: 207.9018, Train: 100.00%, Valid: 63.20% Test: 62.20%
Split: 01, Run: 02, Epoch: 300, Loss: 196.9994, Train: 100.00%, Valid: 64.60% Test: 63.30%
Split: 01, Run: 02, Epoch: 400, Loss: 198.8398, Train: 100.00%, Valid: 65.60% Test: 66.10%
Split: 01, Run: 02, Epoch: 500, Loss: 196.7867, Train: 100.00%, Valid: 66.20% Test: 67.10%
Split: 01, Run: 02
None time:  7.597202133387327
None Run 02:
Highest Train: 100.00
Highest Valid: 69.00
  Final Train: 100.00
   Final Test: 69.30
Split: 01, Run: 03, Epoch: 100, Loss: 235.3830, Train: 100.00%, Valid: 72.80% Test: 73.70%
Split: 01, Run: 03, Epoch: 200, Loss: 215.8106, Train: 100.00%, Valid: 64.60% Test: 67.10%
Split: 01, Run: 03, Epoch: 300, Loss: 205.2302, Train: 100.00%, Valid: 63.80% Test: 63.50%
Split: 01, Run: 03, Epoch: 400, Loss: 202.6054, Train: 100.00%, Valid: 63.80% Test: 63.80%
Split: 01, Run: 03, Epoch: 500, Loss: 193.2564, Train: 100.00%, Valid: 60.80% Test: 61.40%
Split: 01, Run: 03
None time:  7.6321399211883545
None Run 03:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 98.57
   Final Test: 76.20
total time:  22.887627767398953
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 71.33 Â± 4.39
  Final Train: 99.52 Â± 0.82
   Final Test: 71.50 Â± 4.07
[32m[I 2021-07-26 09:26:46,208][0m Trial 7 finished with value: 71.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.7, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.01
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 445.3999, Train: 99.29%, Valid: 70.20% Test: 72.50%
Split: 01, Run: 01, Epoch: 200, Loss: 398.7984, Train: 100.00%, Valid: 74.00% Test: 74.10%
Split: 01, Run: 01, Epoch: 300, Loss: 393.8703, Train: 100.00%, Valid: 73.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 400, Loss: 365.6066, Train: 100.00%, Valid: 73.20% Test: 75.50%
Split: 01, Run: 01, Epoch: 500, Loss: 357.2861, Train: 100.00%, Valid: 72.00% Test: 73.90%
Split: 01, Run: 01
None time:  7.9831989873200655
None Run 01:
Highest Train: 100.00
Highest Valid: 74.80
  Final Train: 100.00
   Final Test: 77.20
Using backend: pytorch
main:  Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=None, lambda2=None, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
search_space:  {'lr': [0.01], 'weight_decay': [0.0005], 'lambda1': [0.01, 0.02, 0.05], 'lambda2': [0.5, 0.7, 1], 'alpha': [0.1], 'dropout': [0.5], 'K': [10]}
num_trial:  9
[32m[I 2021-07-26 09:26:55,278][0m A new study created in memory with name: no-name-df53d0cb-3999-49d4-9df0-9ee19fc41db9[0m
lambda1:  0.02
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 100, Loss: 418.8086, Train: 100.00%, Valid: 71.40% Test: 72.50%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 370.6071, Train: 100.00%, Valid: 73.80% Test: 74.00%
Split: 01, Run: 01, Epoch: 100, Loss: 244.8326, Train: 100.00%, Valid: 74.00% Test: 74.00%
Split: 01, Run: 02, Epoch: 300, Loss: 351.7220, Train: 100.00%, Valid: 75.20% Test: 75.90%
Split: 01, Run: 01, Epoch: 200, Loss: 220.2957, Train: 100.00%, Valid: 74.40% Test: 74.10%
Split: 01, Run: 02, Epoch: 400, Loss: 345.3934, Train: 100.00%, Valid: 70.80% Test: 74.60%
Split: 01, Run: 01, Epoch: 300, Loss: 216.0706, Train: 100.00%, Valid: 73.60% Test: 72.60%
Split: 01, Run: 02, Epoch: 500, Loss: 348.7186, Train: 100.00%, Valid: 72.80% Test: 75.10%
Split: 01, Run: 02
None time:  10.648405089974403
None Run 02:
Highest Train: 100.00
Highest Valid: 75.60
  Final Train: 100.00
   Final Test: 79.60
Split: 01, Run: 01, Epoch: 400, Loss: 202.4162, Train: 100.00%, Valid: 74.20% Test: 74.00%
Split: 01, Run: 03, Epoch: 100, Loss: 419.2680, Train: 98.57%, Valid: 76.00% Test: 77.40%
Split: 01, Run: 01, Epoch: 500, Loss: 196.2939, Train: 100.00%, Valid: 74.20% Test: 73.10%
Split: 01, Run: 01
None time:  12.394069224596024
None Run 01:
Highest Train: 100.00
Highest Valid: 75.00
  Final Train: 100.00
   Final Test: 74.50
Split: 01, Run: 03, Epoch: 200, Loss: 386.7608, Train: 100.00%, Valid: 75.80% Test: 78.20%
Split: 01, Run: 02, Epoch: 100, Loss: 226.2037, Train: 100.00%, Valid: 75.80% Test: 73.90%
Split: 01, Run: 03, Epoch: 300, Loss: 357.9519, Train: 100.00%, Valid: 72.40% Test: 74.00%
Split: 01, Run: 02, Epoch: 200, Loss: 205.1508, Train: 100.00%, Valid: 75.20% Test: 73.30%
Split: 01, Run: 03, Epoch: 400, Loss: 354.3317, Train: 100.00%, Valid: 73.60% Test: 75.90%
Split: 01, Run: 02, Epoch: 300, Loss: 196.4917, Train: 100.00%, Valid: 73.60% Test: 73.10%
Split: 01, Run: 03, Epoch: 500, Loss: 340.4495, Train: 100.00%, Valid: 72.40% Test: 74.20%
Split: 01, Run: 03
None time:  12.026843970641494
None Run 03:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 99.29
   Final Test: 79.80
total time:  30.71146408841014
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.07 Â± 1.55
  Final Train: 99.76 Â± 0.41
   Final Test: 78.87 Â± 1.45
[32m[I 2021-07-26 09:27:16,925][0m Trial 8 finished with value: 76.06666564941406 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.7, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.02
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 195.5587, Train: 100.00%, Valid: 74.00% Test: 73.60%
Split: 01, Run: 01, Epoch: 100, Loss: 371.2876, Train: 98.57%, Valid: 68.60% Test: 70.30%
Split: 01, Run: 02, Epoch: 500, Loss: 195.0179, Train: 100.00%, Valid: 73.80% Test: 72.80%
Split: 01, Run: 02
None time:  11.979826554656029
None Run 02:
Highest Train: 100.00
Highest Valid: 75.80
  Final Train: 100.00
   Final Test: 73.90
Split: 01, Run: 01, Epoch: 200, Loss: 327.5809, Train: 100.00%, Valid: 70.20% Test: 71.50%
Split: 01, Run: 03, Epoch: 100, Loss: 232.8314, Train: 100.00%, Valid: 76.20% Test: 75.50%
Split: 01, Run: 01, Epoch: 300, Loss: 327.7431, Train: 100.00%, Valid: 71.20% Test: 70.70%
Split: 01, Run: 03, Epoch: 200, Loss: 213.9146, Train: 100.00%, Valid: 75.60% Test: 75.10%
Split: 01, Run: 01, Epoch: 400, Loss: 301.3780, Train: 100.00%, Valid: 70.60% Test: 72.40%
Split: 01, Run: 03, Epoch: 300, Loss: 201.1044, Train: 100.00%, Valid: 73.60% Test: 73.40%
Split: 01, Run: 01, Epoch: 500, Loss: 296.9295, Train: 100.00%, Valid: 69.40% Test: 69.60%
Split: 01, Run: 01
None time:  11.964028630405664
None Run 01:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 73.70
Split: 01, Run: 03, Epoch: 400, Loss: 200.6351, Train: 100.00%, Valid: 73.40% Test: 72.70%
Split: 01, Run: 02, Epoch: 100, Loss: 349.7551, Train: 100.00%, Valid: 69.40% Test: 70.80%
Split: 01, Run: 03, Epoch: 500, Loss: 191.0713, Train: 100.00%, Valid: 73.40% Test: 72.50%
Split: 01, Run: 03
None time:  12.022262679412961
None Run 03:
Highest Train: 100.00
Highest Valid: 78.00
  Final Train: 100.00
   Final Test: 76.60
total time:  38.194501319900155
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.27 Â± 1.55
  Final Train: 100.00 Â± 0.00
   Final Test: 75.00 Â± 1.42
[32m[I 2021-07-26 09:27:33,484][0m Trial 0 finished with value: 76.26667022705078 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.5, 'K': 10}. Best is trial 0 with value: 76.26667022705078.[0m
lambda1:  0.01
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 309.2337, Train: 100.00%, Valid: 69.80% Test: 69.90%
Split: 01, Run: 01, Epoch: 100, Loss: 716.8908, Train: 100.00%, Valid: 75.60% Test: 75.70%
Split: 01, Run: 02, Epoch: 300, Loss: 299.2513, Train: 100.00%, Valid: 72.00% Test: 71.50%
Split: 01, Run: 01, Epoch: 200, Loss: 621.0987, Train: 100.00%, Valid: 75.60% Test: 75.50%
Split: 01, Run: 02, Epoch: 400, Loss: 293.4009, Train: 100.00%, Valid: 68.60% Test: 69.80%
Split: 01, Run: 01, Epoch: 300, Loss: 618.8636, Train: 100.00%, Valid: 75.40% Test: 75.60%
Split: 01, Run: 02, Epoch: 500, Loss: 295.6058, Train: 100.00%, Valid: 70.00% Test: 72.00%
Split: 01, Run: 02
None time:  11.938356444239616
None Run 02:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 99.29
   Final Test: 71.20
Split: 01, Run: 01, Epoch: 400, Loss: 572.2792, Train: 100.00%, Valid: 74.60% Test: 75.40%
Split: 01, Run: 03, Epoch: 100, Loss: 351.5161, Train: 98.57%, Valid: 75.60% Test: 77.30%
Split: 01, Run: 01, Epoch: 500, Loss: 555.8688, Train: 100.00%, Valid: 75.00% Test: 75.00%
Split: 01, Run: 01
None time:  11.976167237386107
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 76.30
Split: 01, Run: 03, Epoch: 200, Loss: 324.3275, Train: 100.00%, Valid: 73.20% Test: 77.70%
Split: 01, Run: 02, Epoch: 100, Loss: 666.0555, Train: 100.00%, Valid: 76.00% Test: 75.60%
Split: 01, Run: 03, Epoch: 300, Loss: 297.7803, Train: 100.00%, Valid: 69.00% Test: 70.80%
Split: 01, Run: 02, Epoch: 200, Loss: 581.6281, Train: 100.00%, Valid: 76.00% Test: 75.50%
Split: 01, Run: 03, Epoch: 400, Loss: 295.9301, Train: 100.00%, Valid: 68.20% Test: 70.60%
Split: 01, Run: 02, Epoch: 300, Loss: 553.3561, Train: 100.00%, Valid: 75.20% Test: 75.30%
Split: 01, Run: 03, Epoch: 500, Loss: 287.4965, Train: 100.00%, Valid: 68.80% Test: 71.00%
Split: 01, Run: 03
None time:  11.914544757455587
None Run 03:
Highest Train: 100.00
Highest Valid: 77.20
  Final Train: 100.00
   Final Test: 78.10
total time:  35.87646623887122
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.27 Â± 2.55
  Final Train: 99.76 Â± 0.41
   Final Test: 74.33 Â± 3.49
[32m[I 2021-07-26 09:27:52,808][0m Trial 9 finished with value: 74.26667022705078 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.7, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.01
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 549.4619, Train: 100.00%, Valid: 75.80% Test: 76.10%
Split: 01, Run: 01, Epoch: 100, Loss: 299.9953, Train: 98.57%, Valid: 69.00% Test: 71.00%
Split: 01, Run: 02, Epoch: 500, Loss: 542.9150, Train: 100.00%, Valid: 75.40% Test: 75.30%
Split: 01, Run: 02
None time:  11.93889125622809
None Run 02:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 100.00
   Final Test: 75.80
Split: 01, Run: 01, Epoch: 200, Loss: 267.6496, Train: 100.00%, Valid: 72.60% Test: 74.30%
Split: 01, Run: 03, Epoch: 100, Loss: 662.9578, Train: 100.00%, Valid: 77.20% Test: 76.70%
Split: 01, Run: 01, Epoch: 300, Loss: 263.4734, Train: 100.00%, Valid: 71.40% Test: 71.40%
Split: 01, Run: 03, Epoch: 200, Loss: 600.7442, Train: 100.00%, Valid: 76.60% Test: 77.10%
Split: 01, Run: 01, Epoch: 400, Loss: 243.9023, Train: 100.00%, Valid: 71.80% Test: 72.20%
Split: 01, Run: 03, Epoch: 300, Loss: 558.0021, Train: 100.00%, Valid: 76.40% Test: 77.00%
Split: 01, Run: 01, Epoch: 500, Loss: 236.1204, Train: 100.00%, Valid: 70.20% Test: 70.80%
Split: 01, Run: 01
None time:  11.94627046212554
None Run 01:
Highest Train: 100.00
Highest Valid: 73.60
  Final Train: 100.00
   Final Test: 74.70
Split: 01, Run: 03, Epoch: 400, Loss: 556.1805, Train: 100.00%, Valid: 76.40% Test: 77.10%
Split: 01, Run: 02, Epoch: 100, Loss: 275.2697, Train: 99.29%, Valid: 71.20% Test: 71.50%
Split: 01, Run: 03, Epoch: 500, Loss: 536.5043, Train: 100.00%, Valid: 75.20% Test: 76.60%
Split: 01, Run: 03
None time:  12.028054052963853
None Run 03:
Highest Train: 100.00
Highest Valid: 78.00
  Final Train: 100.00
   Final Test: 76.90
total time:  36.01928202435374
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.13 Â± 0.76
  Final Train: 100.00 Â± 0.00
   Final Test: 76.33 Â± 0.55
[32m[I 2021-07-26 09:28:09,509][0m Trial 1 finished with value: 77.13333129882812 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 1, 'K': 10}. Best is trial 1 with value: 77.13333129882812.[0m
lambda1:  0.05
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 249.0819, Train: 100.00%, Valid: 72.40% Test: 71.50%
Split: 01, Run: 02, Epoch: 300, Loss: 237.3735, Train: 100.00%, Valid: 72.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 100, Loss: 155.7217, Train: 100.00%, Valid: 70.00% Test: 71.10%
Split: 01, Run: 02, Epoch: 400, Loss: 235.3148, Train: 100.00%, Valid: 70.40% Test: 72.80%
Split: 01, Run: 01, Epoch: 200, Loss: 137.2589, Train: 100.00%, Valid: 68.80% Test: 70.60%
Split: 01, Run: 02, Epoch: 500, Loss: 232.8045, Train: 100.00%, Valid: 72.60% Test: 74.20%
Split: 01, Run: 02
None time:  11.972013410180807
None Run 02:
Highest Train: 100.00
Highest Valid: 73.80
  Final Train: 100.00
   Final Test: 74.10
Split: 01, Run: 01, Epoch: 300, Loss: 138.3974, Train: 100.00%, Valid: 69.20% Test: 70.80%
Split: 01, Run: 03, Epoch: 100, Loss: 281.7350, Train: 98.57%, Valid: 75.20% Test: 77.00%
Split: 01, Run: 01, Epoch: 400, Loss: 129.7901, Train: 100.00%, Valid: 69.60% Test: 71.70%
Split: 01, Run: 03, Epoch: 200, Loss: 255.9219, Train: 100.00%, Valid: 74.40% Test: 77.20%
Split: 01, Run: 01, Epoch: 500, Loss: 124.1418, Train: 100.00%, Valid: 69.60% Test: 71.40%
Split: 01, Run: 01
None time:  12.01115870848298
None Run 01:
Highest Train: 100.00
Highest Valid: 71.00
  Final Train: 100.00
   Final Test: 73.10
Split: 01, Run: 03, Epoch: 300, Loss: 240.2383, Train: 100.00%, Valid: 69.60% Test: 71.70%
Split: 01, Run: 02, Epoch: 100, Loss: 146.9274, Train: 100.00%, Valid: 70.20% Test: 70.60%
Split: 01, Run: 03, Epoch: 400, Loss: 237.0365, Train: 100.00%, Valid: 69.80% Test: 70.60%
Split: 01, Run: 02, Epoch: 200, Loss: 131.4936, Train: 100.00%, Valid: 68.80% Test: 70.20%
Split: 01, Run: 03, Epoch: 500, Loss: 228.6676, Train: 100.00%, Valid: 70.20% Test: 72.30%
Split: 01, Run: 03
None time:  11.934123495593667
None Run 03:
Highest Train: 100.00
Highest Valid: 77.60
  Final Train: 99.29
   Final Test: 78.70
total time:  35.91221869550645
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 75.00 Â± 2.25
  Final Train: 99.76 Â± 0.41
   Final Test: 75.83 Â± 2.50
[32m[I 2021-07-26 09:28:28,726][0m Trial 10 finished with value: 75.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.5, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
lambda1:  0.02
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Split: 01, Run: 02, Epoch: 300, Loss: 129.6758, Train: 100.00%, Valid: 68.60% Test: 69.60%
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 400, Loss: 128.9052, Train: 100.00%, Valid: 70.20% Test: 71.50%
Split: 01, Run: 01, Epoch: 100, Loss: 244.8326, Train: 98.57%, Valid: 67.00% Test: 68.00%
Split: 01, Run: 02, Epoch: 500, Loss: 131.5558, Train: 100.00%, Valid: 69.80% Test: 70.80%
Split: 01, Run: 02
None time:  11.914479240775108
None Run 02:
Highest Train: 100.00
Highest Valid: 72.20
  Final Train: 100.00
   Final Test: 71.70
Split: 01, Run: 01, Epoch: 200, Loss: 220.2957, Train: 100.00%, Valid: 68.80% Test: 70.20%
Split: 01, Run: 03, Epoch: 100, Loss: 151.3408, Train: 100.00%, Valid: 72.80% Test: 76.00%
Split: 01, Run: 01, Epoch: 300, Loss: 216.0706, Train: 100.00%, Valid: 68.40% Test: 69.20%
Split: 01, Run: 03, Epoch: 200, Loss: 141.0094, Train: 100.00%, Valid: 70.80% Test: 73.20%
Split: 01, Run: 01, Epoch: 400, Loss: 202.4162, Train: 100.00%, Valid: 69.60% Test: 70.70%
Split: 01, Run: 03, Epoch: 300, Loss: 133.3622, Train: 100.00%, Valid: 69.20% Test: 70.60%
Split: 01, Run: 01, Epoch: 500, Loss: 196.2939, Train: 100.00%, Valid: 67.60% Test: 68.30%
Split: 01, Run: 01
None time:  12.17056574486196
None Run 01:
Highest Train: 100.00
Highest Valid: 70.80
  Final Train: 100.00
   Final Test: 72.30
Split: 01, Run: 03, Epoch: 400, Loss: 129.5757, Train: 100.00%, Valid: 69.20% Test: 71.10%
Split: 01, Run: 02, Epoch: 100, Loss: 226.2037, Train: 99.29%, Valid: 66.80% Test: 68.30%
Split: 01, Run: 03, Epoch: 500, Loss: 125.1004, Train: 100.00%, Valid: 68.80% Test: 70.00%
Split: 01, Run: 03
None time:  11.983726693317294
None Run 03:
Highest Train: 100.00
Highest Valid: 78.80
  Final Train: 100.00
   Final Test: 78.90
total time:  35.976167276501656
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.00 Â± 4.20
  Final Train: 100.00 Â± 0.00
   Final Test: 74.57 Â± 3.82
[32m[I 2021-07-26 09:28:45,491][0m Trial 2 finished with value: 74.0 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.5, 'K': 10}. Best is trial 1 with value: 77.13333129882812.[0m
lambda1:  0.05
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 02, Epoch: 200, Loss: 205.1508, Train: 100.00%, Valid: 67.00% Test: 67.70%
Split: 01, Run: 01, Epoch: 100, Loss: 241.5226, Train: 100.00%, Valid: 70.00% Test: 72.60%
Split: 01, Run: 02, Epoch: 300, Loss: 196.4917, Train: 100.00%, Valid: 69.60% Test: 69.40%
Split: 01, Run: 01, Epoch: 200, Loss: 217.8604, Train: 100.00%, Valid: 70.00% Test: 72.10%
Split: 01, Run: 02, Epoch: 400, Loss: 195.5587, Train: 100.00%, Valid: 68.40% Test: 69.10%
Split: 01, Run: 01, Epoch: 300, Loss: 215.1773, Train: 100.00%, Valid: 69.40% Test: 71.20%
Split: 01, Run: 02, Epoch: 500, Loss: 195.0179, Train: 100.00%, Valid: 69.00% Test: 69.80%
Split: 01, Run: 02
None time:  11.979658398777246
None Run 02:
Highest Train: 100.00
Highest Valid: 72.60
  Final Train: 100.00
   Final Test: 72.10
Split: 01, Run: 01, Epoch: 400, Loss: 200.0375, Train: 100.00%, Valid: 71.40% Test: 72.60%
Split: 01, Run: 03, Epoch: 100, Loss: 232.8314, Train: 99.29%, Valid: 74.20% Test: 76.70%
Split: 01, Run: 01, Epoch: 500, Loss: 192.2067, Train: 100.00%, Valid: 71.00% Test: 72.30%
Split: 01, Run: 01
None time:  12.04870573990047
None Run 01:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 74.10
Split: 01, Run: 03, Epoch: 200, Loss: 213.9146, Train: 100.00%, Valid: 73.60% Test: 77.80%
Split: 01, Run: 02, Epoch: 100, Loss: 228.4145, Train: 100.00%, Valid: 71.00% Test: 71.90%
Split: 01, Run: 03, Epoch: 300, Loss: 201.1044, Train: 100.00%, Valid: 65.00% Test: 67.70%
Split: 01, Run: 02, Epoch: 200, Loss: 207.9018, Train: 100.00%, Valid: 70.60% Test: 71.10%
Split: 01, Run: 03, Epoch: 400, Loss: 200.6351, Train: 100.00%, Valid: 67.00% Test: 67.40%
Split: 01, Run: 02, Epoch: 300, Loss: 196.9994, Train: 100.00%, Valid: 70.00% Test: 71.00%
Split: 01, Run: 03, Epoch: 500, Loss: 191.0713, Train: 100.00%, Valid: 67.40% Test: 68.10%
Split: 01, Run: 03
None time:  12.021821983158588
None Run 03:
Highest Train: 100.00
Highest Valid: 76.60
  Final Train: 98.57
   Final Test: 78.10
total time:  36.23278100974858
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 73.33 Â± 2.97
  Final Train: 99.52 Â± 0.82
   Final Test: 74.17 Â± 3.41
[32m[I 2021-07-26 09:29:04,965][0m Trial 11 finished with value: 73.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.5, 'K': 10}. Best is trial 5 with value: 77.0666732788086.[0m
Study statistics: 
  Number of finished trials:  12
  Number of pruned trials:  0
  Number of complete trials:  12
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 1, 'K': 10}   trial.value:  77.067    {'train': '100.00 Â± 0.00', 'valid': '77.07 Â± 1.03', 'test': '78.47 Â± 1.97'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.7, 'K': 10}   trial.value:  76.067    {'train': '99.76 Â± 0.41', 'valid': '76.07 Â± 1.55', 'test': '78.87 Â± 1.45'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.5, 'K': 10}   trial.value:  75    {'train': '99.76 Â± 0.41', 'valid': '75.00 Â± 2.25', 'test': '75.83 Â± 2.50'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}   trial.value:  77.067    {'train': '100.00 Â± 0.00', 'valid': '77.07 Â± 0.58', 'test': '78.60 Â± 0.62'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.7, 'K': 10}   trial.value:  74.267    {'train': '99.76 Â± 0.41', 'valid': '74.27 Â± 2.55', 'test': '74.33 Â± 3.49'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.5, 'K': 10}   trial.value:  73.333    {'train': '99.52 Â± 0.82', 'valid': '73.33 Â± 2.97', 'test': '74.17 Â± 3.41'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 1, 'K': 10}   trial.value:  71.933    {'train': '99.29 Â± 0.71', 'valid': '71.93 Â± 4.41', 'test': '71.17 Â± 5.75'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.5, 'K': 10}   trial.value:  69.267    {'train': '99.52 Â± 0.82', 'valid': '69.27 Â± 6.17', 'test': '67.27 Â± 7.88'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.7, 'K': 10}   trial.value:  71.333    {'train': '99.52 Â± 0.82', 'valid': '71.33 Â± 4.39', 'test': '71.50 Â± 4.07'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.7, 'K': 10}   trial.value:  65.467    {'train': '99.52 Â± 0.82', 'valid': '65.47 Â± 8.63', 'test': '65.00 Â± 8.32'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 0.5, 'K': 10}   trial.value:  62.933    {'train': '99.52 Â± 0.41', 'valid': '62.93 Â± 11.28', 'test': '61.23 Â± 11.60'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.1, 'lambda2': 1, 'K': 10}   trial.value:  67    {'train': '99.76 Â± 0.41', 'valid': '67.00 Â± 7.47', 'test': '64.83 Â± 8.59'}
test_acc
['78.47 Â± 1.97', '78.87 Â± 1.45', '75.83 Â± 2.50', '78.60 Â± 0.62', '74.33 Â± 3.49', '74.17 Â± 3.41', '71.17 Â± 5.75', '67.27 Â± 7.88', '71.50 Â± 4.07', '65.00 Â± 8.32', '61.23 Â± 11.60', '64.83 Â± 8.59']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}
Best trial Value:  77.0666732788086
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '77.07 Â± 0.58', 'test': '78.60 Â± 0.62'}
optuna total time:  329.4352213665843
Split: 01, Run: 02, Epoch: 400, Loss: 198.8398, Train: 100.00%, Valid: 71.40% Test: 72.50%
Split: 01, Run: 02, Epoch: 500, Loss: 196.7867, Train: 100.00%, Valid: 71.60% Test: 72.50%
Split: 01, Run: 02
None time:  10.071413807570934
None Run 02:
Highest Train: 100.00
Highest Valid: 73.00
  Final Train: 100.00
   Final Test: 73.80
Split: 01, Run: 03, Epoch: 100, Loss: 235.3830, Train: 100.00%, Valid: 74.40% Test: 76.70%
Split: 01, Run: 03, Epoch: 200, Loss: 215.8106, Train: 100.00%, Valid: 73.60% Test: 74.90%
Split: 01, Run: 03, Epoch: 300, Loss: 205.2302, Train: 100.00%, Valid: 70.80% Test: 72.70%
Split: 01, Run: 03, Epoch: 400, Loss: 202.6054, Train: 100.00%, Valid: 71.00% Test: 72.90%
Split: 01, Run: 03, Epoch: 500, Loss: 193.2564, Train: 100.00%, Valid: 70.00% Test: 71.40%
Split: 01, Run: 03
None time:  7.51670934446156
None Run 03:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 100.00
   Final Test: 77.70
total time:  29.70751476660371
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 74.80 Â± 3.12
  Final Train: 100.00 Â± 0.00
   Final Test: 75.20 Â± 2.17
[32m[I 2021-07-26 09:29:15,205][0m Trial 3 finished with value: 74.79999542236328 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.7, 'K': 10}. Best is trial 1 with value: 77.13333129882812.[0m
lambda1:  0.05
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.05, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 397.0610, Train: 100.00%, Valid: 71.00% Test: 72.80%
Split: 01, Run: 01, Epoch: 200, Loss: 357.3250, Train: 100.00%, Valid: 70.60% Test: 72.20%
Split: 01, Run: 01, Epoch: 300, Loss: 353.8421, Train: 100.00%, Valid: 69.80% Test: 71.00%
Split: 01, Run: 01, Epoch: 400, Loss: 323.4117, Train: 100.00%, Valid: 71.60% Test: 72.40%
Split: 01, Run: 01, Epoch: 500, Loss: 311.8922, Train: 100.00%, Valid: 71.00% Test: 71.90%
Split: 01, Run: 01
None time:  7.638325694948435
None Run 01:
Highest Train: 100.00
Highest Valid: 73.40
  Final Train: 100.00
   Final Test: 74.00
Split: 01, Run: 02, Epoch: 100, Loss: 374.6374, Train: 100.00%, Valid: 73.40% Test: 73.80%
Split: 01, Run: 02, Epoch: 200, Loss: 332.4854, Train: 100.00%, Valid: 71.40% Test: 72.00%
Split: 01, Run: 02, Epoch: 300, Loss: 315.9682, Train: 100.00%, Valid: 71.00% Test: 71.90%
Split: 01, Run: 02, Epoch: 400, Loss: 322.5914, Train: 100.00%, Valid: 71.60% Test: 72.40%
Split: 01, Run: 02, Epoch: 500, Loss: 319.3086, Train: 100.00%, Valid: 71.80% Test: 73.00%
Split: 01, Run: 02
None time:  7.689292296767235
None Run 02:
Highest Train: 100.00
Highest Valid: 74.20
  Final Train: 100.00
   Final Test: 74.10
Split: 01, Run: 03, Epoch: 100, Loss: 386.3424, Train: 100.00%, Valid: 73.40% Test: 76.00%
Split: 01, Run: 03, Epoch: 200, Loss: 355.2132, Train: 100.00%, Valid: 73.20% Test: 75.70%
Split: 01, Run: 03, Epoch: 300, Loss: 332.7571, Train: 100.00%, Valid: 73.00% Test: 74.40%
Split: 01, Run: 03, Epoch: 400, Loss: 320.4848, Train: 100.00%, Valid: 71.60% Test: 73.50%
Split: 01, Run: 03, Epoch: 500, Loss: 307.0777, Train: 100.00%, Valid: 71.80% Test: 72.90%
Split: 01, Run: 03
None time:  7.670410109683871
None Run 03:
Highest Train: 100.00
Highest Valid: 78.40
  Final Train: 100.00
   Final Test: 77.80
total time:  23.05696522258222
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 75.33 Â± 2.69
  Final Train: 100.00 Â± 0.00
   Final Test: 75.30 Â± 2.17
[32m[I 2021-07-26 09:29:38,268][0m Trial 4 finished with value: 75.33333587646484 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 1, 'K': 10}. Best is trial 1 with value: 77.13333129882812.[0m
lambda1:  0.01
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 445.3999, Train: 100.00%, Valid: 75.60% Test: 74.90%
Split: 01, Run: 01, Epoch: 200, Loss: 398.7984, Train: 100.00%, Valid: 75.60% Test: 75.00%
Split: 01, Run: 01, Epoch: 300, Loss: 393.8703, Train: 100.00%, Valid: 74.80% Test: 74.70%
Split: 01, Run: 01, Epoch: 400, Loss: 365.6066, Train: 100.00%, Valid: 74.80% Test: 75.10%
Split: 01, Run: 01, Epoch: 500, Loss: 357.2861, Train: 100.00%, Valid: 74.20% Test: 74.60%
Split: 01, Run: 01
None time:  7.7881970796734095
None Run 01:
Highest Train: 100.00
Highest Valid: 76.40
  Final Train: 100.00
   Final Test: 75.40
Split: 01, Run: 02, Epoch: 100, Loss: 418.8086, Train: 100.00%, Valid: 76.00% Test: 75.30%
Split: 01, Run: 02, Epoch: 200, Loss: 370.6071, Train: 100.00%, Valid: 76.00% Test: 75.40%
Split: 01, Run: 02, Epoch: 300, Loss: 351.7220, Train: 100.00%, Valid: 75.00% Test: 74.90%
Split: 01, Run: 02, Epoch: 400, Loss: 345.3934, Train: 100.00%, Valid: 76.00% Test: 76.30%
Split: 01, Run: 02, Epoch: 500, Loss: 348.7186, Train: 100.00%, Valid: 76.00% Test: 75.70%
Split: 01, Run: 02
None time:  7.80673442594707
None Run 02:
Highest Train: 100.00
Highest Valid: 77.00
  Final Train: 100.00
   Final Test: 76.00
Split: 01, Run: 03, Epoch: 100, Loss: 419.2680, Train: 100.00%, Valid: 76.80% Test: 76.60%
Split: 01, Run: 03, Epoch: 200, Loss: 386.7608, Train: 100.00%, Valid: 76.40% Test: 76.30%
Split: 01, Run: 03, Epoch: 300, Loss: 357.9519, Train: 100.00%, Valid: 76.00% Test: 76.30%
Split: 01, Run: 03, Epoch: 400, Loss: 354.3317, Train: 100.00%, Valid: 74.40% Test: 74.60%
Split: 01, Run: 03, Epoch: 500, Loss: 340.4495, Train: 100.00%, Valid: 73.60% Test: 73.90%
Split: 01, Run: 03
None time:  7.740237325429916
None Run 03:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 76.60
total time:  23.40060012973845
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.07 Â± 0.70
  Final Train: 100.00 Â± 0.00
   Final Test: 76.00 Â± 0.60
[32m[I 2021-07-26 09:30:01,674][0m Trial 5 finished with value: 77.06666564941406 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.7, 'K': 10}. Best is trial 1 with value: 77.13333129882812.[0m
lambda1:  0.02
lambda2:  1
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=1, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 592.9175, Train: 100.00%, Valid: 76.00% Test: 75.40%
Split: 01, Run: 01, Epoch: 200, Loss: 521.5066, Train: 100.00%, Valid: 74.80% Test: 75.30%
Split: 01, Run: 01, Epoch: 300, Loss: 524.2834, Train: 100.00%, Valid: 74.60% Test: 75.10%
Split: 01, Run: 01, Epoch: 400, Loss: 479.5249, Train: 100.00%, Valid: 76.00% Test: 76.10%
Split: 01, Run: 01, Epoch: 500, Loss: 472.4229, Train: 100.00%, Valid: 75.40% Test: 75.70%
Split: 01, Run: 01
None time:  7.8074995912611485
None Run 01:
Highest Train: 100.00
Highest Valid: 76.80
  Final Train: 100.00
   Final Test: 76.50
Split: 01, Run: 02, Epoch: 100, Loss: 558.1734, Train: 100.00%, Valid: 76.20% Test: 75.80%
Split: 01, Run: 02, Epoch: 200, Loss: 487.3016, Train: 100.00%, Valid: 76.80% Test: 75.00%
Split: 01, Run: 02, Epoch: 300, Loss: 469.8120, Train: 100.00%, Valid: 74.80% Test: 74.50%
Split: 01, Run: 02, Epoch: 400, Loss: 453.5393, Train: 100.00%, Valid: 76.80% Test: 76.70%
Split: 01, Run: 02, Epoch: 500, Loss: 463.9928, Train: 100.00%, Valid: 77.00% Test: 76.30%
Split: 01, Run: 02
None time:  7.775216393172741
None Run 02:
Highest Train: 100.00
Highest Valid: 77.40
  Final Train: 100.00
   Final Test: 76.00
Split: 01, Run: 03, Epoch: 100, Loss: 564.6490, Train: 100.00%, Valid: 77.00% Test: 76.60%
Split: 01, Run: 03, Epoch: 200, Loss: 516.8737, Train: 100.00%, Valid: 76.80% Test: 77.30%
Split: 01, Run: 03, Epoch: 300, Loss: 474.1263, Train: 100.00%, Valid: 75.60% Test: 76.40%
Split: 01, Run: 03, Epoch: 400, Loss: 469.3538, Train: 100.00%, Valid: 74.40% Test: 75.40%
Split: 01, Run: 03, Epoch: 500, Loss: 457.7332, Train: 100.00%, Valid: 74.60% Test: 75.30%
Split: 01, Run: 03
None time:  7.744478428736329
None Run 03:
Highest Train: 100.00
Highest Valid: 78.20
  Final Train: 100.00
   Final Test: 77.20
total time:  23.388464376330376
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 77.47 Â± 0.70
  Final Train: 100.00 Â± 0.00
   Final Test: 76.57 Â± 0.60
[32m[I 2021-07-26 09:30:25,068][0m Trial 6 finished with value: 77.46666717529297 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}. Best is trial 6 with value: 77.46666717529297.[0m
lambda1:  0.01
lambda2:  0.5
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.01, lambda2=0.5, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 299.9953, Train: 100.00%, Valid: 75.40% Test: 74.80%
Split: 01, Run: 01, Epoch: 200, Loss: 267.6496, Train: 100.00%, Valid: 74.60% Test: 74.50%
Split: 01, Run: 01, Epoch: 300, Loss: 263.4734, Train: 100.00%, Valid: 74.40% Test: 73.90%
Split: 01, Run: 01, Epoch: 400, Loss: 243.9023, Train: 100.00%, Valid: 73.60% Test: 74.10%
Split: 01, Run: 01, Epoch: 500, Loss: 236.1204, Train: 100.00%, Valid: 73.60% Test: 73.60%
Split: 01, Run: 01
None time:  7.699130054563284
None Run 01:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 100.00
   Final Test: 74.70
Split: 01, Run: 02, Epoch: 100, Loss: 275.2697, Train: 100.00%, Valid: 75.80% Test: 74.70%
Split: 01, Run: 02, Epoch: 200, Loss: 249.0819, Train: 100.00%, Valid: 75.40% Test: 74.60%
Split: 01, Run: 02, Epoch: 300, Loss: 237.3735, Train: 100.00%, Valid: 74.40% Test: 74.20%
Split: 01, Run: 02, Epoch: 400, Loss: 235.3148, Train: 100.00%, Valid: 74.20% Test: 74.80%
Split: 01, Run: 02, Epoch: 500, Loss: 232.8045, Train: 100.00%, Valid: 75.00% Test: 74.80%
Split: 01, Run: 02
None time:  7.727422399446368
None Run 02:
Highest Train: 100.00
Highest Valid: 75.80
  Final Train: 100.00
   Final Test: 74.70
Split: 01, Run: 03, Epoch: 100, Loss: 281.7350, Train: 100.00%, Valid: 76.20% Test: 75.50%
Split: 01, Run: 03, Epoch: 200, Loss: 255.9219, Train: 100.00%, Valid: 75.60% Test: 75.30%
Split: 01, Run: 03, Epoch: 300, Loss: 240.2383, Train: 100.00%, Valid: 75.00% Test: 74.90%
Split: 01, Run: 03, Epoch: 400, Loss: 237.0365, Train: 100.00%, Valid: 73.00% Test: 73.60%
Split: 01, Run: 03, Epoch: 500, Loss: 228.6676, Train: 100.00%, Valid: 73.40% Test: 73.20%
Split: 01, Run: 03
None time:  7.72917796485126
None Run 03:
Highest Train: 100.00
Highest Valid: 77.80
  Final Train: 100.00
   Final Test: 75.80
total time:  23.227518958970904
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.60 Â± 1.06
  Final Train: 100.00 Â± 0.00
   Final Test: 75.07 Â± 0.64
[32m[I 2021-07-26 09:30:48,302][0m Trial 7 finished with value: 76.5999984741211 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.5, 'K': 10}. Best is trial 6 with value: 77.46666717529297.[0m
lambda1:  0.02
lambda2:  0.7
K:  10
alpha:  0.1
lr:  0.01
weight_decay:  0.0005
dropout:  0.5
Namespace(K=10, L21=True, LP=False, alpha=0.1, dataset='Cora', debug=True, defense=None, device=0, dropout=0.5, epochs=500, gamma=None, hidden_channels=64, lambda1=0.02, lambda2=0.7, log_steps=100, loss='CE', lr=0.01, model='ALTOPT', normalize_features=True, num_layers=2, ogb=False, prop='ALTOPT', ptb_rate=0, random_splits=0, runs=3, seed=12321312, sort_key='lambda1', weight_decay=0.0005)
fix split and run 3 times
Data: Data(adj_t=[2708, 2708, nnz=10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
ALTOPT(
  (lin1): Linear(in_features=1433, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=7, bias=True)
  (prop): Propagation(K=10, mode=ALTOPT, lambda1=None, lambda2=None, L21=True, alpha=0.1)
)
Split: 01, Run: 01, Epoch: 100, Loss: 371.2876, Train: 100.00%, Valid: 75.40% Test: 75.00%
Split: 01, Run: 01, Epoch: 200, Loss: 327.5809, Train: 100.00%, Valid: 74.60% Test: 74.20%
Split: 01, Run: 01, Epoch: 300, Loss: 327.7431, Train: 100.00%, Valid: 73.60% Test: 73.90%
Split: 01, Run: 01, Epoch: 400, Loss: 301.3780, Train: 100.00%, Valid: 74.60% Test: 74.20%
Split: 01, Run: 01, Epoch: 500, Loss: 296.9295, Train: 100.00%, Valid: 74.40% Test: 73.50%
Split: 01, Run: 01
None time:  7.671392036601901
None Run 01:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 100.00
   Final Test: 74.60
Split: 01, Run: 02, Epoch: 100, Loss: 349.7551, Train: 100.00%, Valid: 76.00% Test: 75.00%
Split: 01, Run: 02, Epoch: 200, Loss: 309.2337, Train: 100.00%, Valid: 75.80% Test: 74.50%
Split: 01, Run: 02, Epoch: 300, Loss: 299.2513, Train: 100.00%, Valid: 74.60% Test: 73.90%
Split: 01, Run: 02, Epoch: 400, Loss: 293.4009, Train: 100.00%, Valid: 74.20% Test: 74.60%
Split: 01, Run: 02, Epoch: 500, Loss: 295.6058, Train: 100.00%, Valid: 75.00% Test: 74.60%
Split: 01, Run: 02
None time:  7.7350423857569695
None Run 02:
Highest Train: 100.00
Highest Valid: 76.20
  Final Train: 100.00
   Final Test: 75.00
Split: 01, Run: 03, Epoch: 100, Loss: 351.5161, Train: 100.00%, Valid: 76.80% Test: 76.60%
Split: 01, Run: 03, Epoch: 200, Loss: 324.3275, Train: 100.00%, Valid: 76.40% Test: 76.10%
Split: 01, Run: 03, Epoch: 300, Loss: 297.7803, Train: 100.00%, Valid: 74.60% Test: 75.10%
Split: 01, Run: 03, Epoch: 400, Loss: 295.9301, Train: 100.00%, Valid: 73.20% Test: 73.50%
Split: 01, Run: 03, Epoch: 500, Loss: 287.4965, Train: 100.00%, Valid: 73.80% Test: 73.80%
Split: 01, Run: 03
None time:  8.043567292392254
None Run 03:
Highest Train: 100.00
Highest Valid: 78.00
  Final Train: 100.00
   Final Test: 76.60
total time:  23.508722869679332
None All runs:
Highest Train: 100.00 Â± 0.00
Highest Valid: 76.80 Â± 1.04
  Final Train: 100.00 Â± 0.00
   Final Test: 75.40 Â± 1.06
[32m[I 2021-07-26 09:31:11,816][0m Trial 8 finished with value: 76.80000305175781 and parameters: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.7, 'K': 10}. Best is trial 6 with value: 77.46666717529297.[0m
Study statistics: 
  Number of finished trials:  9
  Number of pruned trials:  0
  Number of complete trials:  9
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 1, 'K': 10}   trial.value:  77.133    {'train': '100.00 Â± 0.00', 'valid': '77.13 Â± 0.76', 'test': '76.33 Â± 0.55'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.7, 'K': 10}   trial.value:  77.067    {'train': '100.00 Â± 0.00', 'valid': '77.07 Â± 0.70', 'test': '76.00 Â± 0.60'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.01, 'lambda2': 0.5, 'K': 10}   trial.value:  76.6    {'train': '100.00 Â± 0.00', 'valid': '76.60 Â± 1.06', 'test': '75.07 Â± 0.64'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.5, 'K': 10}   trial.value:  76.267    {'train': '100.00 Â± 0.00', 'valid': '76.27 Â± 1.55', 'test': '75.00 Â± 1.42'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}   trial.value:  77.467    {'train': '100.00 Â± 0.00', 'valid': '77.47 Â± 0.70', 'test': '76.57 Â± 0.60'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 0.7, 'K': 10}   trial.value:  76.8    {'train': '100.00 Â± 0.00', 'valid': '76.80 Â± 1.04', 'test': '75.40 Â± 1.06'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.5, 'K': 10}   trial.value:  74    {'train': '100.00 Â± 0.00', 'valid': '74.00 Â± 4.20', 'test': '74.57 Â± 3.82'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 0.7, 'K': 10}   trial.value:  74.8    {'train': '100.00 Â± 0.00', 'valid': '74.80 Â± 3.12', 'test': '75.20 Â± 2.17'}
trial.params:  {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.05, 'lambda2': 1, 'K': 10}   trial.value:  75.333    {'train': '100.00 Â± 0.00', 'valid': '75.33 Â± 2.69', 'test': '75.30 Â± 2.17'}
test_acc
['76.33 Â± 0.55', '76.00 Â± 0.60', '75.07 Â± 0.64', '75.00 Â± 1.42', '76.57 Â± 0.60', '75.40 Â± 1.06', '74.57 Â± 3.82', '75.20 Â± 2.17', '75.30 Â± 2.17']
Best params: {'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'lambda1': 0.02, 'lambda2': 1, 'K': 10}
Best trial Value:  77.46666717529297
Best trial Acc:  {'train': '100.00 Â± 0.00', 'valid': '77.47 Â± 0.70', 'test': '76.57 Â± 0.60'}
optuna total time:  256.54644554294646
